{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-02-03 06:10:49,549 : INFO : collecting all words and their counts\n",
      "2018-02-03 06:10:49,550 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2018-02-03 06:10:49,551 : INFO : collected 3 word types from a corpus of 4 raw words and 2 sentences\n",
      "2018-02-03 06:10:49,551 : INFO : Loading a fresh vocabulary\n",
      "2018-02-03 06:10:49,553 : INFO : min_count=1 retains 3 unique words (100% of original 3, drops 0)\n",
      "2018-02-03 06:10:49,554 : INFO : min_count=1 leaves 4 word corpus (100% of original 4, drops 0)\n",
      "2018-02-03 06:10:49,555 : INFO : deleting the raw counts dictionary of 3 items\n",
      "2018-02-03 06:10:49,557 : INFO : sample=0.001 downsamples 3 most-common words\n",
      "2018-02-03 06:10:49,558 : INFO : downsampling leaves estimated 0 word corpus (5.7% of prior 4)\n",
      "2018-02-03 06:10:49,560 : INFO : estimated required memory for 3 words and 100 dimensions: 3900 bytes\n",
      "2018-02-03 06:10:49,561 : INFO : resetting layer weights\n",
      "2018-02-03 06:10:49,563 : INFO : training model with 3 workers on 3 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2018-02-03 06:10:49,567 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-02-03 06:10:49,569 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-02-03 06:10:49,570 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-02-03 06:10:49,571 : INFO : training on 20 raw words (0 effective words) took 0.0s, 0 effective words/s\n",
      "2018-02-03 06:10:49,573 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2018-02-03 06:10:49,582 : INFO : collecting all words and their counts\n",
      "2018-02-03 06:10:49,583 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2018-02-03 06:10:49,587 : INFO : collected 2694 word types from a corpus of 6891 raw words and 363 sentences\n",
      "2018-02-03 06:10:49,590 : INFO : Loading a fresh vocabulary\n",
      "2018-02-03 06:10:49,594 : INFO : min_count=5 retains 177 unique words (6% of original 2694, drops 2517)\n",
      "2018-02-03 06:10:49,595 : INFO : min_count=5 leaves 2239 word corpus (32% of original 6891, drops 4652)\n",
      "2018-02-03 06:10:49,598 : INFO : deleting the raw counts dictionary of 2694 items\n",
      "2018-02-03 06:10:49,600 : INFO : sample=0.001 downsamples 118 most-common words\n",
      "2018-02-03 06:10:49,601 : INFO : downsampling leaves estimated 1136 word corpus (50.8% of prior 2239)\n",
      "2018-02-03 06:10:49,603 : INFO : estimated required memory for 177 words and 100 dimensions: 230100 bytes\n",
      "2018-02-03 06:10:49,605 : INFO : resetting layer weights\n",
      "2018-02-03 06:10:49,610 : INFO : training model with 3 workers on 177 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2018-02-03 06:10:49,625 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-02-03 06:10:49,628 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-02-03 06:10:49,630 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-02-03 06:10:49,631 : INFO : training on 34455 raw words (5601 effective words) took 0.0s, 504900 effective words/s\n",
      "2018-02-03 06:10:49,632 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    }
   ],
   "source": [
    "# https://rare-technologies.com/word2vec-tutorial/\n",
    "import gensim, logging, os\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    " \n",
    "other_sentences = [['first', 'sentence'], ['second', 'sentence']]\n",
    "# train word2vec on the two other_sentences\n",
    "model = gensim.models.Word2Vec(other_sentences, min_count=1)\n",
    "\n",
    "class MySentences(object):\n",
    "    def __init__(self, dirname):\n",
    "        self.dirname = dirname\n",
    " \n",
    "    def __iter__(self):\n",
    "        for fname in os.listdir(self.dirname):\n",
    "            for line in open(os.path.join(self.dirname, fname)):\n",
    "                yield line.split()\n",
    " \n",
    "some_sentences = list(MySentences('yezheng_test/')) # a memory-friendly iterator\n",
    "model = gensim.models.Word2Vec(some_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-02-03 06:10:49,646 : INFO : collecting all words and their counts\n",
      "2018-02-03 06:10:49,648 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2018-02-03 06:10:49,652 : INFO : collected 2694 word types from a corpus of 6891 raw words and 363 sentences\n",
      "2018-02-03 06:10:49,653 : INFO : Loading a fresh vocabulary\n",
      "2018-02-03 06:10:49,656 : INFO : min_count=5 retains 177 unique words (6% of original 2694, drops 2517)\n",
      "2018-02-03 06:10:49,658 : INFO : min_count=5 leaves 2239 word corpus (32% of original 6891, drops 4652)\n",
      "2018-02-03 06:10:49,661 : INFO : deleting the raw counts dictionary of 2694 items\n",
      "2018-02-03 06:10:49,662 : INFO : sample=0.001 downsamples 118 most-common words\n",
      "2018-02-03 06:10:49,663 : INFO : downsampling leaves estimated 1136 word corpus (50.8% of prior 2239)\n",
      "2018-02-03 06:10:49,665 : INFO : estimated required memory for 177 words and 100 dimensions: 230100 bytes\n",
      "2018-02-03 06:10:49,667 : INFO : resetting layer weights\n",
      "2018-02-03 06:10:49,672 : INFO : training model with 3 workers on 177 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2018-02-03 06:10:49,675 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-02-03 06:10:49,676 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-02-03 06:10:49,677 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-02-03 06:10:49,678 : INFO : training on 4 raw words (0 effective words) took 0.0s, 0 effective words/s\n",
      "2018-02-03 06:10:49,679 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2018-02-03 06:10:49,681 : WARNING : supplied example count (2) did not equal expected count (363)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = gensim.models.Word2Vec(iter=1)  # an empty model, no training yet\n",
    "model.build_vocab(some_sentences)  # can be a non-repeatable, 1-pass generator\n",
    "other_sentences = [['first', 'sentence'], ['second', 'sentence']]\n",
    "model.train(other_sentences,total_examples=model.corpus_count,epochs=model.iter)   # can be a non-repeatable, 1-pass generator\n",
    "# You must specify an explict epochs count. The usual value is epochs=model.iter.\n",
    "# https://github.com/llSourcell/word_vectors_game_of_thrones-LIVE/issues/7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(some_sentences)\n",
    "# print(other_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-02-03 06:15:05,334 : INFO : collecting all words and their counts\n",
      "2018-02-03 06:15:05,338 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2018-02-03 06:15:05,353 : INFO : collected 2694 word types from a corpus of 6891 raw words and 363 sentences\n",
      "2018-02-03 06:15:05,357 : INFO : Loading a fresh vocabulary\n",
      "2018-02-03 06:15:05,362 : INFO : min_count=10 retains 35 unique words (1% of original 2694, drops 2659)\n",
      "2018-02-03 06:15:05,364 : INFO : min_count=10 leaves 1382 word corpus (20% of original 6891, drops 5509)\n",
      "2018-02-03 06:15:05,366 : INFO : deleting the raw counts dictionary of 2694 items\n",
      "2018-02-03 06:15:05,368 : INFO : sample=0.001 downsamples 35 most-common words\n",
      "2018-02-03 06:15:05,375 : INFO : downsampling leaves estimated 248 word corpus (18.0% of prior 1382)\n",
      "2018-02-03 06:15:05,376 : INFO : estimated required memory for 35 words and 100 dimensions: 45500 bytes\n",
      "2018-02-03 06:15:05,379 : INFO : resetting layer weights\n",
      "2018-02-03 06:15:05,386 : INFO : training model with 3 workers on 35 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2018-02-03 06:15:05,408 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-02-03 06:15:05,410 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-02-03 06:15:05,417 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-02-03 06:15:05,419 : INFO : training on 34455 raw words (1212 effective words) took 0.0s, 68001 effective words/s\n",
      "2018-02-03 06:15:05,420 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2018-02-03 06:15:05,422 : INFO : collecting all words and their counts\n",
      "2018-02-03 06:15:05,424 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2018-02-03 06:15:05,428 : INFO : collected 2694 word types from a corpus of 6891 raw words and 363 sentences\n",
      "2018-02-03 06:15:05,430 : INFO : Loading a fresh vocabulary\n",
      "2018-02-03 06:15:05,434 : INFO : min_count=5 retains 177 unique words (6% of original 2694, drops 2517)\n",
      "2018-02-03 06:15:05,436 : INFO : min_count=5 leaves 2239 word corpus (32% of original 6891, drops 4652)\n",
      "2018-02-03 06:15:05,439 : INFO : deleting the raw counts dictionary of 2694 items\n",
      "2018-02-03 06:15:05,440 : INFO : sample=0.001 downsamples 118 most-common words\n",
      "2018-02-03 06:15:05,442 : INFO : downsampling leaves estimated 1136 word corpus (50.8% of prior 2239)\n",
      "2018-02-03 06:15:05,443 : INFO : estimated required memory for 177 words and 200 dimensions: 371700 bytes\n",
      "2018-02-03 06:15:05,445 : INFO : resetting layer weights\n",
      "2018-02-03 06:15:05,452 : INFO : training model with 3 workers on 177 vocabulary and 200 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2018-02-03 06:15:05,470 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-02-03 06:15:05,472 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-02-03 06:15:05,473 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-02-03 06:15:05,474 : INFO : training on 34455 raw words (5646 effective words) took 0.0s, 425689 effective words/s\n",
      "2018-02-03 06:15:05,475 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2018-02-03 06:15:05,477 : INFO : collecting all words and their counts\n",
      "2018-02-03 06:15:05,478 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2018-02-03 06:15:05,483 : INFO : collected 2694 word types from a corpus of 6891 raw words and 363 sentences\n",
      "2018-02-03 06:15:05,484 : INFO : Loading a fresh vocabulary\n",
      "2018-02-03 06:15:05,489 : INFO : min_count=5 retains 177 unique words (6% of original 2694, drops 2517)\n",
      "2018-02-03 06:15:05,490 : INFO : min_count=5 leaves 2239 word corpus (32% of original 6891, drops 4652)\n",
      "2018-02-03 06:15:05,492 : INFO : deleting the raw counts dictionary of 2694 items\n",
      "2018-02-03 06:15:05,494 : INFO : sample=0.001 downsamples 118 most-common words\n",
      "2018-02-03 06:15:05,495 : INFO : downsampling leaves estimated 1136 word corpus (50.8% of prior 2239)\n",
      "2018-02-03 06:15:05,496 : INFO : estimated required memory for 177 words and 100 dimensions: 230100 bytes\n",
      "2018-02-03 06:15:05,498 : INFO : resetting layer weights\n",
      "2018-02-03 06:15:05,506 : INFO : training model with 4 workers on 177 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2018-02-03 06:15:05,517 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-02-03 06:15:05,523 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-02-03 06:15:05,525 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-02-03 06:15:05,526 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-02-03 06:15:05,527 : INFO : training on 34455 raw words (5660 effective words) took 0.0s, 385617 effective words/s\n",
      "2018-02-03 06:15:05,529 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2018-02-03 06:15:05,530 : INFO : collecting all words and their counts\n",
      "2018-02-03 06:15:05,532 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2018-02-03 06:15:05,537 : INFO : collected 2694 word types from a corpus of 6891 raw words and 363 sentences\n",
      "2018-02-03 06:15:05,538 : INFO : Loading a fresh vocabulary\n",
      "2018-02-03 06:15:05,543 : INFO : min_count=10 retains 35 unique words (1% of original 2694, drops 2659)\n",
      "2018-02-03 06:15:05,544 : INFO : min_count=10 leaves 1382 word corpus (20% of original 6891, drops 5509)\n",
      "2018-02-03 06:15:05,546 : INFO : deleting the raw counts dictionary of 2694 items\n",
      "2018-02-03 06:15:05,548 : INFO : sample=0.001 downsamples 35 most-common words\n",
      "2018-02-03 06:15:05,550 : INFO : downsampling leaves estimated 248 word corpus (18.0% of prior 1382)\n",
      "2018-02-03 06:15:05,551 : INFO : estimated required memory for 35 words and 200 dimensions: 73500 bytes\n",
      "2018-02-03 06:15:05,553 : INFO : resetting layer weights\n",
      "2018-02-03 06:15:05,556 : INFO : training model with 4 workers on 35 vocabulary and 200 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2018-02-03 06:15:05,589 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-02-03 06:15:05,591 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-02-03 06:15:05,592 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-02-03 06:15:05,594 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-02-03 06:15:05,596 : INFO : training on 34455 raw words (1268 effective words) took 0.0s, 55268 effective words/s\n",
      "2018-02-03 06:15:05,598 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    }
   ],
   "source": [
    "# Word2vec accepts several parameters that affect both training speed and quality.\n",
    "model = gensim.models.Word2Vec(some_sentences, min_count=10)  # default value is 5\n",
    "model = gensim.models.Word2Vec(some_sentences, size=200)  # default value is 100\n",
    "model = gensim.models.Word2Vec(some_sentences, workers=4) # default = 1 worker = no parallelization\n",
    "model = gensim.models.Word2Vec(some_sentences, min_count=10, size = 200, workers=4) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-02-03 06:16:19,248 : INFO : loading projection weights from GoogleNews-vectors-negative300.bin\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'GoogleNews-vectors-negative300.bin'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-5529e501504d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mKeyedVectors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mvecfile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'GoogleNews-vectors-negative300.bin'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mvecs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mKeyedVectors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_word2vec_format\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvecfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbinary\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mload_word2vec_format\u001b[0;34m(cls, fname, fvocab, binary, encoding, unicode_errors, limit, datatype)\u001b[0m\n\u001b[1;32m    193\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m         \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"loading projection weights from %s\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 195\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msmart_open\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfin\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    196\u001b[0m             \u001b[0mheader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_unicode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfin\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m             \u001b[0mvocab_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvector_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mheader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# throws for invalid file format\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/smart_open/smart_open_lib.py\u001b[0m in \u001b[0;36msmart_open\u001b[0;34m(uri, mode, **kw)\u001b[0m\n\u001b[1;32m    174\u001b[0m             \u001b[0mencoding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'encoding'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m             \u001b[0merrors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'errors'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDEFAULT_ERRORS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 176\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfile_smart_open\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparsed_uri\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muri_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    177\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mparsed_uri\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscheme\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"s3\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"s3n\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m's3u'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0ms3_open_uri\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparsed_uri\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/smart_open/smart_open_lib.py\u001b[0m in \u001b[0;36mfile_smart_open\u001b[0;34m(fname, mode, encoding, errors)\u001b[0m\n\u001b[1;32m    669\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    670\u001b[0m         \u001b[0mraw_mode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 671\u001b[0;31m     \u001b[0mraw_fobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mraw_mode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    672\u001b[0m     \u001b[0mdecompressed_fobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompression_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_fobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mraw_mode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    673\u001b[0m     \u001b[0mdecoded_fobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoding_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdecompressed_fobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'GoogleNews-vectors-negative300.bin'"
     ]
    }
   ],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "vecfile = 'data/GoogleNews-vectors-negative300.bin'\n",
    "vecs = KeyedVectors.load_word2vec_format(vecfile, binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
