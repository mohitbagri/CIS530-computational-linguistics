{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['human', 'interface', 'computer'],\n",
      " ['survey', 'user', 'computer', 'system', 'response', 'time'],\n",
      " ['eps', 'user', 'interface', 'system'],\n",
      " ['system', 'human', 'system', 'eps'],\n",
      " ['user', 'response', 'time'],\n",
      " ['trees'],\n",
      " ['graph', 'trees'],\n",
      " ['graph', 'minors', 'trees'],\n",
      " ['graph', 'minors', 'survey']]\n"
     ]
    }
   ],
   "source": [
    "from gensim import corpora\n",
    "documents = [\"Human machine interface for lab abc computer applications\", \\\n",
    "            \"A survey of user opinion of computer system response time\", \\\n",
    "            \"The EPS user interface management system\", \\\n",
    "            \"System and human system engineering testing of EPS\", \\\n",
    "            \"Relation of user perceived response time to error measurement\", \\\n",
    "            \"The generation of random binary unordered trees\", \\\n",
    "            \"The intersection graph of paths in trees\", \\\n",
    "            \"Graph minors IV Widths of trees and well quasi ordering\", \\\n",
    "            \"Graph minors A survey\"]\n",
    "# remove common words and tokenize\n",
    "stoplist = set('for a of the and to in'.split())\n",
    "texts = [[word for word in document.lower().split() if word not in stoplist] for document in documents]\n",
    "# remove words that appear only once\n",
    "from collections import defaultdict\n",
    "frequency = defaultdict(int)\n",
    "for text in texts:\n",
    "    for token in text:\n",
    "        frequency[token] += 1\n",
    "texts = [[token for token in text if frequency[token] > 1]for text in texts]\n",
    "from pprint import pprint  # pretty-printer\n",
    "pprint(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary(12 unique tokens: ['computer', 'human', 'interface', 'response', 'survey']...)\n",
      "{'computer': 0, 'human': 1, 'interface': 2, 'response': 3, 'survey': 4, 'system': 5, 'time': 6, 'user': 7, 'eps': 8, 'trees': 9, 'graph': 10, 'minors': 11}\n"
     ]
    }
   ],
   "source": [
    "dictionary = corpora.Dictionary(texts)\n",
    "dictionary.save('/tmp/deerwester.dict')  # store the dictionary, for future reference\n",
    "print(dictionary)\n",
    "print(dictionary.token2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[(0, 1), (1, 1), (2, 1)], [(0, 1), (3, 1), (4, 1), (5, 1), (6, 1), (7, 1)], [(2, 1), (5, 1), (7, 1), (8, 1)], [(1, 1), (5, 2), (8, 1)], [(3, 1), (6, 1), (7, 1)], [(9, 1)], [(9, 1), (10, 1)], [(9, 1), (10, 1), (11, 1)], [(4, 1), (10, 1), (11, 1)]]\n"
     ]
    }
   ],
   "source": [
    "corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "corpora.MmCorpus.serialize('/tmp/deerwester.mm', corpus)  # store to disk, for later use\n",
    "print(corpus) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary(12 unique tokens: ['computer', 'human', 'interface', 'response', 'survey']...)\n"
     ]
    }
   ],
   "source": [
    "# https://radimrehurek.com/gensim/tut1.html\n",
    "from gensim import corpora, models, similarities\n",
    "corpus = [[(0, 1.0), (1, 1.0), (2, 1.0)],  \\\n",
    "          [(2, 1.0), (3, 1.0), (4, 1.0), (5, 1.0), (6, 1.0), (8, 1.0)], \\\n",
    "          [(1, 1.0), (3, 1.0), (4, 1.0), (7, 1.0)], [(0, 1.0), (4, 2.0), (7, 1.0)],\\\n",
    "          [(3, 1.0), (5, 1.0), (6, 1.0)],  [(9, 1.0)],  [(9, 1.0), (10, 1.0)], \\\n",
    "          [(9, 1.0), (10, 1.0), (11, 1.0)], [(8, 1.0), (10, 1.0), (11, 1.0)]]\n",
    "from six import iteritems\n",
    "# collect statistics about all tokens\n",
    "dictionary = corpora.Dictionary(line.lower().split() for line in open('mycorpus.txt'))\n",
    "# remove stop words and words that appear only once\n",
    "stop_ids = [dictionary.token2id[stopword] for stopword in stoplist if stopword in dictionary.token2id]\n",
    "once_ids = [tokenid for tokenid, docfreq in iteritems(dictionary.dfs) if docfreq == 1]\n",
    "dictionary.filter_tokens(stop_ids + once_ids)  # remove stop words and words that appear only once\n",
    "dictionary.compactify()  # remove gaps in id sequence after words that were removed\n",
    "print(dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [[(1, 0.5)], []]\n",
    "corpora.MmCorpus.serialize('/tmp/corpus.mm', corpus)\n",
    "corpora.SvmLightCorpus.serialize('/tmp/corpus.svmlight', corpus)\n",
    "corpora.BleiCorpus.serialize('/tmp/corpus.lda-c', corpus)\n",
    "corpora.LowCorpus.serialize('/tmp/corpus.low', corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = corpora.MmCorpus('/tmp/corpus.mm')\n",
    "# print(corpus)\n",
    "# print(list(corpus))\n",
    "corpora.BleiCorpus.serialize('/tmp/corpus.lda-c', corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-02-03 06:08:33,841 : INFO : loading Dictionary object from /tmp/deerwester.dict\n",
      "2018-02-03 06:08:33,843 : INFO : loaded /tmp/deerwester.dict\n",
      "2018-02-03 06:08:33,846 : INFO : loaded corpus index from /tmp/deerwester.mm.index\n",
      "2018-02-03 06:08:33,847 : INFO : initializing corpus reader from /tmp/deerwester.mm\n",
      "2018-02-03 06:08:33,849 : INFO : accepted corpus with 9 documents, 12 features, 28 non-zero entries\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Used files generated from first tutorial\n"
     ]
    }
   ],
   "source": [
    "# https://radimrehurek.com/gensim/tut2.html\n",
    "import logging, os\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "\n",
    "from gensim import corpora, models, similarities\n",
    "if (os.path.exists(\"/tmp/deerwester.dict\")):\n",
    "    dictionary = corpora.Dictionary.load('/tmp/deerwester.dict')\n",
    "    corpus = corpora.MmCorpus('/tmp/deerwester.mm')\n",
    "    print(\"Used files generated from first tutorial\")\n",
    "else: print(\"Please run first tutorial to generate data set\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-02-03 06:08:33,866 : INFO : collecting document frequencies\n",
      "2018-02-03 06:08:33,870 : INFO : PROGRESS: processing document #0\n",
      "2018-02-03 06:08:33,873 : INFO : calculating IDF weights for 9 documents and 11 features (28 matrix non-zeros)\n",
      "2018-02-03 06:08:33,878 : INFO : using serial LSI version on this node\n",
      "2018-02-03 06:08:33,880 : INFO : updating model with new documents\n",
      "2018-02-03 06:08:33,882 : INFO : preparing a new chunk of documents\n",
      "2018-02-03 06:08:33,884 : INFO : using 100 extra samples and 2 power iterations\n",
      "2018-02-03 06:08:33,886 : INFO : 1st phase: constructing (12, 102) action matrix\n",
      "2018-02-03 06:08:33,887 : INFO : orthonormalizing (12, 102) action matrix\n",
      "2018-02-03 06:08:33,891 : INFO : 2nd phase: running dense svd on (12, 9) matrix\n",
      "2018-02-03 06:08:33,894 : INFO : computing the final decomposition\n",
      "2018-02-03 06:08:33,896 : INFO : keeping 2 factors (discarding 47.565% of energy spectrum)\n",
      "2018-02-03 06:08:33,898 : INFO : processed documents up to #9\n",
      "2018-02-03 06:08:33,900 : INFO : topic #0(1.594): 0.703*\"trees\" + 0.538*\"graph\" + 0.402*\"minors\" + 0.187*\"survey\" + 0.061*\"system\" + 0.060*\"response\" + 0.060*\"time\" + 0.058*\"user\" + 0.049*\"computer\" + 0.035*\"interface\"\n",
      "2018-02-03 06:08:33,901 : INFO : topic #1(1.476): -0.460*\"system\" + -0.373*\"user\" + -0.332*\"eps\" + -0.328*\"interface\" + -0.320*\"response\" + -0.320*\"time\" + -0.293*\"computer\" + -0.280*\"human\" + -0.171*\"survey\" + 0.161*\"trees\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0.7071067811865476), (1, 0.7071067811865476)]\n",
      "[(0, 0.5773502691896257), (1, 0.5773502691896257), (2, 0.5773502691896257)]\n",
      "[(0, 0.44424552527467476), (3, 0.44424552527467476), (4, 0.44424552527467476), (5, 0.3244870206138555), (6, 0.44424552527467476), (7, 0.3244870206138555)]\n",
      "[(2, 0.5710059809418182), (5, 0.4170757362022777), (7, 0.4170757362022777), (8, 0.5710059809418182)]\n",
      "[(1, 0.49182558987264147), (5, 0.7184811607083769), (8, 0.49182558987264147)]\n",
      "[(3, 0.6282580468670046), (6, 0.6282580468670046), (7, 0.45889394536615247)]\n",
      "[(9, 1.0)]\n",
      "[(9, 0.7071067811865475), (10, 0.7071067811865475)]\n",
      "[(9, 0.5080429008916749), (10, 0.5080429008916749), (11, 0.695546419520037)]\n",
      "[(4, 0.6282580468670046), (10, 0.45889394536615247), (11, 0.6282580468670046)]\n"
     ]
    }
   ],
   "source": [
    "tfidf = models.TfidfModel(corpus) # step 1 -- initialize a model\n",
    "doc_bow = [(0, 1), (1, 1)]\n",
    "print(tfidf[doc_bow]) # step 2 -- use the model to transform vectors\n",
    "corpus_tfidf = tfidf[corpus]\n",
    "for doc in corpus_tfidf: print(doc)\n",
    "lsi = models.LsiModel(corpus_tfidf, id2word=dictionary, num_topics=2) # initialize an LSI transformation\n",
    "corpus_lsi = lsi[corpus_tfidf] # create a double wrapper over the original corpus: bow->tfidf->fold-in-lsi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-02-03 06:08:33,913 : INFO : topic #0(1.594): 0.703*\"trees\" + 0.538*\"graph\" + 0.402*\"minors\" + 0.187*\"survey\" + 0.061*\"system\" + 0.060*\"response\" + 0.060*\"time\" + 0.058*\"user\" + 0.049*\"computer\" + 0.035*\"interface\"\n",
      "2018-02-03 06:08:33,915 : INFO : topic #1(1.476): -0.460*\"system\" + -0.373*\"user\" + -0.332*\"eps\" + -0.328*\"interface\" + -0.320*\"response\" + -0.320*\"time\" + -0.293*\"computer\" + -0.280*\"human\" + -0.171*\"survey\" + 0.161*\"trees\"\n",
      "2018-02-03 06:08:33,921 : INFO : saving Projection object under /tmp/model.lsi.projection, separately None\n",
      "2018-02-03 06:08:33,927 : INFO : saved /tmp/model.lsi.projection\n",
      "2018-02-03 06:08:33,928 : INFO : saving LsiModel object under /tmp/model.lsi, separately None\n",
      "2018-02-03 06:08:33,930 : INFO : not storing attribute projection\n",
      "2018-02-03 06:08:33,931 : INFO : not storing attribute dispatcher\n",
      "2018-02-03 06:08:33,933 : INFO : saved /tmp/model.lsi\n",
      "2018-02-03 06:08:33,934 : INFO : loading LsiModel object from /tmp/model.lsi\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0.0660078339609029), (1, -0.5200703306361849)]\n",
      "[(0, 0.1966759285914242), (1, -0.7609563167700055)]\n",
      "[(0, 0.08992639972446287), (1, -0.7241860626752509)]\n",
      "[(0, 0.07585847652178014), (1, -0.6320551586003428)]\n",
      "[(0, 0.10150299184980074), (1, -0.5737308483002964)]\n",
      "[(0, 0.7032108939378318), (1, 0.16115180214025732)]\n",
      "[(0, 0.8774787673119837), (1, 0.1675890686465932)]\n",
      "[(0, 0.9098624686818583), (1, 0.14086553628718912)]\n",
      "[(0, 0.6165825350569281), (1, -0.05392907566389463)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-02-03 06:08:33,937 : INFO : loading id2word recursively from /tmp/model.lsi.id2word.* with mmap=None\n",
      "2018-02-03 06:08:33,939 : INFO : setting ignored attribute projection to None\n",
      "2018-02-03 06:08:33,940 : INFO : setting ignored attribute dispatcher to None\n",
      "2018-02-03 06:08:33,942 : INFO : loaded /tmp/model.lsi\n",
      "2018-02-03 06:08:33,943 : INFO : loading LsiModel object from /tmp/model.lsi.projection\n",
      "2018-02-03 06:08:33,945 : INFO : loaded /tmp/model.lsi.projection\n"
     ]
    }
   ],
   "source": [
    "lsi.print_topics(2)\n",
    "for doc in corpus_lsi: # both bow->tfidf and tfidf->lsi transformations are actually executed here, on the fly\n",
    "    print(doc)\n",
    "lsi.save('/tmp/model.lsi') # same for tfidf, lda, ...\n",
    "lsi = models.LsiModel.load('/tmp/model.lsi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-02-03 06:08:33,956 : INFO : loading Dictionary object from /tmp/deerwester.dict\n",
      "2018-02-03 06:08:33,958 : INFO : loaded /tmp/deerwester.dict\n",
      "2018-02-03 06:08:33,960 : INFO : loaded corpus index from /tmp/deerwester.mm.index\n",
      "2018-02-03 06:08:33,961 : INFO : initializing corpus reader from /tmp/deerwester.mm\n",
      "2018-02-03 06:08:33,963 : INFO : accepted corpus with 9 documents, 12 features, 28 non-zero entries\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MmCorpus(9 documents, 12 features, 28 non-zero entries)\n"
     ]
    }
   ],
   "source": [
    "# https://radimrehurek.com/gensim/tut3.html\n",
    "from gensim import corpora, models, similarities\n",
    "dictionary = corpora.Dictionary.load('/tmp/deerwester.dict')\n",
    "corpus = corpora.MmCorpus('/tmp/deerwester.mm') # comes from the first tutorial, \"From strings to vectors\"\n",
    "print(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-02-03 06:08:33,977 : INFO : using serial LSI version on this node\n",
      "2018-02-03 06:08:33,979 : INFO : updating model with new documents\n",
      "2018-02-03 06:08:33,981 : INFO : preparing a new chunk of documents\n",
      "2018-02-03 06:08:33,983 : INFO : using 100 extra samples and 2 power iterations\n",
      "2018-02-03 06:08:33,984 : INFO : 1st phase: constructing (12, 102) action matrix\n",
      "2018-02-03 06:08:33,986 : INFO : orthonormalizing (12, 102) action matrix\n",
      "2018-02-03 06:08:33,990 : INFO : 2nd phase: running dense svd on (12, 9) matrix\n",
      "2018-02-03 06:08:33,991 : INFO : computing the final decomposition\n",
      "2018-02-03 06:08:33,993 : INFO : keeping 2 factors (discarding 43.156% of energy spectrum)\n",
      "2018-02-03 06:08:33,994 : INFO : processed documents up to #9\n",
      "2018-02-03 06:08:33,995 : INFO : topic #0(3.341): 0.644*\"system\" + 0.404*\"user\" + 0.301*\"eps\" + 0.265*\"time\" + 0.265*\"response\" + 0.240*\"computer\" + 0.221*\"human\" + 0.206*\"survey\" + 0.198*\"interface\" + 0.036*\"graph\"\n",
      "2018-02-03 06:08:33,997 : INFO : topic #1(2.542): -0.623*\"graph\" + -0.490*\"trees\" + -0.451*\"minors\" + -0.274*\"survey\" + 0.167*\"system\" + 0.141*\"eps\" + 0.113*\"human\" + -0.107*\"response\" + -0.107*\"time\" + 0.072*\"interface\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0.4618210045327159), (1, 0.07002766527899992)]\n"
     ]
    }
   ],
   "source": [
    "lsi = models.LsiModel(corpus, id2word=dictionary, num_topics=2)\n",
    "doc = \"Human computer interaction\"\n",
    "vec_bow = dictionary.doc2bow(doc.lower().split())\n",
    "vec_lsi = lsi[vec_bow] # convert the query to LSI space\n",
    "print(vec_lsi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-02-03 06:08:34,014 : WARNING : scanning corpus to determine the number of features (consider setting `num_features` explicitly)\n",
      "2018-02-03 06:08:34,016 : INFO : creating matrix with 9 documents and 2 features\n",
      "2018-02-03 06:08:34,020 : INFO : saving MatrixSimilarity object under /tmp/deerwester.index, separately None\n",
      "2018-02-03 06:08:34,022 : INFO : saved /tmp/deerwester.index\n",
      "2018-02-03 06:08:34,023 : INFO : loading MatrixSimilarity object from /tmp/deerwester.index\n",
      "2018-02-03 06:08:34,025 : INFO : loaded /tmp/deerwester.index\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0.998093), (1, 0.93748635), (2, 0.9984453), (3, 0.9865886), (4, 0.90755945), (5, -0.12416792), (6, -0.10639259), (7, -0.09879463), (8, 0.05004177)]\n",
      "[(2, 0.9984453), (0, 0.998093), (3, 0.9865886), (1, 0.93748635), (4, 0.90755945), (8, 0.05004177), (7, -0.09879463), (6, -0.10639259), (5, -0.12416792)]\n"
     ]
    }
   ],
   "source": [
    "index = similarities.MatrixSimilarity(lsi[corpus])\n",
    "index.save('/tmp/deerwester.index')\n",
    "index = similarities.MatrixSimilarity.load('/tmp/deerwester.index')\n",
    "sims = index[vec_lsi] # perform a similarity query against the corpus\n",
    "print(list(enumerate(sims)))\n",
    "sims = sorted(enumerate(sims), key=lambda item: -item[1])\n",
    "print(sims) # print sorted (document number, similarity score) 2-tuples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-02-03 06:08:34,040 : INFO : collecting all words and their counts\n",
      "2018-02-03 06:08:34,041 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2018-02-03 06:08:34,043 : INFO : collected 3 word types from a corpus of 4 raw words and 2 sentences\n",
      "2018-02-03 06:08:34,043 : INFO : Loading a fresh vocabulary\n",
      "2018-02-03 06:08:34,045 : INFO : min_count=1 retains 3 unique words (100% of original 3, drops 0)\n",
      "2018-02-03 06:08:34,046 : INFO : min_count=1 leaves 4 word corpus (100% of original 4, drops 0)\n",
      "2018-02-03 06:08:34,048 : INFO : deleting the raw counts dictionary of 3 items\n",
      "2018-02-03 06:08:34,050 : INFO : sample=0.001 downsamples 3 most-common words\n",
      "2018-02-03 06:08:34,051 : INFO : downsampling leaves estimated 0 word corpus (5.7% of prior 4)\n",
      "2018-02-03 06:08:34,053 : INFO : estimated required memory for 3 words and 100 dimensions: 3900 bytes\n",
      "2018-02-03 06:08:34,054 : INFO : resetting layer weights\n",
      "2018-02-03 06:08:34,056 : INFO : training model with 3 workers on 3 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2018-02-03 06:08:34,059 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-02-03 06:08:34,060 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-02-03 06:08:34,061 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-02-03 06:08:34,062 : INFO : training on 20 raw words (0 effective words) took 0.0s, 0 effective words/s\n",
      "2018-02-03 06:08:34,063 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    }
   ],
   "source": [
    "# https://rare-technologies.com/word2vec-tutorial/\n",
    "import gensim, logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    " \n",
    "other_sentences = [['first', 'sentence'], ['second', 'sentence']]\n",
    "# train word2vec on the two other_sentences\n",
    "model = gensim.models.Word2Vec(other_sentences, min_count=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-02-03 06:08:34,088 : INFO : collecting all words and their counts\n",
      "2018-02-03 06:08:34,090 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2018-02-03 06:08:34,096 : INFO : collected 2694 word types from a corpus of 6891 raw words and 363 sentences\n",
      "2018-02-03 06:08:34,097 : INFO : Loading a fresh vocabulary\n",
      "2018-02-03 06:08:34,101 : INFO : min_count=5 retains 177 unique words (6% of original 2694, drops 2517)\n",
      "2018-02-03 06:08:34,102 : INFO : min_count=5 leaves 2239 word corpus (32% of original 6891, drops 4652)\n",
      "2018-02-03 06:08:34,104 : INFO : deleting the raw counts dictionary of 2694 items\n",
      "2018-02-03 06:08:34,105 : INFO : sample=0.001 downsamples 118 most-common words\n",
      "2018-02-03 06:08:34,106 : INFO : downsampling leaves estimated 1136 word corpus (50.8% of prior 2239)\n",
      "2018-02-03 06:08:34,107 : INFO : estimated required memory for 177 words and 100 dimensions: 230100 bytes\n",
      "2018-02-03 06:08:34,109 : INFO : resetting layer weights\n",
      "2018-02-03 06:08:34,114 : INFO : training model with 3 workers on 177 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2018-02-03 06:08:34,127 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-02-03 06:08:34,128 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-02-03 06:08:34,130 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-02-03 06:08:34,131 : INFO : training on 34455 raw words (5597 effective words) took 0.0s, 490100 effective words/s\n",
      "2018-02-03 06:08:34,132 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    }
   ],
   "source": [
    "class MySentences(object):\n",
    "    def __init__(self, dirname):\n",
    "        self.dirname = dirname\n",
    " \n",
    "    def __iter__(self):\n",
    "        for fname in os.listdir(self.dirname):\n",
    "            for line in open(os.path.join(self.dirname, fname)):\n",
    "                yield line.split()\n",
    " \n",
    "some_sentences = list(MySentences('yezheng_test/')) # a memory-friendly iterator\n",
    "model = gensim.models.Word2Vec(some_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-02-03 06:08:34,144 : INFO : collecting all words and their counts\n",
      "2018-02-03 06:08:34,146 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2018-02-03 06:08:34,149 : INFO : collected 2694 word types from a corpus of 6891 raw words and 363 sentences\n",
      "2018-02-03 06:08:34,150 : INFO : Loading a fresh vocabulary\n",
      "2018-02-03 06:08:34,153 : INFO : min_count=5 retains 177 unique words (6% of original 2694, drops 2517)\n",
      "2018-02-03 06:08:34,154 : INFO : min_count=5 leaves 2239 word corpus (32% of original 6891, drops 4652)\n",
      "2018-02-03 06:08:34,157 : INFO : deleting the raw counts dictionary of 2694 items\n",
      "2018-02-03 06:08:34,158 : INFO : sample=0.001 downsamples 118 most-common words\n",
      "2018-02-03 06:08:34,159 : INFO : downsampling leaves estimated 1136 word corpus (50.8% of prior 2239)\n",
      "2018-02-03 06:08:34,160 : INFO : estimated required memory for 177 words and 100 dimensions: 230100 bytes\n",
      "2018-02-03 06:08:34,161 : INFO : resetting layer weights\n",
      "2018-02-03 06:08:34,165 : INFO : training model with 3 workers on 177 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2018-02-03 06:08:34,170 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-02-03 06:08:34,171 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-02-03 06:08:34,172 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-02-03 06:08:34,174 : INFO : training on 4 raw words (0 effective words) took 0.0s, 0 effective words/s\n",
      "2018-02-03 06:08:34,174 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2018-02-03 06:08:34,175 : WARNING : supplied example count (2) did not equal expected count (363)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = gensim.models.Word2Vec(iter=1)  # an empty model, no training yet\n",
    "model.build_vocab(some_sentences)  # can be a non-repeatable, 1-pass generator\n",
    "model.train(other_sentences,total_examples=model.corpus_count,epochs=model.iter)   # can be a non-repeatable, 1-pass generator\n",
    "# You must specify an explict epochs count. The usual value is epochs=model.iter.\n",
    "# https://github.com/llSourcell/word_vectors_game_of_thrones-LIVE/issues/7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import time\n",
    "# T0 = time.time()\n",
    "# import os, re\n",
    "# class MySentences(object):\n",
    "#     def __init__(self, dirname):\n",
    "#         self.dirname = dirname\n",
    " \n",
    "#     def __iter__(self):\n",
    "#         for fname in os.listdir(self.dirname):\n",
    "#             for line in open(os.path.join(self.dirname, fname)):\n",
    "#                 yield line.split()\n",
    "# sentences = list(MySentences('yezheng_test/'))\n",
    "# vocab = set()\n",
    "# for s in sentences:\n",
    "#     Ltemp = [re.split('[0-9.]+',w)[0] for w in s if not '::' == w]\n",
    "#     Ltemp = [w for w in Ltemp if len(w) >0]\n",
    "#     for ele in Ltemp[1:]: vocab.add(ele)\n",
    "# print(len(vocab))\n",
    "\n",
    "# with open(\"Vocab.txt\",'w') as f:\n",
    "#     for w in vocab: f.write(w+'\\n')\n",
    "# print(time.time() - T0)\n",
    "# print(len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.23245024681091309\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "T0 = time.time()\n",
    "import os, re\n",
    "class MySentences(object):\n",
    "    def __init__(self, dirname):\n",
    "        self.dirname = dirname\n",
    " \n",
    "    def __iter__(self):\n",
    "        for fname in os.listdir(self.dirname):\n",
    "            for line in open(os.path.join(self.dirname, fname)):\n",
    "                yield line.split()\n",
    "sentences = list(MySentences('yezheng_test/'))\n",
    "vocab = set()\n",
    "with open(\"data/coocvec-500mostfreq-window-3.vec.filter\") as fread:\n",
    "    line = fread.readline()\n",
    "    for line in fread: vocab.add(line.split()[0])\n",
    "with open(\"data/GoogleNews-vectors-negative300.filter\") as fread:\n",
    "    line = fread.readline()\n",
    "    for line in fread: vocab.add(line.split()[0])\n",
    "\n",
    "with open(\"Vocab2.txt\",'w') as f:\n",
    "    for w in vocab: f.write(w+'\\n')\n",
    "print(len(vocab))\n",
    "print(time.time() - T0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calinski-Harabasz Score 14907.099436228207\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "X, y = datasets.make_blobs(n_samples=500, n_features=6, centers=5, cluster_std=[0.4, 0.3, 0.4, 0.3, 0.4], random_state=11)\n",
    "from sklearn.cluster import SpectralClustering\n",
    "y_pred = SpectralClustering().fit_predict(X)\n",
    "from sklearn import metrics\n",
    "print(\"Calinski-Harabasz Score\", metrics.calinski_harabaz_score(X, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3 6 6 0 1 1 2 0 1 1 5 3 1 1 1 0 1 2 2 2 4 4 5 2 4 7 7 7 6 3 3 2 1 6 2 3 3\n",
      " 0 4 7 7 2 2 1 0 4 3 3 2 4 4 5 0 7 5 2 4 4 7 0 3 4 4 4 2 6 7 4 4 7 4 2 2 5\n",
      " 6 2 0 2 7 3 1 7 4 4 7 6 1 6 1 3 0 1 1 4 7 2 2 1 3 6 7 3 0 5 2 7 5 5 6 2 1\n",
      " 5 7 0 0 4 3 7 7 1 3 1 2 6 6 5 5 2 3 7 4 0 6 4 2 2 2 2 6 2 4 3 4 4 4 4 4 3\n",
      " 0 2 4 3 2 5 0 4 2 2 2 4 1 2 3 7 4 3 0 0 2 4 4 1 2 7 7 2 7 4 4 0 7 1 7 1 2\n",
      " 6 5 1 4 2 4 2 4 1 0 5 1 0 5 2 4 4 5 4 3 0 4 6 0 4 2 3 2 1 0 3 4 4 7 4 2 2\n",
      " 3 2 5 5 1 3 6 2 1 2 5 4 2 7 1 2 6 1 4 1 2 2 1 7 2 5 0 0 3 1 0 4 1 2 4 2 2\n",
      " 3 2 3 2 2 2 4 5 5 7 1 4 4 7 5 2 1 2 5 6 5 7 4 1 2 3 2 4 0 3 4 3 4 4 2 4 4\n",
      " 4 0 2 5 3 2 3 6 3 2 2 3 6 7 1 4 1 7 2 4 2 2 2 1 3 0 2 1 2 5 5 2 7 4 1 0 2\n",
      " 1 1 7 3 0 7 3 5 6 6 3 4 7 2 4 7 4 3 6 2 4 6 1 3 2 0 4 2 5 4 0 7 0 3 4 1 7\n",
      " 4 3 6 3 4 0 4 5 5 5 3 5 1 1 4 2 4 7 5 4 1 2 6 0 4 1 6 2 3 2 2 4 4 1 5 4 5\n",
      " 6 1 0 6 4 1 2 5 4 7 7 4 4 4 0 7 1 5 4 2 1 3 5 2 0 3 6 5 2 0 3 6 3 1 4 2 2\n",
      " 6 4 5 4 4 5 7 1 6 4 1 0 7 6 5 2 7 4 2 2 1 4 7 7 7 7 6 0 6 0 4 4 7 0 5 2 3\n",
      " 5 2 6 6 6 2 2 5 2 4 4 0 7 3 4 2 4 0 0]\n"
     ]
    }
   ],
   "source": [
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Vocab' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-756bb98cbf7e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mVocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'Vocab' is not defined"
     ]
    }
   ],
   "source": [
    "print(len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
