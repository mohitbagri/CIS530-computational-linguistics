{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import conll2002\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "# import sklearn_crfsuite\n",
    "from sklearn.linear_model import Perceptron, LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.ensemble import AdaBoostClassifier, GradientBoostingClassifier\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import sklearn\n",
    "import time\n",
    "from itertools import groupby"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assignment 7: NER\n",
    "# This is just to help you get going. Feel free to\n",
    "# add to or modify any part of it.\n",
    "\n",
    "\n",
    "def getfeats(word, o, tag): \n",
    "    #if just word 61.08 in F1-score (window size = 2,3; optimal window size = 2)\n",
    "    #if just word_prefix 36.54 in F1-score (window size = 2, 10,20; optimal window size = 20)\n",
    "    \"\"\" This takes the word in question and\n",
    "    the offset with respect to the instance\n",
    "    word \"\"\"\n",
    "    \n",
    "    o = str(o)\n",
    "    shape_feature = ['X' if ch.isupper() else ch for ch in word]\n",
    "    shape_feature = \"\".join(['x' if ch.islower() else ch for ch in shape_feature]) #if just shape_feature 36.19 in F1-score (best amongst window size = 2,3,5,10; optimal window size = 10)\n",
    "    short_shape_feature = \"\".join([x[0] for x in groupby(shape_feature)]) # if just short_shape_feature 29.43 in F1-score (window size = 2,5,10; optimal window size = 5)\n",
    "    features = [\n",
    "        (o + 'word', word)\n",
    "#         , (o+\"PoS\", tag) #yezheng: I like tags\n",
    "#         (o + 'word', shape_feature),\n",
    "#         (o + 'word', short_shape_feature)\n",
    "#         (o + 'word_prefix', word[0])\n",
    "        \n",
    "        # TODO: add more features here.\n",
    "    ]\n",
    "    return features\n",
    "\n",
    "def getfeats_tag(word, o, tag): \n",
    "    #if just word 61.08 in F1-score (window size = 2,3; optimal window size = 2)\n",
    "    #if just word_prefix 36.54 in F1-score (window size = 2, 10,20; optimal window size = 20)\n",
    "    \"\"\" This takes the word in question and\n",
    "    the offset with respect to the instance\n",
    "    word \"\"\"\n",
    "    o = str(o)\n",
    "    shape_feature = ['X' if ch.isupper() else ch for ch in word]\n",
    "    shape_feature = \"\".join(['x' if ch.islower() else ch for ch in shape_feature]) #if just shape_feature 36.19 in F1-score (best amongst window size = 2,3,5,10; optimal window size = 10)\n",
    "    short_shape_feature = \"\".join([x[0] for x in groupby(shape_feature)]) # if just short_shape_feature 29.43 in F1-score (window size = 2,5,10; optimal window size = 5)\n",
    "    features = [ (o+\"PoS\", tag) #yezheng: I like tags\n",
    "#         (o + 'word_shape', shape_feature),\n",
    "#         (o + 'word_short_shape', short_shape_feature)\n",
    "#         (o + 'word_prefix', word[0])\n",
    "        \n",
    "        # TODO: add more features here.\n",
    "    ]\n",
    "    return features\n",
    "    \n",
    "    \n",
    "def word2features(sent, i, DEBUG_FLAG = False):\n",
    "    \"\"\" The function generates all features\n",
    "    for the word at position i in the\n",
    "    sentence.\"\"\"\n",
    "    features = []\n",
    "    # the window around the token\n",
    "    win_size = 4 # results2 results3\n",
    "    win_size_tag = 10\n",
    "    for o in range(-win_size,win_size+1):\n",
    "#     for o in [-1,0,1]: \n",
    "        if i+o >= 0 and i+o < len(sent):\n",
    "            featlist = getfeats(sent[i+o][0], o, sent[i+o][1])\n",
    "            features.extend(featlist)\n",
    "    for o in range(-win_size_tag,win_size_tag + 1):\n",
    "        if i+o >= 0 and i+o < len(sent):\n",
    "            featlist = getfeats_tag(sent[i+o][0], o, sent[i+o][1])\n",
    "            features.extend(featlist)\n",
    "    if DEBUG_FLAG: print(\"sent\",sent,\"i\",i,\"features\",features)\n",
    "    return dict(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time: 36.54191589355469\n",
      "Elapsed time: 136.66878581047058\n"
     ]
    }
   ],
   "source": [
    "# if __name__ == \"__main__\":\n",
    "# Load the training data\n",
    "train_sents = list(conll2002.iob_sents('esp.train'))\n",
    "#--------\n",
    "# with open(\"content_train.txt\",'w')  as f:\n",
    "#     for sent in train_sents:\n",
    "#         f.write(\" \".join([ele[0] for ele in sent]) + '\\n')\n",
    "#--------\n",
    "train_feats = []\n",
    "train_labels = []\n",
    "\n",
    "T0 = time.time()\n",
    "for sent in train_sents:\n",
    "    train_feats += [word2features(sent,i) for i in range(len(sent))]\n",
    "    train_labels += [sent[i][-1] for i in range(len(sent))]\n",
    "    \n",
    "    \n",
    "#--------    \n",
    "# S = train_sents[0]\n",
    "# for i in range(len(sent)): word2features(S,i, DEBUG_FLAG = True)\n",
    "#--------\n",
    "# sent [('Melbourne', 'NP', 'B-LOC'), ('(', 'Fpa', 'O'), ('Australia', 'NP', 'B-LOC'), (')', 'Fpt', 'O'), (',', 'Fc', 'O'), ('25', 'Z', 'O'), ('may', 'NC', 'O'), ('(', 'Fpa', 'O'), ('EFE', 'NC', 'B-ORG'), (')', 'Fpt', 'O'), ('.', 'Fp', 'O')] i 0 features [('0word', 'Melbourne'), ('1word', '('), ('2word', 'Australia')]\n",
    "# sent [('Melbourne', 'NP', 'B-LOC'), ('(', 'Fpa', 'O'), ('Australia', 'NP', 'B-LOC'), (')', 'Fpt', 'O'), (',', 'Fc', 'O'), ('25', 'Z', 'O'), ('may', 'NC', 'O'), ('(', 'Fpa', 'O'), ('EFE', 'NC', 'B-ORG'), (')', 'Fpt', 'O'), ('.', 'Fp', 'O')] i 1 features [('-1word', 'Melbourne'), ('0word', '('), ('1word', 'Australia'), ('2word', ')')]\n",
    "# sent [('Melbourne', 'NP', 'B-LOC'), ('(', 'Fpa', 'O'), ('Australia', 'NP', 'B-LOC'), (')', 'Fpt', 'O'), (',', 'Fc', 'O'), ('25', 'Z', 'O'), ('may', 'NC', 'O'), ('(', 'Fpa', 'O'), ('EFE', 'NC', 'B-ORG'), (')', 'Fpt', 'O'), ('.', 'Fp', 'O')] i 2 features [('-2word', 'Melbourne'), ('-1word', '('), ('0word', 'Australia'), ('1word', ')'), ('2word', ',')]\n",
    "\n",
    "\n",
    "vectorizer = DictVectorizer()\n",
    "\n",
    "X_train = vectorizer.fit_transform(train_feats) \n",
    "# print(\"X_train\",X_train)\n",
    "# print(X_train)\n",
    "# X_train = X_train.toarray()\n",
    "# scaler = sklearn.preprocessing.StandardScaler(); scaler.fit(X_train); X_train = scaler.transform(X_train)\n",
    "print(\"Elapsed time:\",time.time() - T0)\n",
    "# for i in range(len(train_sents)):\n",
    "#     print( train_feats[i], X_train[i])\n",
    "# X_train = sp.sparse.bsr_matrix(np.array(X_train))\n",
    "# TODO: play with other models\n",
    "model = LogisticRegression()\n",
    "# model = Perceptron(); #model.fit(X_train, train_labels)# verbose=1# Whether to print progress messages to stdoutmodel.fit(X_train, train_labels)\n",
    "# model = LinearSVC(penalty = \"l1\");# not work\n",
    "# model = GaussianNB(); # not work\n",
    "# model = AdaBoostClassifier() # F1-score 20\n",
    "# model = GradientBoostingClassifier() # too slow\n",
    "model.fit(X_train, train_labels)\n",
    "del X_train\n",
    "print(\"Elapsed time:\",time.time() - T0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time: 5.4618048667907715\n",
      "Writing to results.txt\n",
      "Now run: python conlleval.py results.txt\n",
      "Elapsed time: 7.377550840377808\n"
     ]
    }
   ],
   "source": [
    "dev_sents = list(conll2002.iob_sents('esp.testa'))\n",
    "test_feats = []\n",
    "test_labels = []\n",
    "\n",
    "# with open(\"content_dev.txt\",'w')  as f:\n",
    "#     for sent in dev_sents:\n",
    "#         f.write(\" \".join([ele[0] for ele in sent]) + '\\n')\n",
    "\n",
    "\n",
    "T0 = time.time()\n",
    "for sent in dev_sents:\n",
    "    test_feats+=[word2features(sent,i) for i in range(len(sent))]\n",
    "    test_labels+=[sent[i][-1] for i in range(len(sent))]\n",
    "print(\"Elapsed time:\",time.time() - T0)\n",
    "\n",
    "X_test = vectorizer.transform(test_feats)#sparse\n",
    "# X_test = X_test.toarray()\n",
    "# scaler = sklearn.preprocessing.StandardScaler(); scaler.fit(X_test); X_test = scaler.transform(X_test)\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "j = 0\n",
    "print(\"Writing to results.txt\")\n",
    "# format is: word gold pred\n",
    "with open(\"results.txt\", \"w\") as out:\n",
    "    for sent in dev_sents: \n",
    "        for i in range(len(sent)):\n",
    "            word = sent[i][0]\n",
    "            gold = sent[i][-1]\n",
    "            pred = y_pred[j]\n",
    "            j += 1\n",
    "            out.write(\"{}\\t{}\\t{}\\n\".format(word,gold,pred))\n",
    "    out.write(\"\\n\")\n",
    "\n",
    "print(\"Now run: python conlleval.py results.txt\")\n",
    "del test_feats\n",
    "del test_labels\n",
    "del dev_sents\n",
    "del X_test\n",
    "del y_pred\n",
    "print(\"Elapsed time:\",time.time() - T0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time: 5.529991865158081\n",
      "Writing to results.txt\n",
      "Now run: python conlleval.py constrained_results.txt\n",
      "Elapsed time: 7.454993009567261\n"
     ]
    }
   ],
   "source": [
    "test_sents = list(conll2002.iob_sents('esp.testb'))\n",
    "#-------------\n",
    "# with open(\"content_test.txt\",'w')  as f:\n",
    "#     for sent in test_sents:\n",
    "#         f.write(\" \".join([ele[0] for ele in sent]) + '\\n')\n",
    "#-------------\n",
    "test2_feats = []\n",
    "test2_labels = []\n",
    "T0 = time.time()\n",
    "for sent in test_sents:\n",
    "    test2_feats+=[word2features(sent,i) for i in range(len(sent))]\n",
    "    test2_labels+=[sent[i][-1] for i in range(len(sent))]\n",
    "print(\"Elapsed time:\",time.time() - T0)\n",
    "\n",
    "X_test2 = vectorizer.transform(test2_feats)\n",
    "# X_test2 = X_test2.toarray()\n",
    "# scaler = sklearn.preprocessing.StandardScaler(with_mean=False); scaler.fit(X_test2); X_test2 = scaler.transform(X_test2)\n",
    "y_pred2 = model.predict(X_test2)\n",
    "j=0\n",
    "print(\"Writing to results.txt\")\n",
    "# format is: word gold pred\n",
    "with open(\"constrained_results.txt\", \"w\") as out:\n",
    "    for sent in test_sents: \n",
    "        for i in range(len(sent)):\n",
    "            word = sent[i][0]\n",
    "            gold = sent[i][-1]\n",
    "            pred = y_pred2[j]\n",
    "            j += 1\n",
    "            out.write(\"{}\\t{}\\t{}\\n\".format(word,gold,pred))\n",
    "    out.write(\"\\n\")\n",
    "\n",
    "print(\"Now run: python conlleval.py constrained_results.txt\")\n",
    "print(\"Elapsed time:\",time.time() - T0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
