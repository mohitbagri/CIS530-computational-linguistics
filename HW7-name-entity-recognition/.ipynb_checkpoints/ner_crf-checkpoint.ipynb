{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import sklearn_crfsuite\n",
    "import eli5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import conll2002\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from itertools import groupby\n",
    "import multiprocessing\n",
    "num_cores = multiprocessing.cpu_count()\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def getfeats(word, o, tag, freq):\n",
    "#     \"\"\" This takes the word in question and\n",
    "#     the offset with respect to the instance\n",
    "#     word \"\"\"\n",
    "#     #print('word',word)\n",
    "#     o = str(o)\n",
    "#     shape_feature = ['X' if ch.isupper() else ch for ch in word]\n",
    "# #     all_upper_FLAG = 1- int(shape_feature == word)\n",
    "#     shape_feature = \"\".join(['x' if ch.islower() else ch for ch in shape_feature]) #if just shape_feature 36.19 in F1-score (best amongst window size = 2,3,5,10; optimal window size = 10)\n",
    "#     short_shape_feature = \"\".join([x[0] for x in groupby(shape_feature)]) # if just short_shape_feature 29.43 in F1-score (window size = 2,5,10; optimal window size = 5)\n",
    "#     features = [   (o + 'word', word)\n",
    "#          ,(o + 'word_shape', shape_feature)\n",
    "#          ,(o + 'word_short_shape', short_shape_feature)\n",
    "#          ,(o + 'word_prefix', word[0])\n",
    "# #          (o + \"PoS\", part_of_speech),\n",
    "# #          (o + \"upper\",# len(re.findall(r'[A-Z]',word))\n",
    "# #          all_upper_FLAG),\n",
    "#          ,(o + \"hyphen\", int('-' in word ))\n",
    "#         ,(o + \"len_word\", len(word))\n",
    "#          ,(o + \"last_two\", word[-2:])\n",
    "#     ]\n",
    "\n",
    "#     if o ==0:\n",
    "#         features.append((o + \"sentence_frequency\", freq))\n",
    "#     return features\n",
    "\n",
    "# def getfeats_tag(word, o, tag): \n",
    "#     #if just word 61.08 in F1-score (window size = 2,3; optimal window size = 2)\n",
    "#     #if just word_prefix 36.54 in F1-score (window size = 2, 10,20; optimal window size = 20)\n",
    "#     \"\"\" This takes the word in question and\n",
    "#     the offset with respect to the instance\n",
    "#     word \"\"\"\n",
    "#     o = str(o)\n",
    "#     shape_feature = ['X' if ch.isupper() else ch for ch in word]\n",
    "#     all_upper_FLAG = 1- int(shape_feature == word)\n",
    "# #     shape_feature = \"\".join(['x' if ch.islower() else ch for ch in shape_feature]) #if just shape_feature 36.19 in F1-score (best amongst window size = 2,3,5,10; optimal window size = 10)\n",
    "# #     print(shape_feature)\n",
    "# #     short_shape_feature = \"\".join([x[0] for x in groupby(shape_feature)]) # if just short_shape_feature 29.43 in F1-score (window size = 2,5,10; optimal window size = 5)\n",
    "#     features = [ (o+\"PoS\", tag) #yezheng: I like tags\n",
    "# #           ,(o + 'word_shape', shape_feature),\n",
    "# #          ,(o + 'word_short_shape', short_shape_feature)\n",
    "# #          ,(o + 'word_prefix', word[0])\n",
    "# # #          (o + \"PoS\", part_of_speech)\n",
    "#          ,(o + \"upper\",# len(re.findall(r'[A-Z]',word))\n",
    "#          all_upper_FLAG)\n",
    "# #          ,(o + \"hyphen\", int('-' in word ))\n",
    "# #         ,(o + \"len_word\", len(word))\n",
    "# #          ,(o + \"last_two\", word[-2:])\n",
    "#     ]\n",
    "#     return features\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "# def word2features(sent, i):\n",
    "#     \"\"\" The function generates all features\n",
    "#     for the word at position i in the\n",
    "#     sentence.\"\"\"\n",
    "#     features = []\n",
    "#     # the window around the token\n",
    "\n",
    "#     word = sent[i][0]\n",
    "\n",
    "#     sentence = []\n",
    "#     for item in sent:\n",
    "#         word = item[0]\n",
    "#         sentence.append(word)\n",
    "#     sent_frequency = sentence.count(word)\n",
    "#     #sentence_frequency = full_text.count(word)\n",
    "\n",
    "#     win_size = 5 # results2 results3\n",
    "#     win_size_tag = 10\n",
    "#     for o in range(-win_size,win_size+1):\n",
    "#         if i+o >= 0 and i+o < len(sent):\n",
    "#             word = sent[i+o][0]\n",
    "#             tag = sent[i+o][1]\n",
    "#             featlist = getfeats(word, o, tag, sent_frequency)\n",
    "#             features.extend(featlist)\n",
    "#     for o in range(-win_size_tag,win_size_tag + 1):\n",
    "#         if i+o >= 0 and i+o < len(sent):\n",
    "#             featlist = getfeats_tag(sent[i+o][0], o, sent[i+o][1])\n",
    "#             features.extend(featlist)\n",
    "#     return dict(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def word2features(sent, i):\n",
    "    word = sent[i][0]\n",
    "    postag = sent[i][1]\n",
    "\n",
    "#     features = {\n",
    "#         'bias': 1.0,\n",
    "#         'word.lower()': word.lower(),\n",
    "#         'word[-3:]': word[-3:],\n",
    "#         'word.isupper()': word.isupper(),\n",
    "#         'word.istitle()': word.istitle(),\n",
    "#         'word.isdigit()': word.isdigit(),\n",
    "#         'postag': postag,\n",
    "#         'postag[:2]': postag[:2],\n",
    "#     }\n",
    "    features = {}\n",
    "#     if i > 0:\n",
    "#         word1 = sent[i-1][0]\n",
    "#         postag1 = sent[i-1][1]\n",
    "#         features.update({\n",
    "#             '-1:word.lower()': word1.lower(),\n",
    "#             '-1:word.istitle()': word1.istitle(),\n",
    "#             '-1:word.isupper()': word1.isupper(),\n",
    "#             '-1:postag': postag1,\n",
    "#             '-1:postag[:2]': postag1[:2],\n",
    "#         })\n",
    "#     else:\n",
    "#         features['BOS'] = True\n",
    "\n",
    "#     if i < len(sent)-1:\n",
    "#         word1 = sent[i+1][0]\n",
    "#         postag1 = sent[i+1][1]\n",
    "#         features.update({\n",
    "#             '+1:word.lower()': word1.lower(),\n",
    "#             '+1:word.istitle()': word1.istitle(),\n",
    "#             '+1:word.isupper()': word1.isupper(),\n",
    "#             '+1:postag': postag1,\n",
    "#             '+1:postag[:2]': postag1[:2],\n",
    "#         })\n",
    "    win_size = 4 # results2 results3\n",
    "    for o in range(-win_size,win_size+1):\n",
    "        if i+o >= 0 and i+o < len(sent):\n",
    "            word = sent[i+o][0]\n",
    "            postag = sent[i+o][1]\n",
    "            shape_feature = ['X' if ch.isupper() else ch for ch in word]\n",
    "            all_upper_FLAG = 1- int(shape_feature == word)\n",
    "            shape_feature = \"\".join(['x' if ch.islower() else ch for ch in shape_feature])\n",
    "            short_shape_feature = \"\".join([x[0] for x in groupby(shape_feature)])\n",
    "            o_str = str(o)\n",
    "            features.update({\n",
    "            o_str+'word.lower()': word,\n",
    "            o_str+'shape':shape_feature,\n",
    "            o_str+'short_shape':short_shape_feature,\n",
    "            o_str+'word_prefix': word[0],\n",
    "            o_str+'word.isupper': all_upper_FLAG,\n",
    "            o_str+'postag': postag,\n",
    "            o_str+'postag[:2]': postag[:2],\n",
    "            o_str+'-': int('-' in word),\n",
    "#             o_str+'len': len(word)\n",
    "        })\n",
    "    if i == len(sent)-1:\n",
    "        features['EOS'] = True\n",
    "    return features\n",
    "\n",
    "\n",
    "def sent2features(sent):\n",
    "    return [word2features(sent, i) for i in range(len(sent))]\n",
    "\n",
    "def sent2labels(sent):\n",
    "    return [label for token, postag, label in sent]\n",
    "\n",
    "def sent2tokens(sent):\n",
    "    return [token for token, postag, label in sent]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time: 18.98472762107849\n"
     ]
    }
   ],
   "source": [
    "T0 = time.time()\n",
    "train_sents = list(nltk.corpus.conll2002.iob_sents('esp.train'))\n",
    "X_train = [sent2features(s) for s in train_sents]\n",
    "y_train = [sent2labels(s) for s in train_sents]\n",
    "print(\"Elapsed time:\",time.time() - T0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time: 62.52008390426636\n"
     ]
    }
   ],
   "source": [
    "T0 =time.time()\n",
    "model = sklearn_crfsuite.CRF(\n",
    "    algorithm='lbfgs',\n",
    "    c1=200,\n",
    "    c2=0.1,\n",
    "    max_iterations=20,\n",
    "    all_possible_transitions=False,\n",
    ")\n",
    "model.fit(X_train, y_train)\n",
    "print(\"Elapsed time:\",time.time() - T0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time: 3.600120544433594e-05\n",
      "Writing to results.txt\n",
      "Now run: python conlleval.py unconstrained_results.txt\n",
      "Elapsed time: 2.032670021057129\n"
     ]
    }
   ],
   "source": [
    "\n",
    "T0 = time.time()\n",
    "\n",
    "test_sents = list(nltk.corpus.conll2002.iob_sents('esp.testb'))\n",
    "test2_feats = [sent2features(s) for s in test_sents]\n",
    "test2_labels = [sent2labels(s) for s in test_sents]\n",
    "T0 = time.time()\n",
    "print(\"Elapsed time:\",time.time() - T0)\n",
    "# X_test2 = vectorizer.transform(test2_feats)\n",
    "# X_test2 = X_test2.toarray()\n",
    "# scaler = sklearn.preprocessing.StandardScaler(with_mean=False); scaler.fit(X_test2); X_test2 = scaler.transform(X_test2)\n",
    "y_pred1 = model.predict(test2_feats)\n",
    "# print(\"y_pred2\", y_pred2)\n",
    "import itertools\n",
    "y_pred2 = list(itertools.chain(*y_pred1))\n",
    "j=0\n",
    "print(\"Writing to results.txt\")\n",
    "# format is: word gold pred\n",
    "with open(\"unconstrained_results.txt\", \"w\") as out:\n",
    "    for sent in test_sents: \n",
    "        for i in range(len(sent)):\n",
    "            word = sent[i][0]\n",
    "            gold = sent[i][-1]\n",
    "            pred = y_pred2[j]\n",
    "            j += 1\n",
    "            out.write(\"{}\\t{}\\t{}\\n\".format(word,gold,pred))\n",
    "    out.write(\"\\n\")\n",
    "\n",
    "print(\"Now run: python conlleval.py unconstrained_results.txt\")\n",
    "print(\"Elapsed time:\",time.time() - T0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
