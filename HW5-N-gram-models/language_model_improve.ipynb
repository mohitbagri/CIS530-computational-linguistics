{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training language model\n"
     ]
    }
   ],
   "source": [
    "from collections import *\n",
    "from random import random\n",
    "import numpy as np\n",
    "def train_char_lm(fname, order=4, add_k=1):\n",
    "  ''' Trains a language model.\n",
    "\n",
    "  This code was borrowed from http://nbviewer.jupyter.org/gist/yoavg/d76121dfde2618422139\n",
    "\n",
    "  Inputs:\n",
    "    fname: Path to a text corpus.\n",
    "    order: The length of the n-grams.\n",
    "    add_k: k value for add-k smoothing. NOT YET IMPLMENTED\n",
    "\n",
    "  Returns:\n",
    "    A dictionary mapping from n-grams of length n to a list of tuples.\n",
    "    Each tuple consists of a possible net character and its probability.\n",
    "  '''\n",
    "\n",
    "  # TODO: Add your implementation of add-k smoothing.\n",
    "  #   data = open(fname).read()\n",
    "#-------------\n",
    "  lm = defaultdict(Counter)\n",
    "  fnameLst = fname\n",
    "  if isinstance(fname, str): fnameLst = [fname]\n",
    "  lm = defaultdict(Counter)\n",
    "#   print(fnameLst)\n",
    "  for fnm in fnameLst:\n",
    "      try: # yezheng -- tackle with \"ISO-8859-1\"\n",
    "            fd = open(fnm, encoding='utf-8', errors='ignore')\n",
    "      except:\n",
    "            fd = open(fnm, encoding=\"ISO-8859-1\")\n",
    "      AllChars = set()\n",
    "      for data in fd.readlines():\n",
    "          data = data.lower()\n",
    "          AllChars.update(data)\n",
    "          pad = \"~\" * order # yezheng: this is just setting beginning of a line -- just like <s><s> mentioned in chapter 4\n",
    "          data = pad + data\n",
    "          for i in range(len(data)-order):\n",
    "            history, char = data[i:i+order], data[i+order]\n",
    "            lm[history][char]+=1\n",
    "          del history\n",
    "          del char\n",
    "          del i\n",
    "      for his in lm.keys():\n",
    "        for ch in AllChars: lm[his][ch]+=0 \n",
    "      fd.close()\n",
    "#-------------\n",
    "  def normalize(counter): # input is a dictionary\n",
    "    s = float(sum(counter.values())) + add_k *len(counter)\n",
    "    return [(c,(cnt+add_k)/s) for c,cnt in counter.items()]\n",
    "  outlm = {hist:normalize(chars) for hist, chars in lm.items()}\n",
    "\n",
    "  return outlm\n",
    "\n",
    "def generate_letter(lm, history, order):\n",
    "  ''' Randomly chooses the next letter using the language model.  \n",
    "  Inputs:\n",
    "    lm: The output from calling train_char_lm.\n",
    "    history: A sequence of text at least 'order' long.\n",
    "    order: The length of the n-grams in the language model. \n",
    "  Returns: \n",
    "    A letter\n",
    "  '''\n",
    "  history = history[-order:]\n",
    "  dist = lm[history]\n",
    "  x = random()\n",
    "  for c,v in dist:\n",
    "    x = x - v\n",
    "    if x <= 0: return c\n",
    "    \n",
    "    \n",
    "# def generate_text(lm, order, nletters=500):\n",
    "#   '''Generates a bunch of random text based on the language model.\n",
    "  \n",
    "#   Inputs:\n",
    "#   lm: The output from calling train_char_lm.\n",
    "#   history: A sequence of previous text.\n",
    "#   order: The length of the n-grams in the language model.\n",
    "  \n",
    "#   Returns: \n",
    "#     A letter  \n",
    "#   '''\n",
    "#   history = \"~\" * order\n",
    "#   out = []\n",
    "#   for i in range(nletters):\n",
    "#     c = generate_letter(lm, history, order)\n",
    "#     history = history[-order:] + c\n",
    "#     out.append(c)\n",
    "#   return \"\".join(out)\n",
    "#-------------------------\n",
    "def perplexity(test_filename, lm, order=4):\n",
    "  '''Computes the perplexity of a text file given the language model.\n",
    "  Inputs:\n",
    "    test_filename: path to text file\n",
    "    lm: The output from calling train_char_lm.\n",
    "    order: The length of the n-grams in the language model. #yezheng: order can be read from lm?\n",
    "  '''\n",
    "  test = open(test_filename).read()\n",
    "  if len(lm.keys()[0]) == order: \n",
    "    print(f\"order given ({order}) is inconsistent with lm model's order ({len(lm.keys()[0])})\")\n",
    "    return -1.0 # normally, return value should not be negative\n",
    "  pad = \"~\" * order\n",
    "  test = pad + test \n",
    "  # TODO: YOUR CODE HERE\n",
    "  # Daphne: make sure (num of characters > order)\n",
    "  logPP = 0\n",
    "  for i in range(len(test)-order): \n",
    "    history, char = test[i:(i+order)], test[i+order]\n",
    "    if not history in lm: return -1.0# float(\"-inf\") # yezheng: deal with unknowns\n",
    "    dict_temp = dict(lm[history])\n",
    "    if not char in dict_temp: return -1.0 #float(\"-inf\")  # yezheng: deal with unknowns\n",
    "    logPP += - np.log2(dict_temp[char])\n",
    "  return logPP # yezheng: should I return it? notice the SPECIFICATION above does not have \"Outputs\"\n",
    "    #yezheng: I have not dealt with UNKNOWN/ unseen cases (piazza @335 led a discussion, not quite for sure they resolve this issue)\n",
    "  \n",
    "\n",
    "\n",
    "\n",
    "def calculate_prob_with_backoff(char, history, lms, lambdas):\n",
    "    '''Uses interpolation to compute the probability of char given a series of \n",
    "     language models trained with different length n-grams.\n",
    "\n",
    "   Inputs:\n",
    "     char: Character to compute the probability of.\n",
    "     history: A sequence of previous text.\n",
    "     lms: A list of language models, outputted by calling train_char_lm.\n",
    "     lambdas: A list of weights for each lambda model. These should sum to 1.\n",
    "    \n",
    "  Returns:\n",
    "    Probability of char appearing next in the sequence.\n",
    "  ''' \n",
    "  # TODO: YOUR CODE HRE\n",
    "    #yezheng: notice this is interpolation rather than backoff\n",
    "    #yezheng: Think lambdas are discounting according to page 15 of chapter 4.\n",
    "    if not len(lms) == len(lambdas): \n",
    "        print(f\"length of 'lms' \\ne length of 'lambdas': {len(lms)}\\ne {len(lambdas)}\")\n",
    "        return -1\n",
    "    ret = 0\n",
    "    for idx in range(len(lms)):\n",
    "        ord = len(list(lms[idx].keys())[0])\n",
    "#         print(history[(len(history)-ord):],\"lms[idx][history[(len(history)-ord):]]\",lms[idx][history[(len(history)-ord):]])\n",
    "        D_temp = dict(lms[idx][history[(len(history)-ord):]])\n",
    "        if char in D_temp: \n",
    "            ret +=  lambdas[idx] *D_temp[char]\n",
    "#         else: return float(\"-Inf\") #yezheng\n",
    "    return ret\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "def set_lambdas(lms, dev_filename):\n",
    "  '''Returns a list of lambda values that weight the contribution of each n-gram model\n",
    "\n",
    "  This can either be done heuristically or by using a development set.\n",
    "\n",
    "  Inputs:\n",
    "    lms: A list of language models, outputted by calling train_char_lm.\n",
    "    dev_filename: Path to a development text file to optionally use for tuning the lmabdas. \n",
    "\n",
    "  Returns:\n",
    "    Probability of char appearing next in the sequence. # yezheng: should I return lambdas\n",
    "  '''\n",
    "  # TODO: YOUR CODE HERE\n",
    "  # piazza: @341 as well as @350\n",
    "  with open(dev_filename) as f:\n",
    "    Lines = f.readlines()\n",
    "#    # # EM algorithm\n",
    "#     for lm in lms:\n",
    "#         order = lm.keys()[0]\n",
    "#         pad = \"~\" * order\n",
    "#         for line in Lines:\n",
    "#             data = pad + line\n",
    "  return [1.0/len(lms)] *len(lms)\n",
    "        \n",
    "if __name__ == '__main__':\n",
    "    print('Training language model')\n",
    "    lm = train_char_lm(\"shakespeare_input.txt\", order=2)\n",
    "    lm = train_char_lm(\"shakespeare_input.txt\", order=4)\n",
    "    lm['hell']\n",
    "#     print(generate_text(lm, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pprint\n",
    "# import operator\n",
    "\n",
    "def print_probs(lm, history):\n",
    "    if not history in lm: \n",
    "        print(f\"Error: no word '{history}' in lm\")\n",
    "        return -1\n",
    "    probs = sorted(lm[history],key=lambda x:(-x[1],x[0]))\n",
    "    pp = pprint.PrettyPrinter()\n",
    "    pp.pprint(probs)\n",
    "# print_probs(lm, \"hell\")\n",
    "# print_probs(lm, \"shell\") # KeyError: 'shell'\n",
    "# print_probs(lm, \"worl\")\n",
    "from random import random\n",
    "def generate_letter(lm, history, order):\n",
    "    history = history[-order:]\n",
    "    dist = lm[history]\n",
    "    x = random()\n",
    "    for c,v in dist:\n",
    "        x = x - v\n",
    "        if x <= 0: return c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def perplexity_test(text, lm, order=4):\n",
    "#   '''Computes the perplexity of a text file given the language model.\n",
    "#   Inputs:\n",
    "#     test_filename: path to text file\n",
    "#     lm: The output from calling train_char_lm.\n",
    "#     order: The length of the n-grams in the language model. #yezheng: order can be read from lm?\n",
    "#   '''\n",
    "\n",
    "#   if len(list(lm.keys())[0]) != order:\n",
    "#     print(f\"order given ({order}) is inconsistent with lm model's order ({len(list(lm.keys())[0])})\")\n",
    "#     return -1.0 # normally, return value should not be negative\n",
    "\n",
    "#   pad = \"~\" * order\n",
    "#   test = pad + text\n",
    "#   # TODO: YOUR CODE HERE\n",
    "#   # Daphne: make sure (num of characters > order)\n",
    "#   logPP = 0\n",
    "\n",
    "#   for i in range(len(test)-order):\n",
    "#     history, char = test[i:(i+order)], test[i+order]\n",
    "    \n",
    "#     if history not in lm: \n",
    "#       #print('A')\n",
    "#       logPP -= 1.0# float(\"-inf\") # yezheng: deal with unknowns\n",
    "#     else:\n",
    "#       dict_temp = dict(lm[history])\n",
    "#       if char not in dict_temp:\n",
    "#         #print('B',char)\n",
    "#         #print(dict_temp)\n",
    "#         logPP -= 1.0 #float(\"-inf\")  # yezheng: deal with unknowns\n",
    "        \n",
    "#       else:\n",
    "#         logPP -= np.log2(dict_temp[char])\n",
    "#   return logPP\n",
    "\n",
    "# def perplexity_yezheng_string(cityname, lm, order=4,filename_debug = ''):\n",
    "#   '''Computes the perplexity of a text file given the language model.\n",
    "#   Inputs:\n",
    "#     test_filename: path to text file\n",
    "#     lms: The output from calling train_char_lms.\n",
    "#     order: The length of the n-grams in the language model. #yezheng: order can be read from lm?\n",
    "#   Outputs:\n",
    "#     max_labels: a list of predicted labels\n",
    "#   '''\n",
    "#   #order = len(list(lm.keys())[0]) #yezheng: I think it should not be an argument\n",
    "#   pad = \"~\" * order\n",
    "#   data = pad + cityname \n",
    "#   # TODO: YOUR CODE HERE\n",
    "#   # Daphne: make sure (num of characters > order)\n",
    "#   logPP = 0\n",
    "#   for i in range(len(data)-order):\n",
    "#     history, char = data[i:(i+order)], data[i+order] \n",
    "#     if history not in lm: logPP -= np.log2(len(lm))\n",
    "#     else:\n",
    "#       dict_temp = dict(lm[history])\n",
    "#       if char not in dict_temp:\n",
    "#         logPP -= np.log2(1.0/len(lm))\n",
    "#       else:\n",
    "#         logPP -= np.log2(dict_temp[char])\n",
    "#   return logPP\n",
    "\n",
    "\n",
    "\n",
    "def perplexity_yezheng_string(cityname, lm, order=4):\n",
    "  '''Computes the perplexity of a text file given the language model.\n",
    "  Inputs:\n",
    "    test_filename: path to text file\n",
    "    lms: The output from calling train_char_lms.\n",
    "    order: The length of the n-grams in the language model. #yezheng: order can be read from lm?\n",
    "  Outputs:\n",
    "    max_labels: a list of predicted labels\n",
    "  '''\n",
    "  #order = len(list(lm.keys())[0]) #yezheng: I think it should not be an argument\n",
    "  pad = \"~\" * order\n",
    "  data = pad + cityname\n",
    "  data = data.lower()\n",
    "  # TODO: YOUR CODE HERE\n",
    "  # Daphne: make sure (num of characters > order)\n",
    "  logPP = 0\n",
    "  for i in range(len(data)-order):\n",
    "    history, char = data[i:(i+order)], data[i+order]   \n",
    "    if history not in lm:\n",
    "      logPP += np.log2(8.0/len(lm)) # float(\"-inf\") # yezheng: deal with unknowns\n",
    "    else:\n",
    "      dict_temp = dict(lm[history])\n",
    "      if char not in dict_temp:\n",
    "        logPP += np.log2(8.0/len(lm)) #float(\"-inf\")  # yezheng: deal with unknowns\n",
    "      else:\n",
    "        logPP += np.log2(dict_temp[char])\n",
    "  return logPP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in.txt: Error rate 0.13\n",
      "pk.txt: Error rate 0.04\n",
      "fr.txt: Error rate 0.01\n",
      "af.txt: Error rate 0.07\n",
      "cn.txt: Error rate 0.03\n",
      "za.txt: Error rate 0.02\n",
      "fi.txt: Error rate 0.03\n",
      "ir.txt: Error rate 0.08\n",
      "de.txt: Error rate 0.06\n"
     ]
    }
   ],
   "source": [
    "# Part 3\n",
    "# All training should be done on the train set and all evaluation \n",
    "# (including confusion matrices and accuracy reports) on the validation set. \n",
    "# You will need to change the data processing code to get this working. \n",
    "# Specifically, you’ll need to modify the code in the 3rd code block to \n",
    "# create two variables category_lines_train and category_lines_val. In addition, to handle unicode, you might need to replace calls to open with calls to codecs.open(filename, \"r\",encoding='utf-8', errors='ignore').\n",
    "#----------------------\n",
    "# yezheng: I do not quite understand his specification of \"3rd code block\"?\n",
    "\n",
    "\n",
    "order = 4\n",
    "\n",
    "#train\n",
    "import os\n",
    "lms_dict = {}# a dictionary of lms\n",
    "for filename in os.listdir('train'):\n",
    "#     filepath = 'train/' + filename \n",
    "    filepath = ['train/' + filename,'val/' + filename]\n",
    "    lms_dict[filename[:2]] = train_char_lm(filepath , order=order)\n",
    "lms_names = list(lms_dict.keys()) # \"af, cn, de, fi, \\ldots\"\n",
    "\n",
    "# yezheng: I define this one myself since\n",
    "\n",
    "\n",
    "# validation set\n",
    "for filename in os.listdir('val'):\n",
    "    filepath = 'val/' + filename\n",
    "    with open(filepath) as f:\n",
    "        CityNames = f.readlines()\n",
    "        err_count = 0\n",
    "        for cname in CityNames:\n",
    "          PP_lst = [perplexity_yezheng_string(cityname = cname[:-1],lm = lms_dict[CountryName], order=order) for CountryName in lms_names]\n",
    "          #print('city',cname)\n",
    "          #print(PP_lst)\n",
    "          #we need something for cities where all values are -1\n",
    "          label_pred = lms_names[PP_lst.index(max(PP_lst))]\n",
    "          #print(label_pred)\n",
    "          if filename[:2] != label_pred:\n",
    "            err_count +=1\n",
    "        print(f\"{filename}: Error rate {err_count*1.0/len(CityNames)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in.txt: Error rate 0.65\n",
    "# pk.txt: Error rate 0.39\n",
    "# fr.txt: Error rate 0.32\n",
    "# af.txt: Error rate 0.43\n",
    "# cn.txt: Error rate 0.13\n",
    "# za.txt: Error rate 0.36\n",
    "# fi.txt: Error rate 0.32\n",
    "# ir.txt: Error rate 0.55\n",
    "# de.txt: Error rate 0.49"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HERE THEY COME\n",
      "HERE THEY FINISH\n"
     ]
    }
   ],
   "source": [
    "#LEADERBOARD\n",
    "\n",
    "order = 2\n",
    "AddK = 1\n",
    "import os\n",
    "lms_dict = {}# a dictionary of lms\n",
    "for filename in os.listdir('train'):\n",
    "    filepath = ['train/' + filename,'val/' + filename]\n",
    "    lms_dict[filename[:2]] = train_char_lm(filepath , order=order, add_k = AddK)\n",
    "\n",
    "lms_names = list(lms_dict.keys()) # \"af, cn, de, fi, \\ldots\"\n",
    "\n",
    "#for filename in os.listdir('val'):\n",
    "#    filepath = 'val/' + filename\n",
    "#    with open(filepath) as f:\n",
    "#        lms_dict[filename[:2]] = train_char_lm(filepath , order=order)\n",
    "\n",
    "\n",
    "print('HERE THEY COME')\n",
    "filepath2 = 'cities_test.txt'\n",
    "labels = []\n",
    "with open(filepath2, encoding='utf-8', errors='ignore') as f:\n",
    "  CityNames = f.readlines()\n",
    "  for cname in CityNames:\n",
    "    PP_lst = [perplexity_yezheng_string(cityname = cname[:-1],lm = lms_dict[CountryName], order=order) for CountryName in lms_names]\n",
    "    #print('city',cname)\n",
    "    #print(PP_lst)\n",
    "    #we need something for cities where all values are -1\n",
    "    if max(PP_lst) == min(PP_lst):\n",
    "      label_pred = lms_names[randrange(0,len(lms_names))]\n",
    "    else:\n",
    "      label_pred = lms_names[PP_lst.index(max(PP_lst))]\n",
    "#     print(label_pred)\n",
    "    labels.append(label_pred)\n",
    "\n",
    "with open('../labels.txt', 'w') as thefile:\n",
    "    for label in labels: thefile.write(\"%s\\n\" % label)\n",
    "print(\"HERE THEY FINISH\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lm4 = train_char_lm(\"shakespeare_input.txt\", order=4)\n",
    "# lm3 = train_char_lm(\"shakespeare_input.txt\", order=3)\n",
    "# lm2 = train_char_lm(\"shakespeare_input.txt\", order=2)\n",
    "# lm1 = train_char_lm(\"shakespeare_input.txt\", order=1)\n",
    "# lms = [lm4,lm3,lm2]\n",
    "# # lms = [lm3,lm2,lm1]\n",
    "# lambdas = set_lambdas(lms, \"shakespeare_input.txt\")\n",
    "# hist = \"Thin\"\n",
    "# ch = 'e'\n",
    "# print(calculate_prob_with_backoff(ch,hist, lms, lambdas))\n",
    "\n",
    "#-------------\n",
    "# lm = train_char_lm(\"shakespeare_input2.txt\", order=5)\n",
    "# print(generate_text(lm, 5, 40),\"------------\")\n",
    "# print(lm['~~~~~'])\n",
    "# print(generate_text(lm, 5, 40),\"------------\")\n",
    "# print(generate_text(lm, 5, 40),\"------------\")\n",
    "# print(generate_text(lm, 5, 40),\"------------\")\n",
    "# print(generate_text(lm, 5, 40),\"------------\")\n",
    "# lm = train_char_lm(\"shakespeare_input2.txt\", order=4)\n",
    "# print(generate_text(lm, 4, 40),\"------------\")\n",
    "# print(lm['~~~~'])\n",
    "# print(generate_text(lm, 4, 40),\"------------\")\n",
    "# print(generate_text(lm, 4, 40),\"------------\")\n",
    "# lm = train_char_lm(\"shakespeare_input.txt\", order=2)\n",
    "# print(generate_text(lm, 2, 40),\"------------\")\n",
    "# print(lm['~~'])\n",
    "# print(generate_text(lm, 2, 40),\"------------\")\n",
    "#--------------\n",
    "# lm = train_char_lm(\"shakespeare_input.txt\", order=2)\n",
    "# print(generate_text(lm, 2))\n",
    "# # print(generate_text(lm, 5, 40)) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
