{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training language model\n"
     ]
    }
   ],
   "source": [
    "from collections import *\n",
    "from random import random\n",
    "import numpy as np\n",
    "def train_char_lm(fname, order=4, add_k=1):\n",
    "  ''' Trains a language model.\n",
    "\n",
    "  This code was borrowed from http://nbviewer.jupyter.org/gist/yoavg/d76121dfde2618422139\n",
    "\n",
    "  Inputs:\n",
    "    fname: Path to a text corpus.\n",
    "    order: The length of the n-grams.\n",
    "    add_k: k value for add-k smoothing. NOT YET IMPLMENTED\n",
    "\n",
    "  Returns:\n",
    "    A dictionary mapping from n-grams of length n to a list of tuples.\n",
    "    Each tuple consists of a possible net character and its probability.\n",
    "  '''\n",
    "\n",
    "  # TODO: Add your implementation of add-k smoothing.\n",
    "#   data = open(fname).read() \n",
    "  try: # yezheng -- tackle with \"ISO-8859-1\"\n",
    "        data = open(fname).read() \n",
    "  except:\n",
    "        data = open(fname, encoding=\"ISO-8859-1\").read()  # \"UTF-8\"\n",
    "  lm = defaultdict(Counter)\n",
    "  pad = \"~\" * order # yezheng: this is just setting beginning of a line -- just like <s><s> mentioned in chapter 4\n",
    "  data = pad + data\n",
    "  for i in range(len(data)-order):\n",
    "    history, char = data[i:i+order], data[i+order]\n",
    "    lm[history][char]+=1\n",
    "  def normalize(counter): # input is a dictionary\n",
    "    s = float(sum(counter.values()) ) + add_k *len(counter)\n",
    "    return [(c,(cnt+add_k)/s) for c,cnt in counter.items()]\n",
    "  outlm = {hist:normalize(chars) for hist, chars in lm.items()}\n",
    "  return outlm\n",
    "\n",
    "\n",
    "def generate_letter(lm, history, order):\n",
    "  ''' Randomly chooses the next letter using the language model.  \n",
    "  Inputs:\n",
    "    lm: The output from calling train_char_lm.\n",
    "    history: A sequence of text at least 'order' long.\n",
    "    order: The length of the n-grams in the language model. \n",
    "  Returns: \n",
    "    A letter\n",
    "  '''\n",
    "  history = history[-order:]\n",
    "  dist = lm[history]\n",
    "  x = random()\n",
    "  for c,v in dist:\n",
    "    x = x - v\n",
    "    if x <= 0: return c\n",
    "    \n",
    "    \n",
    "def generate_text(lm, order, nletters=500):\n",
    "  '''Generates a bunch of random text based on the language model.\n",
    "  \n",
    "  Inputs:\n",
    "  lm: The output from calling train_char_lm.\n",
    "  history: A sequence of previous text.\n",
    "  order: The length of the n-grams in the language model.\n",
    "  \n",
    "  Returns: \n",
    "    A letter  \n",
    "  '''\n",
    "  history = \"~\" * order\n",
    "  out = []\n",
    "  for i in range(nletters):\n",
    "    c = generate_letter(lm, history, order)\n",
    "    history = history[-order:] + c\n",
    "    out.append(c)\n",
    "  return \"\".join(out)\n",
    "\n",
    "def perplexity(test_filename, lm, order=4):\n",
    "  '''Computes the perplexity of a text file given the language model.\n",
    "  Inputs:\n",
    "    test_filename: path to text file\n",
    "    lm: The output from calling train_char_lm.\n",
    "    order: The length of the n-grams in the language model. #yezheng: order can be read from lm?\n",
    "  '''\n",
    "  test = open(test_filename).read()\n",
    "  if len(lm.keys()[0]) == order: \n",
    "    print(f\"order given ({order}) is inconsistent with lm model's order ({len(lm.keys()[0])})\")\n",
    "    return -1.0 # normally, return value should not be negative\n",
    "  pad = \"~\" * order\n",
    "  test = pad + test \n",
    "  # TODO: YOUR CODE HERE\n",
    "  # Daphne: make sure (num of characters > order)\n",
    "  logPP = 0\n",
    "  for i in range(len(test)-order): \n",
    "    history, char = test[i:(i+order)], test[i+order]\n",
    "    if not history in lm: return -1.0# float(\"-inf\") # yezheng: deal with unknowns\n",
    "    dict_temp = dict(lm[history])\n",
    "    if not char in dict_temp: return -1.0 #float(\"-inf\")  # yezheng: deal with unknowns\n",
    "    logPP += - np.log2(dict_temp[char])\n",
    "  return logPP # yezheng: should I return it? notice the SPECIFICATION above does not have \"Outputs\"\n",
    "    #yezheng: I have not dealt with UNKNOWN/ unseen cases (piazza @335 led a discussion, not quite for sure they resolve this issue)\n",
    "  \n",
    "\n",
    "  \n",
    "\n",
    "def calculate_prob_with_backoff(char, history, lms, lambdas):\n",
    "    '''Uses interpolation to compute the probability of char given a series of \n",
    "     language models trained with different length n-grams.\n",
    "\n",
    "   Inputs:\n",
    "     char: Character to compute the probability of.\n",
    "     history: A sequence of previous text.\n",
    "     lms: A list of language models, outputted by calling train_char_lm.\n",
    "     lambdas: A list of weights for each lambda model. These should sum to 1.\n",
    "    \n",
    "  Returns:\n",
    "    Probability of char appearing next in the sequence.\n",
    "  ''' \n",
    "  # TODO: YOUR CODE HRE\n",
    "    #yezheng: notice this is interpolation rather than backoff\n",
    "    #yezheng: Think lambdas are discounting according to page 15 of chapter 4.\n",
    "    if not len(lms) == len(lambdas): \n",
    "        print(f\"length of 'lms' \\ne length of 'lambdas': {len(lms)}\\ne {len(lambdas)}\")\n",
    "        return -1\n",
    "    ret = 0\n",
    "    for idx in range(len(lms)):\n",
    "        ord = len(list(lms[idx].keys())[0])\n",
    "#         print(history[(len(history)-ord):],\"lms[idx][history[(len(history)-ord):]]\",lms[idx][history[(len(history)-ord):]])\n",
    "        D_temp = dict(lms[idx][history[(len(history)-ord):]])\n",
    "        if char in D_temp: \n",
    "            ret +=  lambdas[idx] *D_temp[char]\n",
    "#         else: return float(\"-Inf\") #yezheng\n",
    "    return ret\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "def set_lambdas(lms, dev_filename):\n",
    "  '''Returns a list of lambda values that weight the contribution of each n-gram model\n",
    "\n",
    "  This can either be done heuristically or by using a development set.\n",
    "\n",
    "  Inputs:\n",
    "    lms: A list of language models, outputted by calling train_char_lm.\n",
    "    dev_filename: Path to a development text file to optionally use for tuning the lmabdas. \n",
    "\n",
    "  Returns:\n",
    "    Probability of char appearing next in the sequence. # yezheng: should I return lambdas\n",
    "  '''\n",
    "  # TODO: YOUR CODE HERE\n",
    "  # piazza: @341 as well as @350\n",
    "  with open(dev_filename) as f:\n",
    "    Lines = f.readlines()\n",
    "#    # # EM algorithm\n",
    "#     for lm in lms:\n",
    "#         order = lm.keys()[0]\n",
    "#         pad = \"~\" * order\n",
    "#         for line in Lines:\n",
    "#             data = pad + line\n",
    "  return [1.0/len(lms)] *len(lms)\n",
    "        \n",
    "            \n",
    "        \n",
    "        \n",
    "\n",
    "# if __name__ == '__main__':\n",
    "print('Training language model')\n",
    "lm = train_char_lm(\"shakespeare_input.txt\", order=2)\n",
    "#   print(generate_text(lm, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.10942413586001913\n"
     ]
    }
   ],
   "source": [
    "lm4 = train_char_lm(\"shakespeare_input.txt\", order=4)\n",
    "lm3 = train_char_lm(\"shakespeare_input.txt\", order=3)\n",
    "lm2 = train_char_lm(\"shakespeare_input.txt\", order=2)\n",
    "lm1 = train_char_lm(\"shakespeare_input.txt\", order=1)\n",
    "lms = [lm4,lm3,lm2]\n",
    "# lms = [lm3,lm2,lm1]\n",
    "lambdas = set_lambdas(lms, \"shakespeare_input.txt\")\n",
    "hist = \"Thin\"\n",
    "ch = 'e'\n",
    "print(calculate_prob_with_backoff(ch,hist, lms, lambdas))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('!', 0.06956521739130435),\n",
       " (' ', 0.21304347826086956),\n",
       " (\"'\", 0.021739130434782608),\n",
       " (',', 0.1956521739130435),\n",
       " ('-', 0.06086956521739131),\n",
       " ('.', 0.13043478260869565),\n",
       " ('i', 0.034782608695652174),\n",
       " ('\\n', 0.021739130434782608),\n",
       " (':', 0.021739130434782608),\n",
       " (';', 0.030434782608695653),\n",
       " ('?', 0.034782608695652174),\n",
       " ('s', 0.013043478260869565),\n",
       " ('o', 0.15217391304347827)]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm = train_char_lm(\"shakespeare_input.txt\", order=4)\n",
    "lm['hell']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(' ', 0.21304347826086956),\n",
      " (',', 0.1956521739130435),\n",
      " ('o', 0.15217391304347827),\n",
      " ('.', 0.13043478260869565),\n",
      " ('!', 0.06956521739130435),\n",
      " ('-', 0.06086956521739131),\n",
      " ('?', 0.034782608695652174),\n",
      " ('i', 0.034782608695652174),\n",
      " (';', 0.030434782608695653),\n",
      " ('\\n', 0.021739130434782608),\n",
      " (\"'\", 0.021739130434782608),\n",
      " (':', 0.021739130434782608),\n",
      " ('s', 0.013043478260869565)]\n",
      "Error: no word 'shell' in lm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pprint\n",
    "import operator\n",
    "\n",
    "def print_probs(lm, history):\n",
    "    if not history in lm: \n",
    "        print(f\"Error: no word '{history}' in lm\")\n",
    "        return -1\n",
    "    probs = sorted(lm[history],key=lambda x:(-x[1],x[0]))\n",
    "    pp = pprint.PrettyPrinter()\n",
    "    pp.pprint(probs)\n",
    "print_probs(lm, \"hell\")\n",
    "print_probs(lm, \"shell\") # KeyError: 'shell'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('d', 1.0)]\n"
     ]
    }
   ],
   "source": [
    "print_probs(lm, \"worl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import random\n",
    "def generate_letter(lm, history, order):\n",
    "    history = history[-order:]\n",
    "    dist = lm[history]\n",
    "    x = random()\n",
    "    for c,v in dist:\n",
    "        x = x - v\n",
    "        if x <= 0: return c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fins sich all for thous gody theed, I frome,\n",
      "As feas,--\n",
      "\n",
      "Bruch a sournswor I thaved\n",
      "For o' havill are, what thery lovinge\n",
      "The hileaker: dore, grocathere to buttled ho lights astis to O, tocke shall sided you kin toplent you tor itir yout ow\n",
      "Hently a fron a genty arnin.\n",
      "\n",
      "My brelly: bis youghnes you heyes be my by preubtse me Sercuidea,\n",
      "Ay.\n",
      "Fracts, in con:\n",
      "An he's by fir hade all youtubts the prom to he the thful bell.\n",
      "\n",
      "Publiesudieverincriad wil do.\n",
      "\n",
      "Gody mand the his I ser his verce, the her dinh\n"
     ]
    }
   ],
   "source": [
    "lm = train_char_lm(\"shakespeare_input.txt\", order=2)\n",
    "print(generate_text(lm, 2))\n",
    "# print(generate_text(lm, 5, 40)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All:\n",
      "Genitivo, hig, havings of Hamlet, g ------------\n",
      "[('A', 1.0)]\n",
      "All:\n",
      "Longer, rein: thou shall we loss, c ------------\n",
      "All:\n",
      "Prithee,\n",
      "With two order her, justic ------------\n",
      "All:\n",
      "Dost thou have that malicious;\n",
      "Else ------------\n",
      "All:\n",
      "Gentleman:\n",
      "Sir, my lord? or recover ------------\n",
      "All:\n",
      "To-day a fair Robertiers, truth,\n",
      "Gi ------------\n",
      "[('A', 1.0)]\n",
      "All:\n",
      "Shepherd, to be you, and less the w ------------\n",
      "All:\n",
      "Oh for this veheme?\n",
      "Naturalness, I  ------------\n",
      "Firitit wea,\n",
      "Will fins.\n",
      "\n",
      "Ford\n",
      "Sines him  ------------\n",
      "[('F', 1.0)]\n",
      "Fir puit all be JOHN:\n",
      "Thicts go?\n",
      "\n",
      "Jovenc ------------\n"
     ]
    }
   ],
   "source": [
    "lm = train_char_lm(\"shakespeare_input2.txt\", order=5)\n",
    "print(generate_text(lm, 5, 40),\"------------\")\n",
    "print(lm['~~~~~'])\n",
    "print(generate_text(lm, 5, 40),\"------------\")\n",
    "print(generate_text(lm, 5, 40),\"------------\")\n",
    "print(generate_text(lm, 5, 40),\"------------\")\n",
    "print(generate_text(lm, 5, 40),\"------------\")\n",
    "lm = train_char_lm(\"shakespeare_input2.txt\", order=4)\n",
    "print(generate_text(lm, 4, 40),\"------------\")\n",
    "print(lm['~~~~'])\n",
    "print(generate_text(lm, 4, 40),\"------------\")\n",
    "print(generate_text(lm, 4, 40),\"------------\")\n",
    "lm = train_char_lm(\"shakespeare_input.txt\", order=2)\n",
    "print(generate_text(lm, 2, 40),\"------------\")\n",
    "print(lm['~~'])\n",
    "print(generate_text(lm, 2, 40),\"------------\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'~~~~'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-d9b5c2687f6c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlm\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'~~~~'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m: '~~~~'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in.txt: Error rate 0.0\n",
      "pk.txt: Error rate 1.0\n",
      "fr.txt: Error rate 1.0\n",
      "af.txt: Error rate 1.0\n",
      "cn.txt: Error rate 1.0\n",
      "za.txt: Error rate 1.0\n",
      "fi.txt: Error rate 1.0\n",
      "ir.txt: Error rate 1.0\n",
      "de.txt: Error rate 1.0\n"
     ]
    }
   ],
   "source": [
    "# Part 3\n",
    "# All training should be done on the train set and all evaluation \n",
    "# (including confusion matrices and accuracy reports) on the validation set. \n",
    "# You will need to change the data processing code to get this working. \n",
    "# Specifically, you’ll need to modify the code in the 3rd code block to \n",
    "# create two variables category_lines_train and category_lines_val. In addition, to handle unicode, you might need to replace calls to open with calls to codecs.open(filename, \"r\",encoding='utf-8', errors='ignore').\n",
    "#----------------------\n",
    "# yezheng: I do not quite understand his specification of \"3rd code block\"?\n",
    "import os\n",
    "lms_dict = {}# a dictionary of lms\n",
    "for filename in os.listdir('train'):\n",
    "    filepath = 'train/' + filename \n",
    "    with open(filepath) as f:\n",
    "        lms_dict[filename[:2]] = train_char_lm(filepath , order=4)\n",
    "lms_names = list(lms_dict.keys()) # \"af, cn, de, fi, \\ldots\"\n",
    "# yezheng: I define this one myself since         \n",
    "def perplexity_yezheng_string(cityname, lm):\n",
    "  '''Computes the perplexity of a text file given the language model.\n",
    "  Inputs:\n",
    "    test_filename: path to text file\n",
    "    lms: The output from calling train_char_lms.\n",
    "    order: The length of the n-grams in the language model. #yezheng: order can be read from lm?\n",
    "  Outputs:\n",
    "    max_labels: a list of predicted labels\n",
    "  '''\n",
    "  order = len(list(lm.keys())[0]) #yezheng: I think it should not be an argument\n",
    "  pad = \"~\" * order\n",
    "  data = pad + cityname \n",
    "  # TODO: YOUR CODE HERE\n",
    "  # Daphne: make sure (num of characters > order)\n",
    "  logPP = 0\n",
    "  for i in range(len(data)-order): \n",
    "    history, char = data[i:(i+order)], data[i+order]\n",
    "    if not history in lm: return -1.0 #float(\"-inf\")# yezheng: deal with unknowns\n",
    "    dict_temp = dict(lm[history])\n",
    "    if not char in dict_temp: return -1.0\n",
    "#     print(f\"dict_temp {dict_temp} {char}\")\n",
    "    logPP += - np.log2(dict_temp[char])\n",
    "  return logPP # yezheng: should I return it? notice the SPECIFICATION above does not have \"Outputs\"\n",
    "    #yezheng: I have not dealt with UNKNOWN/ unseen cases (piazza @335 led a discussion, not quite for sure they resolve this issue)\n",
    "\n",
    "# validation set\n",
    "for filename in os.listdir('val'):\n",
    "    filepath = 'val/' + filename\n",
    "    with open(filepath) as f:\n",
    "        CityNames = f.readlines()\n",
    "        err_count = 0\n",
    "        for cname in CityNames:\n",
    "            PP_lst = [perplexity_yezheng_string(cityname = cname[:-1],lm = lms_dict[CountryName]) for CountryName in lms_names]\n",
    "            label_pred = lms_names[PP_lst.index(max(PP_lst))]\n",
    "            if not filename[:2]== label_pred: err_count +=1        \n",
    "        print(f\"{filename }: Error rate {err_count*1.0/len(CityNames)}\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
