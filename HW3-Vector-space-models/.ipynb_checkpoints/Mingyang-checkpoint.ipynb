{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import subprocess\n",
    "import re\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "os.chdir(\"/Users/yezheng/Dropbox/Onedrive/CIS530/HW3/lyz\")\n",
    "\n",
    "\n",
    "def read_in_shakespeare():\n",
    "  '''Reads in the Shakespeare dataset processesit into a list of tuples.\n",
    "     Also reads in the vocab and play name lists from files.\n",
    "\n",
    "  Each tuple consists of\n",
    "  tuple[0]: The name of the play\n",
    "  tuple[1] A line from the play as a list of tokenized words.\n",
    "\n",
    "  Returns:\n",
    "    tuples: A list of tuples in the above format.\n",
    "    document_names: A list of the plays present in the corpus.\n",
    "    vocab: A list of all tokens in the vocabulary.\n",
    "  '''\n",
    "\n",
    "  tuples = []\n",
    "\n",
    "  with open('will_play_text.csv') as f:\n",
    "    csv_reader = csv.reader(f, delimiter=';')\n",
    "    for row in csv_reader:    \n",
    "      play_name = row[1]\n",
    "      line = row[5]\n",
    "      line_tokens = re.sub(r'[^a-zA-Z0-9\\s]', ' ', line).split()\n",
    "      line_tokens = [token.lower() for token in line_tokens]\n",
    "      tuples.append((play_name, line_tokens))\n",
    "\n",
    "  with open('vocab.txt') as f:\n",
    "    vocab =  [line.strip() for line in f]\n",
    "\n",
    "  with open('play_names.txt') as f:\n",
    "    document_names =  [line.strip() for line in f]\n",
    "\n",
    "  return tuples, document_names, vocab\n",
    "\n",
    "def get_row_vector(matrix, row_id):\n",
    "  return matrix[row_id, :]\n",
    "\n",
    "def get_column_vector(matrix, col_id):\n",
    "  return matrix[:, col_id]\n",
    "\n",
    "#line_tuples, document_names, vocab=read_in_shakespeare()\n",
    "\n",
    "def create_term_document_matrix(line_tuples, document_names, vocab):\n",
    "  '''Returns a numpy array containing the term document matrix for the input lines.\n",
    "\n",
    "  Inputs:\n",
    "    line_tuples: A list of tuples, containing the name of the document and \n",
    "    a tokenized line from that document.\n",
    "    document_names: A list of the document names\n",
    "    vocab: A list of the tokens in the vocabulary\n",
    "\n",
    "  Let n = len(document_names) and m = len(vocab).\n",
    "\n",
    "  Returns:\n",
    "    td_matrix: A mxn numpy array where the number of rows is the number of documents\n",
    "        and each column corresponds to a token in the corpus. A_ij contains the\n",
    "        frequency with which word i occurs in document j.\n",
    "    vocab: A list containing the tokens being represented by each column.\n",
    "  '''\n",
    "\n",
    "  vocab_to_id = dict(zip(vocab, range(0, len(vocab))))\n",
    "  docname_to_id = dict(zip(document_names, range(0, len(document_names))))\n",
    "  n = len(docname_to_id) ; m = len(vocab_to_id)\n",
    "  td_matrix=np.array(   [   [0 for i in range(n)] for j in range(m)        ]     )\n",
    "  for doc_vocab in line_tuples:\n",
    "      doc = doc_vocab[0]\n",
    "      for word in doc_vocab[1]:\n",
    "          td_matrix[  vocab_to_id[word]  ,docname_to_id[doc]   ]+=1\n",
    "  return td_matrix\n",
    "\n",
    "def create_term_context_matrix(line_tuples, vocab, context_window_size=1):\n",
    "  '''Returns a numpy array containing the term context matrix for the input lines.\n",
    "\n",
    "  Inputs:\n",
    "    line_tuples: A list of tuples, containing the name of the document and \n",
    "    a tokenized line from that document.\n",
    "    vocab: A list of the tokens in the vocabulary\n",
    "\n",
    "  Let n = len(vocab).\n",
    "\n",
    "  Returns:\n",
    "    tc_matrix: A nxn numpy array where A_ij contains the frequency with which\n",
    "        word j was found within context_window_size to the left or right of\n",
    "        word i in any sentence in the tuples.\n",
    "    vocab: A list containing the tokens being represented by each column.\n",
    "  '''\n",
    "  vocab_to_id = dict(zip(vocab, range(0, len(vocab))))\n",
    "  m = len(vocab)\n",
    "  tc_matrix=np.array(   [   [0 for i in range(m)] for j in range(m)        ]     )\n",
    "  for doc_vocab in line_tuples:\n",
    "      num_of_vocab = len(doc_vocab[1])\n",
    "      vocab_list = doc_vocab[1]\n",
    "      for index in range(num_of_vocab):\n",
    "          target_word = vocab_list[index]\n",
    "          for j in range(   max(index-context_window_size,0), min( index+context_window_size+1, num_of_vocab  )  ): \n",
    "              if(not j==index):\n",
    "                context_word =  vocab_list[j]  \n",
    "                tc_matrix[  vocab_to_id[ target_word ]  ,vocab_to_id[ context_word ]  ]+=1\n",
    "  return tc_matrix\n",
    "  \n",
    "\n",
    "def create_PPMI_matrix(term_context_matrix):\n",
    "  '''Given a term context matrix, output a PPMI matrix.\n",
    "  \n",
    "  See section 15.1 in the textbook.\n",
    "  \n",
    "  Hint: Use numpy matrix and vector operations to speed up implementation.\n",
    "  \n",
    "  Input:\n",
    "    term_context_matrix: A nxn numpy array, where n is\n",
    "        the numer of tokens in the vocab.\n",
    "  \n",
    "  Returns: A nxn numpy matrix, where A_ij is equal to the\n",
    "     point-wise mutual information between the ith word\n",
    "     and the jth word in the term_context_matrix.\n",
    "  '''   \n",
    "  word_freq =  np.sum(term_context_matrix, axis=1);\n",
    "  total_freq = sum(word_freq )\n",
    "  term_context_matrix_rmzero= np.copy(term_context_matrix)\n",
    "  tmp=1.0/(2*total_freq)\n",
    "  #term_context_matrix_rmzero[term_context_matrix_rmzero==0] = tmp\n",
    "  term_context_matrix_rmzero = np.maximum(term_context_matrix_rmzero, tmp)\n",
    "  PPMI=np.subtract(   np.log2(term_context_matrix_rmzero), np.log2(word_freq ) )\n",
    "  PPMI=np.subtract(   PPMI, np.log2(   np.transpose(np.array([word_freq,] ) ))  )\n",
    "  PPMI=PPMI+np.log2(total_freq)\n",
    "  PPMI=np.maximum(PPMI,0) \n",
    "  \n",
    "  \n",
    "     \n",
    "  '''\n",
    "  prob_w = np.sum(term_context_matrix, axis=1); \n",
    "  total_sum = sum(prob_w);total_sum =total_sum *1.0\n",
    "  prob_w =prob_w /total_sum\n",
    "  prob_c=np.sum(term_context_matrix, axis=0)/total_sum\n",
    "  prob_wc=term_context_matrix/total_sum\n",
    "  joint_over_independent = np.multiply(1.0/np.outer(prob_w, prob_c), prob_wc)    #avoid log0 error\n",
    "  joint_over_independent[joint_over_independent==0]=0.1\n",
    "  PPMI=np.maximum(np.log2( joint_over_independent  ),0) \n",
    "  '''\n",
    "  return PPMI\n",
    "  \n",
    "\n",
    "def create_tf_idf_matrix(term_document_matrix):\n",
    "  '''Given the term document matrix, output a tf-idf weighted version.\n",
    "\n",
    "  See section 15.2.1 in the textbook.\n",
    "  \n",
    "  Hint: Use numpy matrix and vector operations to speed up implementation.\n",
    "\n",
    "  Input:\n",
    "    term_document_matrix: Numpy array where each column represents a document \n",
    "    and each row, the frequency of a word in that document.\n",
    "\n",
    "  Returns:\n",
    "    A numpy array with the same dimension as term_document_matrix, where\n",
    "    A_ij is weighted by the inverse document frequency of document h.\n",
    "  '''\n",
    "  # YOUR CODE HERE\n",
    "  N = term_document_matrix.shape[1]; N = 1.0*N\n",
    "  indicator = 1.0*(term_document_matrix>0)\n",
    "  df = np.sum(indicator,axis=1)\n",
    "  idf = np.log(N/df)\n",
    "  tfidf=np.multiply(term_document_matrix, np.transpose(np.array([idf,])))\n",
    "  return tfidf\n",
    "\n",
    "def compute_cosine_similarity(vector1, vector2):\n",
    "  '''Computes the cosine similarity of the two input vectors.\n",
    "\n",
    "  Inputs:\n",
    "    vector1: A nx1 numpy array\n",
    "    vector2: A nx1 numpy array\n",
    "\n",
    "  Returns:\n",
    "    A scalar similarity value.\n",
    "  '''\n",
    "  n1=np.inner(vector1, vector1)\n",
    "  n2=np.inner(vector2, vector2)\n",
    "  if( n1==0 or n2==0 ) :  sim = 0\n",
    "  else:\n",
    "      sim =  1.0*np.inner(vector1, vector2) / (  np.sqrt(n1)  *  np.sqrt(n2) )\n",
    "  return sim\n",
    "\n",
    "def compute_jaccard_similarity(vector1, vector2):\n",
    "  '''Computes the cosine similarity of the two input vectors.\n",
    "\n",
    "  Inputs:\n",
    "    vector1: A nx1 numpy array\n",
    "    vector2: A nx1 numpy array\n",
    "\n",
    "  Returns:\n",
    "    A scalar similarity value.\n",
    "  '''\n",
    "#  if(   sum(1.0*(np.minimum(vector1, vector2)<0))>0   ):\n",
    "#      raise ValueError('negative value in vectors')\n",
    "  denominator = sum(np.maximum(vector1, vector2))\n",
    "  if(denominator==0):sim = 0\n",
    "  else:\n",
    "      sim = 1.0*sum(np.minimum(vector1, vector2) )/denominator\n",
    "  return sim\n",
    "\n",
    "def compute_dice_similarity(vector1, vector2):\n",
    "  '''Computes the cosine similarity of the two input vectors.\n",
    "\n",
    "  Inputs:\n",
    "    vector1: A nx1 numpy array\n",
    "    vector2: A nx1 numpy array\n",
    "\n",
    "  Returns:\n",
    "    A scalar similarity value.\n",
    "  '''\n",
    "#  if(   sum(1.0*(np.minimum(vector1, vector2)<0))>0   ):\n",
    "#      raise ValueError('negative value in vectors')\n",
    "  denominator = sum( vector1+vector2   )*1.0/2\n",
    "  if(denominator==0):sim = 0\n",
    "  else:\n",
    "      sim = 1.0*sum(np.minimum(vector1, vector2))/denominator\n",
    "  return sim  \n",
    "\n",
    "def rank_plays(target_play_index, term_document_matrix, similarity_fn):\n",
    "  ''' Ranks the similarity of all of the plays to the target play.\n",
    "\n",
    "  # NOTE: THIS DOCSTRING WAS UPDATED ON JAN 24, 12:51 PM.\n",
    "\n",
    "  Inputs:\n",
    "    target_play_index: The integer index of the play we want to compare all others against.\n",
    "    term_document_matrix: The term-document matrix as a mxn numpy array.\n",
    "    similarity_fn: Function that should be used to compared vectors for two\n",
    "      documents. Either compute_dice_similarity, compute_jaccard_similarity, or\n",
    "      compute_cosine_similarity.\n",
    "\n",
    "  Returns:\n",
    "    A length-n list of integer indices corresponding to play names,\n",
    "    ordered by decreasing similarity to the play indexed by target_play_index\n",
    "  '''\n",
    "  #ind_to_docname = dict(zip(range(0, len(document_names)), document_names))\n",
    "  play_sim={}\n",
    "  for other_doc_ind in range(term_document_matrix.shape[1]):\n",
    "      play_sim[ other_doc_ind   ] = similarity_fn(  term_document_matrix[:,target_play_index], term_document_matrix[:,other_doc_ind]   )  \n",
    "  rank = sorted(play_sim, key=play_sim.get, reverse=True)     \n",
    "  return rank\n",
    "\n",
    "def rank_words(target_word_index, matrix, similarity_fn):\n",
    "  ''' Ranks the similarity of all of the words to the target word.\n",
    "\n",
    "  Inputs:\n",
    "    vocab: List of terms, corresponding to target_word_index rows (i.e. word corresponding\n",
    "      to target_word_index[i,:] is given by vocab[i])\n",
    "    target_word_index: The index of the word we want to compare all others against.\n",
    "    matrix: Numpy matrix where the ith row represents a vector embedding of the ith word.\n",
    "    similarity_fn: Function that should be used to compared vectors for two word\n",
    "      ebeddings. Either compute_dice_similarity, compute_jaccard_similarity, or\n",
    "      compute_cosine_similarity.\n",
    "\n",
    "  Returns:\n",
    "    A length-n list of words, ordered by decreasing similarity to the \n",
    "    target word indexed by word_index\n",
    "  '''\n",
    "  \n",
    "  word_sim={}\n",
    "  for other_word_ind in range(matrix.shape[1]):\n",
    "      word_sim[ other_word_ind   ] = similarity_fn(  matrix[target_word_index, :], matrix[other_word_ind, :]   )\n",
    "  rank = sorted(word_sim, key=word_sim.get, reverse=True)    \n",
    "  return rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing term document matrix...\n",
      "elapsed time 0.681891679763794 ( 0.681891679763794 )\n",
      "Computing tf-idf matrix...\n",
      "elapsed time 0.7039790153503418 ( 0.02208733558654785 )\n",
      "Computing term context matrix...\n",
      "elapsed time 71.05132269859314 ( 70.3473436832428 )\n",
      "Computing PPMI matrix...\n",
      "elapsed time 197.34517192840576 ( 126.29384922981262 )\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'elapsed_time' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-41bdd38f22c9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    298\u001b[0m   \u001b[0mT4\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    299\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"elapsed time\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mT4\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mT0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"(\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mT4\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mT3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\")\"\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 300\u001b[0;31m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melapsed_time\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    301\u001b[0m   \u001b[0;31m#40s\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'elapsed_time' is not defined"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "  tuples, document_names, vocab = read_in_shakespeare()\n",
    "  import time\n",
    "  print('Computing term document matrix...')\n",
    "  T0 = time.time()\n",
    "  td_matrix = create_term_document_matrix(tuples, document_names, vocab)\n",
    "  T1 = time.time() \n",
    "  print(\"elapsed time\", T1 - T0, \"(\",T1-T0,\")\" ) \n",
    "  \n",
    "  print('Computing tf-idf matrix...')\n",
    "  tf_idf_matrix = create_tf_idf_matrix(td_matrix)\n",
    "  T2 = time.time() \n",
    "  print(\"elapsed time\", T2 - T0, \"(\",T2-T1,\")\" ) \n",
    "  \n",
    "  print('Computing term context matrix...')\n",
    "  tc_matrix = create_term_context_matrix(tuples, vocab, context_window_size=2)\n",
    "  T3 = time.time() \n",
    "  print(\"elapsed time\", T3 - T0, \"(\",T3-T2,\")\" ) \n",
    "  tc_matrix=np.maximum(tc_matrix, 1e-6) \n",
    "  #54s/40s\n",
    "\n",
    "  print('Computing PPMI matrix...')\n",
    "  PPMI_matrix = create_PPMI_matrix(tc_matrix)\n",
    "  T4 = time.time() \n",
    "  print(\"elapsed time\", T4 - T0, \"(\",T4-T3,\")\" ) \n",
    "\n",
    "  random_idx = random.randint(0, len(document_names)-1)\n",
    "  similarity_fns = [compute_cosine_similarity, compute_jaccard_similarity, compute_dice_similarity]\n",
    "  for sim_fn in similarity_fns:\n",
    "    print('\\nThe 10 most similar plays to \"%s\" using %s are:' % (document_names[random_idx], sim_fn.__qualname__))\n",
    "    ranks = rank_plays(random_idx, td_matrix, sim_fn)\n",
    "    for idx in range(0, 10):\n",
    "      doc_id = ranks[idx]\n",
    "      print('%d: %s' % (idx+1, document_names[doc_id]))\n",
    "\n",
    "  word = 'juliet'\n",
    "  vocab_to_index = dict(zip(vocab, range(0, len(vocab))))\n",
    "  for sim_fn in similarity_fns:\n",
    "    print('\\nThe 10 most similar words to \"%s\" using %s on term-context frequency matrix are:' % (word, sim_fn.__qualname__))\n",
    "    ranks = rank_words(vocab_to_index[word], tc_matrix, sim_fn)\n",
    "    for idx in range(0, 10):\n",
    "      word_id = ranks[idx]\n",
    "      print('%d: %s' % (idx+1, vocab[word_id]))\n",
    "  T5 = time.time() \n",
    "  print(\"elapsed time\", T5 - T0, \"(\",T5-T4,\")\" ) \n",
    "\n",
    "\n",
    "\n",
    "  word = 'juliet'\n",
    "  vocab_to_index = dict(zip(vocab, range(0, len(vocab))))\n",
    "  for sim_fn in similarity_fns:\n",
    "    print('\\nThe 10 most similar words to \"%s\" using %s on PPMI matrix are:' % (word, sim_fn.__qualname__))\n",
    "    ranks = rank_words(vocab_to_index[word], PPMI_matrix, sim_fn)\n",
    "    for idx in range(0, 10):\n",
    "      word_id = ranks[idx]\n",
    "      print('%d: %s' % (idx+1, vocab[word_id]))\n",
    "  T6 = time.time() \n",
    "  print(\"elapsed time\", T6 - T0, \"(\",T6-T5,\")\" ) \n",
    "\n",
    "'''\n",
    "1: juliet\n",
    "2: pined\n",
    "3: waken\n",
    "4: capulet\n",
    "5: tybalt\n",
    "6: muffled\n",
    "7: fares\n",
    "8: provost\n",
    "9: wills\n",
    "10: county\n",
    "1: juliet\n",
    "2: tybalt\n",
    "3: capulet\n",
    "4: silvia\n",
    "5: lucio\n",
    "6: nurse\n",
    "7: romeo\n",
    "8: montague\n",
    "9: leonato\n",
    "10: provost\n",
    "1: juliet\n",
    "2: tybalt\n",
    "3: capulet\n",
    "4: silvia\n",
    "5: lucio\n",
    "6: nurse\n",
    "7: romeo\n",
    "8: montague\n",
    "9: leonato\n",
    "10: provost\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
