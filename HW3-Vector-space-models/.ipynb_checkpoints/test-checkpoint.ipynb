{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %config IPCompleter.greedy = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, csv, subprocess, re, random\n",
    "os.chdir(\"lyz\")\n",
    "import numpy as np\n",
    "import multiprocessing, platform\n",
    "# in some cases, global pool is not suggested, leading WARNINGS\n",
    "# num_cores = multiprocessing.cpu_count()\n",
    "# print(\"num of cores:\", num_cores)\n",
    "# global pool\n",
    "# pool = multiprocessing.Pool(processes=num_cores)\n",
    "# from sklearn import metrics as SKmetrics\n",
    "# import operator\n",
    "# from scipy.sparse import lil_matrix\n",
    "\n",
    "def read_in_shakespeare():\n",
    "    '''Reads in the Shakespeare dataset processesit into a list of tuples.\n",
    "     Also reads in the vocab and play name lists from files.\n",
    "\n",
    "    Each tuple consists of\n",
    "    tuple[0]: The name of the play\n",
    "    tuple[1] A line from the play as a list of tokenized words.\n",
    "\n",
    "    Returns:\n",
    "        tuples: A list of tuples in the above format.\n",
    "        document_names: A list of the plays present in the corpus.\n",
    "        vocab: A list of all tokens in the vocabulary.\n",
    "    '''\n",
    "\n",
    "    tuples = []\n",
    "    with open('will_play_text.csv') as f:\n",
    "        csv_reader = csv.reader(f, delimiter=';')\n",
    "        for row in csv_reader:\n",
    "            play_name = row[1]\n",
    "            line = row[5]\n",
    "            line_tokens = re.sub(r'[^a-zA-Z0-9\\s]', ' ', line).split()\n",
    "            line_tokens = [token.lower() for token in line_tokens]\n",
    "            tuples.append((play_name, line_tokens))\n",
    "        f.close()\n",
    "    with open('vocab.txt') as f: \n",
    "        vocab = [line.strip() for line in f.readlines()]\n",
    "        f.close()\n",
    "    with open('play_names.txt') as f: \n",
    "        document_names =  [line.strip() for line in f]\n",
    "        f.close()\n",
    "    return tuples, document_names, vocab\n",
    "\n",
    "def get_row_vector(matrix, row_id): return matrix[row_id, :]\n",
    "\n",
    "def get_column_vector(matrix, col_id): return matrix[:, col_id]\n",
    "\n",
    "def compute_cosine_similarity(v1, v2): \n",
    "    '''Computes the cosine similarity of the two input vectors.\n",
    "    Inputs:()\n",
    "    v1: A nx1 numpy array \n",
    "    v2: A nx1 numpy array \n",
    "\n",
    "    Returns:\n",
    "    A scalar similarity value. # a numpy array if multiple dimension\n",
    "    '''\n",
    "    ret = sum(np.multiply(v1, v2))\n",
    "    if 0 == ret: return ret\n",
    "    ret = ret/ (np.linalg.norm(v1)*np.linalg.norm(v2))\n",
    "#     if 1 == len(vector1.shape) and 1 == len(vector1.shape): return ret[0] # np.double\n",
    "    return ret\n",
    "\n",
    "def compute_jaccard_similarity(vector1, vector2):\n",
    "    '''Computes the cosine similarity of the two input vectors.\n",
    "\n",
    "  Inputs:\n",
    "    vector1: A nx1 numpy array\n",
    "    vector2: A nx1 numpy array\n",
    "\n",
    "  Returns:\n",
    "    A scalar similarity value.\n",
    "  '''  \n",
    "    # http://scikit-learn.org/stable/modules/generated/sklearn.metrics.jaccard_similarity_score.html\n",
    "    ret = np.sum(np.minimum(vector1,vector2))/(np.sum(np.maximum(vector1, vector2)))\n",
    "    return ret\n",
    "\n",
    "def compute_dice_similarity(vector1, vector2):\n",
    "    '''Computes the cosine similarity of the two input vectors.\n",
    "\n",
    "  Inputs:\n",
    "    vector1: A nx1 numpy array\n",
    "    vector2: A nx1 numpy array\n",
    "\n",
    "  Returns:\n",
    "    A scalar similarity value.\n",
    "    '''\n",
    "    ret = np.sum(np.minimum(vector1,vector2))/(np.sum(vector1) + np.sum(vector2))\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def process_impv_rk_pls(args):\n",
    "#     similarity_fn,v1,v2,i = args\n",
    "#     return (i,similarity_fn(v1,v2))\n",
    "def rank_plays(target_play_index, term_document_matrix, similarity_fn):\n",
    "    ''' Ranks the similarity of all of the plays to the target play.\n",
    "  # NOTE: THIS DOCSTRING WAS UPDATED ON JAN 24, 12:51 PM.\n",
    "  Inputs:\n",
    "    target_play_index: The integer index of the play we want to compare all others against.\n",
    "    term_document_matrix: The term-document matrix as a mxn numpy array.\n",
    "    similarity_fn: Function that should be used to compared vectors for two\n",
    "      documents. Either compute_dice_similarity, compute_jaccard_similarity, or\n",
    "      compute_cosine_similarity.\n",
    "  Returns:\n",
    "    A length-n list of integer indices corresponding to play names,\n",
    "    ordered by decreasing similarity to the play indexed by target_play_index\n",
    "  '''\n",
    "#     print(\"term_document_matrix\",term_document_matrix.shape)\n",
    "    n_docs = term_document_matrix.shape[1]\n",
    "    v1 = term_document_matrix[:,target_play_index]\n",
    "    # this is a easy question, no need for parallel computing\n",
    "    # -------\n",
    "    # not parallel\n",
    "    SimDict= dict((i,similarity_fn(v1,term_document_matrix[:,i])) for i in range(n_docs)) # including itself\n",
    "    #----\n",
    "    # excluding itself\n",
    "#     SimDict= dict((i,similarity_fn(v1,term_document_matrix[:,i])) for i in range(target_play_index-1))\n",
    "#     SimDict.update(dict((i,similarity_fn(v1,term_document_matrix[:,i])) for i in range(target_play_index+1,n_docs)))\n",
    "    # not parallel\n",
    "    #-------\n",
    "    # parallel\n",
    "#     num_cores = multiprocessing.cpu_count()\n",
    "# #     print(\"num of cores:\", num_cores)\n",
    "#     pool = multiprocessing.Pool(processes=num_cores)\n",
    "#     SimLst_temp = pool.map(process_impv_rk_pls, [(similarity_fn,v1,term_document_matrix[:,i],i) for i in range(target_play_index-1)] ) \n",
    "#     SimDict= dict(SimLst_temp)\n",
    "#     SimLst_temp = pool.map(process_impv_rk_pls, [(similarity_fn,v1,term_document_matrix[:,i],i) for i in range(target_play_index+1,n_docs)] ) \n",
    "#     SimDict.update(dict(SimLst_temp))\n",
    "    # parallel\n",
    "    #-------\n",
    "    # suggested by Mingyang\n",
    "    retL = sorted(SimDict,  key=SimDict.get, reverse = True)\n",
    "#     retL = list(L.keys())\n",
    "#     retL.reverse()\n",
    "#     print(\"rank plays (\",similarity_fn.__name__,\"):\", retL)\n",
    "    return retL\n",
    "def process_impv_rk_wds(args):\n",
    "    similarity_fn,v1,v2 = args\n",
    "    return similarity_fn(v1,v2)\n",
    "def rank_words(target_word_index, matrix, similarity_fn):\n",
    "    ''' Ranks the similarity of all of the words to the target word.\n",
    "  # NOTE: THIS DOCSTRING WAS UPDATED ON JAN 24, 12:51 PM.\n",
    "  Inputs:\n",
    "    target_word_index: The index of the word we want to compare all others against.\n",
    "    matrix: Numpy matrix where the ith row represents a vector embedding of the ith word.\n",
    "    similarity_fn: Function that should be used to compared vectors for two word\n",
    "      ebeddings. Either compute_dice_similarity, compute_jaccard_similarity, or\n",
    "      compute_cosine_similarity.\n",
    "\n",
    "  Returns:\n",
    "    A length-n list of integer word indices, ordered by decreasing similarity to the \n",
    "    target word indexed by word_index\n",
    "  '''\n",
    "    n_words = matrix.shape[1]\n",
    "    v1 = matrix[:,target_word_index]\n",
    "    #---------------\n",
    "    # not parallel\n",
    "    SimLst_temp= [similarity_fn(v1,matrix[:,i]) for i in range(n_words)]\n",
    "    # not parallel\n",
    "    #---------------\n",
    "    # parallel\n",
    "#     num_cores = multiprocessing.cpu_count()\n",
    "# #     print(\"num of cores:\", num_cores)\n",
    "#     pool = multiprocessing.Pool(processes=num_cores)\n",
    "#     SimLst_temp = pool.map(process_impv_rk_wds, [(similarity_fn,v1,matrix[:,i]) for i in range(n_words)])    \n",
    "    # parallel\n",
    "    #---------------\n",
    "    SimDict= dict(enumerate(SimLst_temp))\n",
    "\n",
    "    #----\n",
    "    # excluding itself\n",
    "#     SimLst_temp = pool.map(process_impv_rk_wds, [(similarity_fn,v1,matrix[:,i],i) for i in range(target_word_index-1)])\n",
    "#     SimDict= dict(SimLst_temp)\n",
    "#     SimLst_temp = pool.map(process_impv_rk_wds, [(similarity_fn,v1,matrix[:,i],i) for i in range(target_word_index+1,n_words)])\n",
    "#     SimDict.update(dict(SimLst_temp))\n",
    "    #----\n",
    "    # improved suggested Mingyang\n",
    "    retL = sorted(SimDict, key=SimDict.get, reverse= True)\n",
    "#     L = dict(L)\n",
    "#     retL = list(L.keys())\n",
    "#     print(\"rank words: retL\", retL)\n",
    "    return retL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cannot put it locally inside due to multiprocessing\n",
    "# cannot use lambda func due to multiprocessing\n",
    "# def process_impv(Tuple):#         counts, Vocab = Tuple\n",
    "#         return [Tuple[0][w] for w in Tuple[1]]\n",
    "def create_term_document_matrix(line_tuples, document_names, vocab):\n",
    "    '''Returns a numpy array containing the term document matrix for the input lines.\n",
    "    Inputs:\n",
    "    line_tuples: A list of tuples, containing the name of the document and \n",
    "    a tokenized line from that document.\n",
    "    document_names: A list of the document names\n",
    "    vocab: A list of the tokens in the vocabulary\n",
    "    # NOTE: THIS DOCSTRING WAS UPDATED ON JAN 24, 12:39 PM.\n",
    "\n",
    "    Let m = len(vocab) and n = len(document_names).\n",
    "\n",
    "    Returns:\n",
    "        td_matrix: A mxn numpy array where the number of rows is the number of words\n",
    "          and each column corresponds to a document. A_ij contains the\n",
    "          frequency with which word i occurs in document j.\n",
    "    '''\n",
    "#     from collections import Counter, defaultdict\n",
    "#     Dict_doc_words_Counter = defaultdict(Counter)\n",
    "#     for d, wList in line_tuples: Dict_doc_words_Counter[d] += Counter(wList)\n",
    "    vocab_to_id = dict(zip(vocab, range(0, len(vocab))))\n",
    "    docname_to_id = dict(zip(document_names, range(0, len(document_names))))\n",
    "    n_docs = len(document_names)\n",
    "    n_words = len(vocab)\n",
    "    ret = np.zeros((n_words, n_docs), dtype = np.int32)\n",
    "    for d, wList in line_tuples: \n",
    "        doc_idx = docname_to_id[d]\n",
    "        for w in wList: ret[vocab_to_id[w]][doc_idx] += 1\n",
    "# --------------\n",
    "# not parallel\n",
    "#     ret = [[Dict_doc_words_Counter[doc][w] for w in vocab] for doc in document_names] # fastest on Macbook\n",
    "# not parallel\n",
    "# --------------\n",
    "# parallel\n",
    "#     num_cores = multiprocessing.cpu_count()\n",
    "#     print(\"num of cores:\", num_cores)\n",
    "#     pool = multiprocessing.Pool(processes=num_cores)\n",
    "#     ret = pool.map(process_impv, ((Dict_doc_words_Counter[doc],vocab) for doc in document_names) ) \n",
    "# parallel\n",
    "# --------------\n",
    "    return ret\n",
    "\n",
    "#-------\n",
    "# lil_matrix, csc_matrix, csr_matrix, dok_matrix do not accelerate the speed\n",
    "#-------\n",
    "def create_term_context_matrix(line_tuples, vocab, context_window_size=1):\n",
    "    '''Returns a numpy array containing the term context matrix for the input lines.\n",
    "\n",
    "  Inputs:\n",
    "    line_tuples: A list of tuples, containing the name of the document and \n",
    "    a tokenized line from that document.\n",
    "    vocab: A list of the tokens in the vocabulary\n",
    "\n",
    "  # NOTE: THIS DOCSTRING WAS UPDATED ON JAN 24, 12:39 PM.\n",
    "\n",
    "  Let n = len(vocab).\n",
    "\n",
    "  Returns:\n",
    "    tc_matrix: A nxn numpy array where A_ij contains the frequency with which\n",
    "        word j was found within context_window_size to the left or right of\n",
    "        word i in any sentence in the tuples.\n",
    "  '''\n",
    "    vocab_to_id = dict(zip(vocab, range(0, len(vocab))))\n",
    "#-----------\n",
    "    n_words = len(vocab)\n",
    "    # suggested by Mingyang, Counter is not an efficient choice\n",
    "#     from collections import Counter, defaultdict\n",
    "#     Dict_term_term_Counter = defaultdict(Counter)\n",
    "    ret = np.zeros((n_words,n_words), dtype = np.int32)\n",
    "    for d, wList in line_tuples:\n",
    "        idxLst = [vocab_to_id[w] for w in wList]\n",
    "        for start_pos in range(len(wList) - context_window_size):\n",
    "#             Dict_term_term_Counter[wList[start_pos]] += Counter( wList[(start_pos+1):(start_pos+context_window_size)]) \n",
    "            idx1 = vocab_to_id[wList[start_pos]]\n",
    "            for idx2 in idxLst[(start_pos+1): (start_pos+context_window_size)]: ret[idx1][idx2] +=1 #? start_pos=0, context_window_size=1\n",
    "            for idx2 in idxLst[(start_pos+1):]: ret[idx1][idx2] +=1   #?  \n",
    "#         for start_pos in range(len(wList)-context_window_size+1, len(wList)-1):\n",
    "#             Dict_term_term_Counter[wList[start_pos]] += Counter(wList[(start_pos+1):])            \n",
    "#-----------\n",
    "    ret += np.transpose(ret)\n",
    "    return ret \n",
    "\n",
    "from numpy import ma\n",
    "def create_PPMI_matrix(term_context_matrix):\n",
    "    '''Given a term context matrix, output a PPMI matrix.\n",
    "  \n",
    "  See section 15.1 in the textbook.\n",
    "  \n",
    "  Hint: Use numpy matrix and vector operations to speed up implementation.\n",
    "  \n",
    "  Input:\n",
    "    term_context_matrix: A nxn numpy array, where n is\n",
    "        the numer of tokens in the vocab.\n",
    "  \n",
    "  Returns: A nxn numpy matrix, where A_ij is equal to the\n",
    "     point-wise mutual information between the ith word\n",
    "     and the jth word in the term_context_matrix.\n",
    "  '''       \n",
    "    n_words = term_context_matrix.shape[0]\n",
    "    P_words_np = np.sum(term_context_matrix, axis = 0)\n",
    "    Denomi_nplog2 = np.log2(np.sum(P_words_np))\n",
    "    P_words_nplog2 = ma.log2(P_words_np)\n",
    "    P_words_nplog2 = P_words_nplog2.filled(0)\n",
    "    P_words_nplog2 = np.array([P_words_nplog2])\n",
    "    ret = ma.log2(term_context_matrix)\n",
    "    ret = ret.filled(- 3*Denomi_nplog2)\n",
    "#     print(\"ret.shape\",ret.shape)\n",
    "#     print(\"np.repeat(P_words_nplog2, n_words, axis = 0)\",np.repeat(P_words_nplog2, n_words, axis = 0).shape)\n",
    "    ret -= np.repeat(P_words_nplog2, n_words, axis = 0)\n",
    "    P_words_nplog2 = np.transpose(P_words_nplog2)\n",
    "#     print(\"np.repeat(P_words_nplog2, n_words, axis = 1)\",np.repeat(P_words_nplog2, n_words, axis = 1).shape)\n",
    "    ret -= np.repeat(P_words_nplog2, n_words, axis = 1)\n",
    "    ret += Denomi_nplog2\n",
    "    ret = np.maximum(ret,0)\n",
    "    return ret\n",
    "\n",
    "def create_tf_idf_matrix(term_document_matrix):\n",
    "  '''Given the term document matrix, output a tf-idf weighted version.\n",
    "\n",
    "  See section 15.2.1 in the textbook.\n",
    "  \n",
    "  Hint: Use numpy matrix and vector operations to speed up implementation.\n",
    "\n",
    "  Input:\n",
    "    term_document_matrix: Numpy array where each column represents a document \n",
    "    and each row, the frequency of a word in that document.\n",
    "\n",
    "  Returns:\n",
    "    A numpy array with the same dimension as term_document_matrix, where\n",
    "    A_ij is weighted by the inverse document frequency of document h.\n",
    "  '''\n",
    "  # YOUR CODE HERE\n",
    "  return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing term document matrix...\n",
      "Computing tf-idf matrix...\n",
      "Computing term context matrix...\n",
      "Time elapsed: 37.72837471961975 ( 37.72821092605591 )\n",
      "Computing PPMI matrix...\n",
      "Time elapsed: 165.92561173439026 ( 128.19740080833435 )\n",
      "\n",
      "The 10 most similar plays to \"Henry V\" using compute_cosine_similarity are:\n",
      "1: Henry V\n",
      "2: King John\n",
      "3: macbeth\n",
      "4: Hamlet\n",
      "5: Henry VI Part 2\n",
      "6: Henry VI Part 1\n",
      "7: Henry IV\n",
      "8: Henry VIII\n",
      "9: Richard II\n",
      "10: Loves Labours Lost\n",
      "\n",
      "The 10 most similar plays to \"Henry V\" using compute_jaccard_similarity are:\n",
      "1: Henry V\n",
      "2: Henry IV\n",
      "3: Henry VI Part 2\n",
      "4: Hamlet\n",
      "5: King John\n",
      "6: Richard III\n",
      "7: Troilus and Cressida\n",
      "8: Henry VIII\n",
      "9: Richard II\n",
      "10: Antony and Cleopatra\n",
      "\n",
      "The 10 most similar plays to \"Henry V\" using compute_dice_similarity are:\n",
      "1: Henry V\n",
      "2: Henry IV\n",
      "3: Henry VI Part 2\n",
      "4: Hamlet\n",
      "5: King John\n",
      "6: Richard III\n",
      "7: Troilus and Cressida\n",
      "8: Henry VIII\n",
      "9: Richard II\n",
      "10: Antony and Cleopatra\n",
      "Time elapsed: 166.18100357055664 ( 0.25539183616638184 )\n",
      "\n",
      "The 10 most similar words to \"juliet\" using compute_cosine_similarity on term-context frequency matrix are:\n",
      "1: juliet\n",
      "2: warwick\n",
      "3: therefore\n",
      "4: paris\n",
      "5: fortune\n",
      "6: lucius\n",
      "7: gloucester\n",
      "8: health\n",
      "9: clarence\n",
      "10: brutus\n",
      "\n",
      "The 10 most similar words to \"juliet\" using compute_jaccard_similarity on term-context frequency matrix are:\n",
      "1: juliet\n",
      "2: silvia\n",
      "3: valentine\n",
      "4: angelo\n",
      "5: montague\n",
      "6: nurse\n",
      "7: helen\n",
      "8: dromio\n",
      "9: petruchio\n",
      "10: study\n",
      "\n",
      "The 10 most similar words to \"juliet\" using compute_dice_similarity on term-context frequency matrix are:\n",
      "1: juliet\n",
      "2: silvia\n",
      "3: valentine\n",
      "4: angelo\n",
      "5: montague\n",
      "6: nurse\n",
      "7: helen\n",
      "8: dromio\n",
      "9: petruchio\n",
      "10: study\n",
      "Time elapsed: 482.272255897522 ( 316.09125232696533 )\n",
      "\n",
      "The 10 most similar words to \"juliet\" using compute_cosine_similarity on PPMI matrix are:\n",
      "1: juliet\n",
      "2: capulet\n",
      "3: mercutio\n",
      "4: thursday\n",
      "5: barnardine\n",
      "6: vauntingly\n",
      "7: romeo\n",
      "8: maskers\n",
      "9: benvolio\n",
      "10: montague\n",
      "\n",
      "The 10 most similar words to \"juliet\" using compute_jaccard_similarity on PPMI matrix are:\n",
      "1: juliet\n",
      "2: capulet\n",
      "3: romeo\n",
      "4: montague\n",
      "5: mercutio\n",
      "6: leonato\n",
      "7: silvia\n",
      "8: tybalt\n",
      "9: provost\n",
      "10: guests\n",
      "\n",
      "The 10 most similar words to \"juliet\" using compute_dice_similarity on PPMI matrix are:\n",
      "1: juliet\n",
      "2: capulet\n",
      "3: romeo\n",
      "4: montague\n",
      "5: mercutio\n",
      "6: leonato\n",
      "7: silvia\n",
      "8: tybalt\n",
      "9: provost\n",
      "10: guests\n",
      "Time elapsed: 808.5873537063599 ( 326.3150978088379 )\n"
     ]
    }
   ],
   "source": [
    "# if __name__ == '__main__':\n",
    "import time\n",
    "T0 = time.time()\n",
    "#-------------------------\n",
    "tuples, document_names, vocab = read_in_shakespeare()\n",
    "print('Computing term document matrix...')\n",
    "td_matrix = create_term_document_matrix(tuples, document_names, vocab)\n",
    "#     print(\"td_matrix\",td_matrix)\n",
    "print('Computing tf-idf matrix...')\n",
    "tf_idf_matrix = create_tf_idf_matrix(td_matrix)\n",
    "\n",
    "print('Computing term context matrix...')\n",
    "tc_matrix = create_term_context_matrix(tuples, vocab, context_window_size=2)\n",
    "T1 = time.time()\n",
    "print(\"Time elapsed:\", time.time()  - T0,\"(\",T1-T0,\")\")\n",
    "print('Computing PPMI matrix...')\n",
    "PPMI_matrix = create_PPMI_matrix(tc_matrix)\n",
    "T2 = time.time()\n",
    "print(\"Time elapsed:\", T2  - T0,\"(\",T2-T1,\")\")\n",
    "random_idx = random.randint(0, len(document_names)-1)\n",
    "similarity_fns = [compute_cosine_similarity, compute_jaccard_similarity, compute_dice_similarity]\n",
    "for sim_fn in similarity_fns:\n",
    "    print('\\nThe 10 most similar plays to \"%s\" using %s are:' % (document_names[random_idx], sim_fn.__qualname__))\n",
    "    ranks = rank_plays(random_idx, td_matrix, sim_fn)\n",
    "    for idx in range(0, 10):\n",
    "        doc_id = ranks[idx]\n",
    "        print('%d: %s' % (idx+1, document_names[doc_id]))\n",
    "T3 = time.time()\n",
    "print(\"Time elapsed:\", T3  - T0,\"(\",T3-T2,\")\")\n",
    "word = 'juliet'\n",
    "vocab_to_index = dict(zip(vocab, range(0, len(vocab))))\n",
    "for sim_fn in similarity_fns:\n",
    "    print('\\nThe 10 most similar words to \"%s\" using %s on term-context frequency matrix are:' % (word, sim_fn.__qualname__))\n",
    "    ranks = rank_words(vocab_to_index[word], tc_matrix, sim_fn)\n",
    "    for idx in range(0, 10):\n",
    "        word_id = ranks[idx]\n",
    "        print('%d: %s' % (idx+1, vocab[word_id]))\n",
    "T4 = time.time()\n",
    "print(\"Time elapsed:\", T4  - T0,\"(\",T4-T3,\")\")\n",
    "word = 'juliet'\n",
    "vocab_to_index = dict(zip(vocab, range(0, len(vocab))))\n",
    "for sim_fn in similarity_fns:\n",
    "    print('\\nThe 10 most similar words to \"%s\" using %s on PPMI matrix are:' % (word, sim_fn.__qualname__))\n",
    "    ranks = rank_words(vocab_to_index[word], PPMI_matrix, sim_fn)\n",
    "    for idx in range(0, 10):\n",
    "        word_id = ranks[idx]\n",
    "        print('%d: %s' % (idx+1, vocab[word_id]))\n",
    "T5 = time.time()\n",
    "print(\"Time elapsed:\",T5  - T0,\"(\",T5-T4,\")\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "The fourth column of will_play_text.csv contains the name of the character who spokeeach line. Using the methods described above, which characters are most similar? Least similar?\n",
      "----------------------------------------\n",
      "\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'read_character_in_shakspeare' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-de1bedffa2cc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m ''')\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mtuples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mch_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mread_character_in_shakspeare\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#, vocab\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0mn_chs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mch_names\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Computing term document matrix... (character)'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'read_character_in_shakspeare' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16 10 12\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
