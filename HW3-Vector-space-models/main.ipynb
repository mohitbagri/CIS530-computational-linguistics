{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# %config IPCompleter.greedy = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import os, csv, subprocess, re, random\n",
    "import numpy as np\n",
    "import multiprocessing, platform\n",
    "# in some cases, global pool is not suggested, leading WARNINGS\n",
    "# num_cores = multiprocessing.cpu_count()\n",
    "# print(\"num of cores:\", num_cores)\n",
    "# global pool\n",
    "# pool = multiprocessing.Pool(processes=num_cores)\n",
    "# from sklearn import metrics as SKmetrics\n",
    "import operator\n",
    "\n",
    "def read_in_shakespeare():\n",
    "    '''Reads in the Shakespeare dataset processesit into a list of tuples.\n",
    "     Also reads in the vocab and play name lists from files.\n",
    "\n",
    "    Each tuple consists of\n",
    "    tuple[0]: The name of the play\n",
    "    tuple[1] A line from the play as a list of tokenized words.\n",
    "\n",
    "    Returns:\n",
    "        tuples: A list of tuples in the above format.\n",
    "        document_names: A list of the plays present in the corpus.\n",
    "        vocab: A list of all tokens in the vocabulary.\n",
    "    '''\n",
    "\n",
    "    tuples = []\n",
    "    with open('will_play_text.csv') as f:\n",
    "        csv_reader = csv.reader(f, delimiter=';')\n",
    "        for row in csv_reader:\n",
    "            play_name = row[1]\n",
    "            line = row[5]\n",
    "            line_tokens = re.sub(r'[^a-zA-Z0-9\\s]', ' ', line).split()\n",
    "            line_tokens = [token.lower() for token in line_tokens]\n",
    "            tuples.append((play_name, line_tokens))\n",
    "    with open('vocab.txt') as f: vocab = [line.strip() for line in f.readlines()]\n",
    "    with open('play_names.txt') as f: document_names =  [line.strip() for line in f]\n",
    "    return tuples, document_names, vocab\n",
    "\n",
    "def get_row_vector(matrix, row_id): return matrix[row_id, :]\n",
    "\n",
    "def get_column_vector(matrix, col_id): return matrix[:, col_id]\n",
    "\n",
    "def compute_cosine_similarity(v1, v2): \n",
    "    '''Computes the cosine similarity of the two input vectors.\n",
    "    Inputs:()\n",
    "    v1: A nx1 numpy array \n",
    "    v2: A nx1 numpy array \n",
    "\n",
    "    Returns:\n",
    "    A scalar similarity value. # a numpy array if multiple dimension\n",
    "    '''\n",
    "    ret = sum(np.multiply(v1, v2))/ (np.linalg.norm(v1) +np.linalg.norm(v2))\n",
    "#     if 1 == len(vector1.shape) and 1 == len(vector1.shape): return ret[0] # np.double\n",
    "    return ret\n",
    "\n",
    "def compute_jaccard_similarity(vector1, vector2):\n",
    "    '''Computes the cosine similarity of the two input vectors.\n",
    "\n",
    "  Inputs:\n",
    "    vector1: A nx1 numpy array\n",
    "    vector2: A nx1 numpy array\n",
    "\n",
    "  Returns:\n",
    "    A scalar similarity value.\n",
    "  '''  \n",
    "    # http://scikit-learn.org/stable/modules/generated/sklearn.metrics.jaccard_similarity_score.html\n",
    "    ret = np.sum(np.minimum(vector1,vector2))/(np.sum(np.maximum(vector1, vector2)))\n",
    "    return ret\n",
    "\n",
    "def compute_dice_similarity(vector1, vector2):\n",
    "    '''Computes the cosine similarity of the two input vectors.\n",
    "\n",
    "  Inputs:\n",
    "    vector1: A nx1 numpy array\n",
    "    vector2: A nx1 numpy array\n",
    "\n",
    "  Returns:\n",
    "    A scalar similarity value.\n",
    "    '''\n",
    "    ret = np.sum(np.minimum(vector1,vector2))/(np.sum(vector1) + np.sum(vector2))\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# cannot put it locally inside due to multiprocessing\n",
    "# cannot use lambda func due to multiprocessing\n",
    "def process_impv(Tuple):#         counts, Vocab = Tuple\n",
    "        return [Tuple[0][w] for w in Tuple[1]]\n",
    "def create_term_document_matrix(line_tuples, document_names, vocab):\n",
    "    '''Returns a numpy array containing the term document matrix for the input lines.\n",
    "    Inputs:\n",
    "    line_tuples: A list of tuples, containing the name of the document and \n",
    "    a tokenized line from that document.\n",
    "    document_names: A list of the document names\n",
    "    vocab: A list of the tokens in the vocabulary\n",
    "    # NOTE: THIS DOCSTRING WAS UPDATED ON JAN 24, 12:39 PM.\n",
    "\n",
    "    Let m = len(vocab) and n = len(document_names).\n",
    "\n",
    "    Returns:\n",
    "        td_matrix: A mxn numpy array where the number of rows is the number of words\n",
    "          and each column corresponds to a document. A_ij contains the\n",
    "          frequency with which word i occurs in document j.\n",
    "    '''\n",
    "    from collections import Counter, defaultdict\n",
    "    Dict_doc_words_Counter = defaultdict(Counter)\n",
    "    for d, wList in line_tuples: Dict_doc_words_Counter[d] += Counter(wList)\n",
    "#     vocab_to_id = dict(zip(vocab, range(0, len(vocab))))\n",
    "#     docname_to_id = dict(zip(document_names, range(0, len(document_names))))\n",
    "# --------------\n",
    "# not parallel\n",
    "    ret = [[Dict_doc_words_Counter[doc][w] for w in vocab] for doc in document_names] # fastest on Macbook\n",
    "# not parallel\n",
    "# --------------\n",
    "# parallel\n",
    "#     num_cores = multiprocessing.cpu_count()\n",
    "#     print(\"num of cores:\", num_cores)\n",
    "#     pool = multiprocessing.Pool(processes=num_cores)\n",
    "#     ret = pool.map(process_impv, ((Dict_doc_words_Counter[doc],vocab) for doc in document_names) ) \n",
    "# parallel\n",
    "# --------------\n",
    "    return np.transpose(np.array(ret))\n",
    "\n",
    "\n",
    "def create_term_context_matrix(line_tuples, vocab, context_window_size=1):\n",
    "    '''Returns a numpy array containing the term context matrix for the input lines.\n",
    "\n",
    "  Inputs:\n",
    "    line_tuples: A list of tuples, containing the name of the document and \n",
    "    a tokenized line from that document.\n",
    "    vocab: A list of the tokens in the vocabulary\n",
    "\n",
    "  # NOTE: THIS DOCSTRING WAS UPDATED ON JAN 24, 12:39 PM.\n",
    "\n",
    "  Let n = len(vocab).\n",
    "\n",
    "  Returns:\n",
    "    tc_matrix: A nxn numpy array where A_ij contains the frequency with which\n",
    "        word j was found within context_window_size to the left or right of\n",
    "        word i in any sentence in the tuples.\n",
    "  '''\n",
    "#     vocab_to_id = dict(zip(vocab, range(0, len(vocab))))\n",
    "    windows_tuple = [(wList[start_pos], wList[(start_pos+1):(start_pos+context_window_size)])  \\\n",
    "                for d, wList in line_tuples for start_pos in range(len(wList)-context_window_size) ]\n",
    "    windows_tuple += [(wList[start_pos], wList[(start_pos+1):])  \\\n",
    "                for d, wList in line_tuples for start_pos in range(len(wList)-context_window_size+1, len(wList)-1) ]\n",
    "    from collections import Counter, defaultdict\n",
    "    Dict_term_term_Counter = defaultdict(Counter)\n",
    "    for w1,Lst_w2 in windows_tuple: Dict_term_term_Counter[w1] += Counter(Lst_w2)\n",
    "    #\n",
    "#     for d, wList in line_tuples: \n",
    "#         for start_pos in range(len(wList)-context_window_size+1):\n",
    "#             Dict_term_term_Counter[wList[start_pos]] += Counter(wList[(start_pos+1):(start_pos+context_window_size)])\n",
    "    ret = np.array([[Dict_term_term_Counter[w1][w2] for w2 in vocab] for w1 in vocab])\n",
    "    ret += np.transpose(ret)\n",
    "    return ret \n",
    "\n",
    "from numpy import ma\n",
    "def create_PPMI_matrix(term_context_matrix):\n",
    "    '''Given a term context matrix, output a PPMI matrix.\n",
    "  \n",
    "  See section 15.1 in the textbook.\n",
    "  \n",
    "  Hint: Use numpy matrix and vector operations to speed up implementation.\n",
    "  \n",
    "  Input:\n",
    "    term_context_matrix: A nxn numpy array, where n is\n",
    "        the numer of tokens in the vocab.\n",
    "  \n",
    "  Returns: A nxn numpy matrix, where A_ij is equal to the\n",
    "     point-wise mutual information between the ith word\n",
    "     and the jth word in the term_context_matrix.\n",
    "  '''       \n",
    "    n_words = term_context_matrix.shape[0]\n",
    "    P_words_np = np.sum(term_context_matrix, axis = 0)\n",
    "    Denomi_nplog2 = np.log2(np.sum(P_words_np))\n",
    "    P_words_nplog2 = ma.log2(P_words_np)\n",
    "    P_words_nplog2 = P_words_nplog2.filled(0)\n",
    "    P_words_nplog2 = np.array([P_words_nplog2])\n",
    "    ret = ma.log2(term_context_matrix)\n",
    "    ret = ret.filled(0)\n",
    "#     print(\"ret.shape\",ret.shape)\n",
    "#     print(\"np.repeat(P_words_nplog2, n_words, axis = 0)\",np.repeat(P_words_nplog2, n_words, axis = 0).shape)\n",
    "    ret -= np.repeat(P_words_nplog2, n_words, axis = 0)\n",
    "    P_words_nplog2 = np.transpose(P_words_nplog2)\n",
    "#     print(\"np.repeat(P_words_nplog2, n_words, axis = 1)\",np.repeat(P_words_nplog2, n_words, axis = 1).shape)\n",
    "    ret -= np.repeat(P_words_nplog2, n_words, axis = 1)\n",
    "    ret += Denomi_nplog2\n",
    "    return ret\n",
    "\n",
    "def create_tf_idf_matrix(term_document_matrix):\n",
    "  '''Given the term document matrix, output a tf-idf weighted version.\n",
    "\n",
    "  See section 15.2.1 in the textbook.\n",
    "  \n",
    "  Hint: Use numpy matrix and vector operations to speed up implementation.\n",
    "\n",
    "  Input:\n",
    "    term_document_matrix: Numpy array where each column represents a document \n",
    "    and each row, the frequency of a word in that document.\n",
    "\n",
    "  Returns:\n",
    "    A numpy array with the same dimension as term_document_matrix, where\n",
    "    A_ij is weighted by the inverse document frequency of document h.\n",
    "  '''\n",
    "  # YOUR CODE HERE\n",
    "  return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# def process_impv_rk_pls(args):\n",
    "#     similarity_fn,v1,v2,i = args\n",
    "#     return (i,similarity_fn(v1,v2))\n",
    "def rank_plays(target_play_index, term_document_matrix, similarity_fn):\n",
    "    ''' Ranks the similarity of all of the plays to the target play.\n",
    "  # NOTE: THIS DOCSTRING WAS UPDATED ON JAN 24, 12:51 PM.\n",
    "  Inputs:\n",
    "    target_play_index: The integer index of the play we want to compare all others against.\n",
    "    term_document_matrix: The term-document matrix as a mxn numpy array.\n",
    "    similarity_fn: Function that should be used to compared vectors for two\n",
    "      documents. Either compute_dice_similarity, compute_jaccard_similarity, or\n",
    "      compute_cosine_similarity.\n",
    "  Returns:\n",
    "    A length-n list of integer indices corresponding to play names,\n",
    "    ordered by decreasing similarity to the play indexed by target_play_index\n",
    "  '''\n",
    "#     print(\"term_document_matrix\",term_document_matrix.shape)\n",
    "    n_docs = term_document_matrix.shape[1]\n",
    "    v1 = term_document_matrix[:,target_play_index]\n",
    "    # this is a easy question, no need for parallel computing\n",
    "    # -------\n",
    "    # not parallel\n",
    "    SimDict= dict((i,similarity_fn(v1,term_document_matrix[:,i])) for i in range(target_play_index-1))\n",
    "    SimDict.update(dict((i,similarity_fn(v1,term_document_matrix[:,i])) for i in range(target_play_index+1,n_docs)))\n",
    "    # not parallel\n",
    "    #-------\n",
    "    # parallel\n",
    "#     num_cores = multiprocessing.cpu_count()\n",
    "# #     print(\"num of cores:\", num_cores)\n",
    "#     pool = multiprocessing.Pool(processes=num_cores)\n",
    "#     SimLst_temp = pool.map(process_impv_rk_pls, [(similarity_fn,v1,term_document_matrix[:,i],i) for i in range(target_play_index-1)] ) \n",
    "#     SimDict= dict(SimLst_temp)\n",
    "#     SimLst_temp = pool.map(process_impv_rk_pls, [(similarity_fn,v1,term_document_matrix[:,i],i) for i in range(target_play_index+1,n_docs)] ) \n",
    "#     SimDict.update(dict(SimLst_temp))\n",
    "    # parallel\n",
    "    #-------\n",
    "    L = dict(sorted(SimDict.items(), key=operator.itemgetter(1)))\n",
    "    retL = list(L.keys())\n",
    "    retL.reverse()\n",
    "#     print(\"rank plays (\",similarity_fn.__name__,\"):\", retL)\n",
    "    return retL\n",
    "def process_impv_rk_wds(args):\n",
    "    similarity_fn,v1,v2,i = args\n",
    "    return (i,similarity_fn(v1,v2))\n",
    "def rank_words(target_word_index, matrix, similarity_fn):\n",
    "    ''' Ranks the similarity of all of the words to the target word.\n",
    "  # NOTE: THIS DOCSTRING WAS UPDATED ON JAN 24, 12:51 PM.\n",
    "  Inputs:\n",
    "    target_word_index: The index of the word we want to compare all others against.\n",
    "    matrix: Numpy matrix where the ith row represents a vector embedding of the ith word.\n",
    "    similarity_fn: Function that should be used to compared vectors for two word\n",
    "      ebeddings. Either compute_dice_similarity, compute_jaccard_similarity, or\n",
    "      compute_cosine_similarity.\n",
    "\n",
    "  Returns:\n",
    "    A length-n list of integer word indices, ordered by decreasing similarity to the \n",
    "    target word indexed by word_index\n",
    "  '''\n",
    "    n_words = matrix.shape[1]\n",
    "    v1 = matrix[:,target_word_index]\n",
    "    #---------------\n",
    "    # not parallel\n",
    "#     SimDict= dict((i,similarity_fn(v1,matrix[:,i])) for i in range(target_word_index-1))\n",
    "#     SimDict.update(dict((i,similarity_fn(v1,matrix[:,i])) for i in range(target_word_index+1,n_words)))\n",
    "    # not parallel\n",
    "    #---------------\n",
    "    num_cores = multiprocessing.cpu_count()\n",
    "#     print(\"num of cores:\", num_cores)\n",
    "    pool = multiprocessing.Pool(processes=num_cores)\n",
    "    SimLst_temp = pool.map(process_impv_rk_wds, [(similarity_fn,v1,matrix[:,i],i) for i in range(target_word_index-1)])\n",
    "    SimDict= dict(SimLst_temp)\n",
    "    SimLst_temp = pool.map(process_impv_rk_wds, [(similarity_fn,v1,matrix[:,i],i) for i in range(target_word_index+1,n_words)])\n",
    "    SimDict.update(dict(SimLst_temp))\n",
    "    L = dict(sorted(SimDict.items(), key=operator.itemgetter(1)))\n",
    "    retL = list(L.keys())\n",
    "    retL.reverse()\n",
    "#     print(\"rank words: retL\", retL)\n",
    "    return retL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing term document matrix...\n",
      "Computing tf-idf matrix...\n",
      "Computing term context matrix...\n",
      "Time elapsed: 324.63192796707153\n",
      "Computing PPMI matrix...\n",
      "Time elapsed: 407.53529691696167\n",
      "\n",
      "The 10 most similar plays to \"Much Ado about nothing\" using compute_cosine_similarity are:\n",
      "1: Hamlet\n",
      "2: Richard III\n",
      "3: Othello\n",
      "4: Cymbeline\n",
      "5: Coriolanus\n",
      "6: A Winters Tale\n",
      "7: King Lear\n",
      "8: Troilus and Cressida\n",
      "9: Alls well that ends well\n",
      "10: Henry IV\n",
      "\n",
      "The 10 most similar plays to \"Much Ado about nothing\" using compute_jaccard_similarity are:\n",
      "1: As you like it\n",
      "2: Twelfth Night\n",
      "3: Merchant of Venice\n",
      "4: Alls well that ends well\n",
      "5: Measure for measure\n",
      "6: Taming of the Shrew\n",
      "7: Merry Wives of Windsor\n",
      "8: Othello\n",
      "9: A Winters Tale\n",
      "10: Romeo and Juliet\n",
      "\n",
      "The 10 most similar plays to \"Much Ado about nothing\" using compute_dice_similarity are:\n",
      "1: As you like it\n",
      "2: Twelfth Night\n",
      "3: Merchant of Venice\n",
      "4: Alls well that ends well\n",
      "5: Measure for measure\n",
      "6: Taming of the Shrew\n",
      "7: Merry Wives of Windsor\n",
      "8: Othello\n",
      "9: A Winters Tale\n",
      "10: Romeo and Juliet\n",
      "Time elapsed: 407.7221369743347\n",
      "\n",
      "The 10 most similar words to \"juliet\" using compute_cosine_similarity on term-context frequency matrix are:\n",
      "1: therefore\n",
      "2: yet\n",
      "3: gloucester\n",
      "4: then\n",
      "5: warwick\n",
      "6: death\n",
      "7: fortune\n",
      "8: blood\n",
      "9: caesar\n",
      "10: here\n",
      "\n",
      "The 10 most similar words to \"juliet\" using compute_jaccard_similarity on term-context frequency matrix are:\n",
      "1: silvia\n",
      "2: valentine\n",
      "3: cloten\n",
      "4: lysander\n",
      "5: falstaff\n",
      "6: health\n",
      "7: antonio\n",
      "8: petruchio\n",
      "9: study\n",
      "10: cymbeline\n",
      "\n",
      "The 10 most similar words to \"juliet\" using compute_dice_similarity on term-context frequency matrix are:\n",
      "1: silvia\n",
      "2: valentine\n",
      "3: cloten\n",
      "4: lysander\n",
      "5: falstaff\n",
      "6: health\n",
      "7: antonio\n",
      "8: petruchio\n",
      "9: study\n",
      "10: cymbeline\n",
      "Time elapsed: 711.7907800674438\n",
      "\n",
      "The 10 most similar words to \"juliet\" using compute_cosine_similarity on PPMI matrix are:\n",
      "1: everlastingly\n",
      "2: chartreux\n",
      "3: clothair\n",
      "4: heartsick\n",
      "5: repeating\n",
      "6: loitering\n",
      "7: drily\n",
      "8: forefathers\n",
      "9: pinked\n",
      "10: untrodden\n",
      "\n",
      "The 10 most similar words to \"juliet\" using compute_jaccard_similarity on PPMI matrix are:\n",
      "1: lysander\n",
      "2: slender\n",
      "3: lewis\n",
      "4: weeping\n",
      "5: triumph\n",
      "6: crack\n",
      "7: table\n",
      "8: prisoner\n",
      "9: pair\n",
      "10: encounter\n",
      "\n",
      "The 10 most similar words to \"juliet\" using compute_dice_similarity on PPMI matrix are:\n"
     ]
    }
   ],
   "source": [
    "# if __name__ == '__main__':\n",
    "import time\n",
    "T0 = time.time()\n",
    "#-------------------------\n",
    "tuples, document_names, vocab = read_in_shakespeare()\n",
    "print('Computing term document matrix...')\n",
    "td_matrix = create_term_document_matrix(tuples, document_names, vocab)\n",
    "#     print(\"td_matrix\",td_matrix)\n",
    "print('Computing tf-idf matrix...')\n",
    "tf_idf_matrix = create_tf_idf_matrix(td_matrix)\n",
    "\n",
    "print('Computing term context matrix...')\n",
    "tc_matrix = create_term_context_matrix(tuples, vocab, context_window_size=2)\n",
    "print(\"Time elapsed:\", time.time()  - T0)\n",
    "print('Computing PPMI matrix...')\n",
    "PPMI_matrix = create_PPMI_matrix(tc_matrix)\n",
    "print(\"Time elapsed:\", time.time()  - T0)\n",
    "random_idx = random.randint(0, len(document_names)-1)\n",
    "similarity_fns = [compute_cosine_similarity, compute_jaccard_similarity, compute_dice_similarity]\n",
    "for sim_fn in similarity_fns:\n",
    "    print('\\nThe 10 most similar plays to \"%s\" using %s are:' % (document_names[random_idx], sim_fn.__qualname__))\n",
    "    ranks = rank_plays(random_idx, td_matrix, sim_fn)\n",
    "    for idx in range(0, 10):\n",
    "        doc_id = ranks[idx]\n",
    "        print('%d: %s' % (idx+1, document_names[doc_id]))\n",
    "print(\"Time elapsed:\", time.time()  - T0)\n",
    "word = 'juliet'\n",
    "vocab_to_index = dict(zip(vocab, range(0, len(vocab))))\n",
    "for sim_fn in similarity_fns:\n",
    "    print('\\nThe 10 most similar words to \"%s\" using %s on term-context frequency matrix are:' % (word, sim_fn.__qualname__))\n",
    "    ranks = rank_words(vocab_to_index[word], tc_matrix, sim_fn)\n",
    "    for idx in range(0, 10):\n",
    "        word_id = ranks[idx]\n",
    "        print('%d: %s' % (idx+1, vocab[word_id]))\n",
    "print(\"Time elapsed:\", time.time()  - T0)\n",
    "word = 'juliet'\n",
    "vocab_to_index = dict(zip(vocab, range(0, len(vocab))))\n",
    "for sim_fn in similarity_fns:\n",
    "    print('\\nThe 10 most similar words to \"%s\" using %s on PPMI matrix are:' % (word, sim_fn.__qualname__))\n",
    "    ranks = rank_words(vocab_to_index[word], PPMI_matrix, sim_fn)\n",
    "    for idx in range(0, 10):\n",
    "        word_id = ranks[idx]\n",
    "        print('%d: %s' % (idx+1, vocab[word_id]))\n",
    "\n",
    "print(\"Time elapsed:\", time.time()  - T0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
