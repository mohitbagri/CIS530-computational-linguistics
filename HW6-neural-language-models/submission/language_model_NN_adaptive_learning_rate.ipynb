{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# http://pytorch.org/tutorials/intermediate/char_rnn_classification_tutorial.html\n",
    "from __future__ import unicode_literals, print_function, division\n",
    "from io import open\n",
    "import glob\n",
    "tri_FLAG = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def findFiles(path): return glob.glob(path)\n",
    "\n",
    "# print(findFiles('data/names/*.txt'))\n",
    "# print(findFiles('train/*.txt'))\n",
    "\n",
    "import unicodedata\n",
    "import string\n",
    "\n",
    "all_letters = string.ascii_letters + \" .,;'\"\n",
    "n_letters = len(all_letters)\n",
    "\n",
    "# Turn a Unicode string to plain ASCII, thanks to http://stackoverflow.com/a/518232/2809427\n",
    "def unicodeToAscii(s):\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "        and c in all_letters\n",
    "    )\n",
    "\n",
    "# print(unicodeToAscii('Ślusàrski'))\n",
    "\n",
    "# Build the category_lines dictionary, a list of names per language\n",
    "category_lines = {}\n",
    "all_categories = [] # yezheng: this is a global variable\n",
    "\n",
    "# Read a file and split into lines\n",
    "def readLines(filename):\n",
    "    try: # yezheng -- tackle with \"ISO-8859-1\"\n",
    "        fd = open(filename, encoding='utf-8', errors='ignore')\n",
    "    except:\n",
    "        fd = open(filename, encoding=\"ISO-8859-1\")\n",
    "    lines = fd.read().strip().split('\\n')\n",
    "    return [unicodeToAscii(line) for line in lines]\n",
    "    fd.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train/in.txt 3000\n",
      "train/pk.txt 3000\n",
      "train/fr.txt 3000\n",
      "train/af.txt 3000\n",
      "train/cn.txt 3000\n",
      "train/za.txt 3000\n",
      "train/fi.txt 3000\n",
      "train/ir.txt 3000\n",
      "train/de.txt 3000\n",
      "n_categories=9 n_letters57\n",
      "dict_keys(['in', 'pk', 'fr', 'af', 'cn', 'za', 'fi', 'ir', 'de'])\n"
     ]
    }
   ],
   "source": [
    "# for filename in findFiles('data/names/*.txt'):\n",
    "for filename in findFiles('train/*.txt'):\n",
    "    category = filename.split('/')[-1].split('.')[0]\n",
    "    all_categories.append(category)\n",
    "    lines = readLines(filename)\n",
    "    category_lines[category] = lines\n",
    "    print(filename,len(lines))\n",
    "\n",
    "n_categories = len(all_categories)\n",
    "\n",
    "\n",
    "\n",
    "category_lines_val = {}\n",
    "global num_tot_val\n",
    "num_tot_val = 0\n",
    "for filename in findFiles('val/*.txt'):\n",
    "    category = filename.split('/')[-1].split('.')[0]\n",
    "    lines = readLines(filename)\n",
    "    category_lines_val[category] = lines\n",
    "    num_tot_val += len(lines)\n",
    "\n",
    "print(f\"n_categories={n_categories} n_letters{n_letters}\")\n",
    "print(category_lines_val.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['yungming', 'xingzhuang', 'liren', 'hongjiaotian', 'guanrenling']\n",
      "abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ .,;'\n",
      "['in', 'pk', 'fr', 'af', 'cn', 'za', 'fi', 'ir', 'de']\n"
     ]
    }
   ],
   "source": [
    "print(category_lines['cn'][:5])\n",
    "print(all_letters)\n",
    "print(all_categories)\n",
    "\n",
    "\n",
    "# In[5]:\n",
    "\n",
    "\n",
    "import torch\n",
    "# Find letter index from all_letters, e.g. \"a\" = 0\n",
    "def letterToIndex(letter): return all_letters.find(letter)\n",
    "\n",
    "# Just for demonstration, turn a letter into a <1 x n_letters> Tensor\n",
    "def letterToTensor(letter):\n",
    "    tensor = torch.zeros(1, n_letters)\n",
    "    tensor[0][letterToIndex(letter)] = 1\n",
    "    return tensor\n",
    "\n",
    "# Turn a line into a <line_length x 1 x n_letters>,\n",
    "# or an array of one-hot letter vectors\n",
    "def lineToTensor(line):\n",
    "    tensor = torch.zeros(len(line), 1, n_letters)\n",
    "    for li, letter in enumerate(line): \n",
    "        tensor[li][0][letterToIndex(letter)] = 1\n",
    "    return tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 513])\n",
      "torch.Size([5, 1, 513])\n"
     ]
    }
   ],
   "source": [
    "#yezheng: from HW5: evaluating trigram\n",
    "from collections import *\n",
    "from random import random\n",
    "import numpy as np\n",
    "def train_char_lm(fname, order=2, add_k=1):\n",
    "  ''' Trains a language model.\n",
    "  This code was borrowed from http://nbviewer.jupyter.org/gist/yoavg/d76121dfde2618422139\n",
    "  Inputs:\n",
    "    fname: Path to a text corpus.\n",
    "    order: The length of the n-grams.\n",
    "    add_k: k value for add-k smoothing. NOT YET IMPLMENTED\n",
    "\n",
    "  Returns:\n",
    "    A dictionary mapping from n-grams of length n to a list of tuples.\n",
    "    Each tuple consists of a possible net character and its probability.\n",
    "  '''\n",
    "  # TODO: Add your implementation of add-k smoothing.\n",
    "  #   data = open(fname).read()\n",
    "#-------------\n",
    "  lm = defaultdict(Counter)\n",
    "  fnameLst = fname\n",
    "  if isinstance(fname, str): fnameLst = [fname]\n",
    "  lm = defaultdict(Counter)\n",
    "#   print(fnameLst)\n",
    "  for fnm in fnameLst:\n",
    "      try: # yezheng -- tackle with \"ISO-8859-1\"\n",
    "            fd = open(fnm, encoding='utf-8', errors='ignore')\n",
    "      except:\n",
    "            fd = open(fnm, encoding=\"ISO-8859-1\")\n",
    "      AllChars = set()\n",
    "      for data in fd.readlines():\n",
    "          data = data.lower()\n",
    "          AllChars.update(data)\n",
    "          pad = \"~\" * order # yezheng: this is just setting beginning of a line -- just like <s><s> mentioned in chapter 4\n",
    "          data = pad + data\n",
    "          for i in range(len(data)-order):\n",
    "            history, char = data[i:i+order], data[i+order]\n",
    "            lm[history][char]+=1\n",
    "          del history\n",
    "          del char\n",
    "          del i\n",
    "      for his in lm.keys():\n",
    "        for ch in AllChars: lm[his][ch]+=0 \n",
    "      fd.close()\n",
    "#-------------\n",
    "  def normalize(counter): # input is a dictionary\n",
    "    s = float(sum(counter.values())) + add_k *len(counter)\n",
    "    return [(c,(cnt+add_k)/s) for c,cnt in counter.items()]\n",
    "  outlm = {hist:normalize(chars) for hist, chars in lm.items()}\n",
    "  return outlm\n",
    "\n",
    "# def perplexity_yezheng_string(cityname, lm, order=2):\n",
    "#   '''Computes the perplexity of a text file given the language model.\n",
    "#   Inputs:\n",
    "#     test_filename: path to text file\n",
    "#     lms: The output from calling train_char_lms.\n",
    "#     order: The length of the n-grams in the language model. #yezheng: order can be read from lm?\n",
    "#   Outputs:\n",
    "#     max_labels: a list of predicted labels\n",
    "#   '''\n",
    "#   #order = len(list(lm.keys())[0]) #yezheng: I think it should not be an argument\n",
    "#   pad = \"~\" * order\n",
    "#   data = pad + cityname\n",
    "#   data = data.lower()\n",
    "#   # TODO: YOUR CODE HERE\n",
    "#   # Daphne: make sure (num of characters > order)\n",
    "#   logPP = 0\n",
    "#   for i in range(len(data)-order):\n",
    "#     history, char = data[i:(i+order)], data[i+order]   \n",
    "#     if history not in lm:\n",
    "#       logPP += np.log2(8.0/len(lm)) # float(\"-inf\") # yezheng: deal with unknowns\n",
    "#     else:\n",
    "#       dict_temp = dict(lm[history])\n",
    "#       if char not in dict_temp:\n",
    "#         logPP += np.log2(8.0/len(lm)) #float(\"-inf\")  # yezheng: deal with unknowns\n",
    "#       else: logPP += np.log2(dict_temp[char])\n",
    "#   return logPP/len(data) #yezheng: we forget to divide this by len(data) in HW5\n",
    "\n",
    "import os\n",
    "lms_dict_tri = {}# a dictionary of lms\n",
    "for filename in os.listdir('train'):\n",
    "    filepath = ['train/' + filename,'val/' + filename]\n",
    "    lms_dict_tri[filename[:2]] = train_char_lm(filepath)  #, order=order, add_k = AddK\n",
    "\n",
    "def trigramTensor(line, lms_dict, order=2): # n_label*n_letters\n",
    "    tensor = torch.zeros(len(line), 1, n_categories*n_letters)\n",
    "    data = \"~\" *order + line\n",
    "    input_feature = []\n",
    "    for li in range(len(data)-order):\n",
    "        for idx_lm,lm_name in enumerate(lms_dict.keys()):\n",
    "            history, ch = data[li:(li+order)], data[li+order]   \n",
    "            lm = lms_dict[lm_name]\n",
    "            if history not in lm:\n",
    "              for j in range(n_letters): \n",
    "#                 print(\"tensor[li][0][idx_lm*n_letters + j]\",tensor[li][0][idx_lm*n_letters + j])\n",
    "                tensor[li][0][idx_lm*n_letters + j] =np.log2(8.0/len(lm)) \n",
    "            else:\n",
    "              dict_temp = dict(lm[history])\n",
    "              if ch not in dict_temp:\n",
    "                tensor[li][0][idx_lm*n_letters + letterToIndex(ch) ]= np.log2(8.0/len(lm)) #float(\"-inf\")  # yezheng: deal with unknowns\n",
    "              else:  \n",
    "                tensor[li][0][idx_lm*n_letters + letterToIndex(ch) ] = np.log2(dict_temp[ch])\n",
    "    return tensor\n",
    "\n",
    "# def lineToTensor(line):\n",
    "#     tensor = torch.zeros(len(line), 1, n_letters)\n",
    "#     for li, letter in enumerate(line): \n",
    "#         tensor[li][0][letterToIndex(letter)] = 1\n",
    "#     return tensor\n",
    "\n",
    "if tri_FLAG:\n",
    "    print(trigramTensor('J',lms_dict_tri).size())\n",
    "    print(trigramTensor('Jones',lms_dict_tri).size())\n",
    "else:\n",
    "    print(letterToTensor('J'))\n",
    "    print(lineToTensor('Jones').size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch import div as tchdiv\n",
    "\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(RNN, self).__init__()\n",
    "        self.input_size = input_size # yezheng\n",
    "        self.hidden_size = hidden_size\n",
    "        self.i2h = nn.Linear(input_size + hidden_size, hidden_size)\n",
    "        self.i2o = nn.Linear(input_size + hidden_size, output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "#         if tri_FLAG: print(\"tri debug\", input.size(),hidden.size())\n",
    "        combined = torch.cat((input, hidden), 1)\n",
    "        hidden = self.i2h(combined)\n",
    "        output = self.i2o(combined)\n",
    "        if tri_FLAG:\n",
    "#             output = tchdiv(self.softmax(output),self.input_size) # yezheng: should be divided by input_size\n",
    "            output = self.softmax(output)\n",
    "        else:\n",
    "            output = self.softmax(output) \n",
    "        #yezheng: self.softmax: transforming into \"probability\"\n",
    "        return output, hidden\n",
    "    def initHidden(self): return Variable(torch.zeros(1, self.hidden_size))\n",
    "\n",
    "n_hidden = 128\n",
    "#yezheng: initialization \n",
    "if tri_FLAG: \n",
    "    rnn = RNN(n_letters *n_categories, n_hidden, n_categories)  # yezheng: trigramTensor\n",
    "else: \n",
    "    rnn = RNN(n_letters, n_hidden, n_categories)  # yezheng: LineToTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "if tri_FLAG:\n",
    "    input = Variable(trigramTensor('A',lms_dict_tri))\n",
    "    hidden = Variable(torch.zeros(1, n_hidden))\n",
    "    output, next_hidden = rnn(input[0], hidden)\n",
    "else:\n",
    "    input = Variable(letterToTensor('A'))\n",
    "    hidden = Variable(torch.zeros(1, n_hidden))\n",
    "    output, next_hidden = rnn(input, hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      "-2.4314 -1.8531 -2.3148 -2.5761 -2.0136 -2.6599 -2.5785 -2.3575 -1.5776\n",
      "[torch.FloatTensor of size 1x9]\n",
      "\n",
      "('de', 8)\n"
     ]
    }
   ],
   "source": [
    "if tri_FLAG:\n",
    "    input = Variable(trigramTensor('Albert',lms_dict_tri))\n",
    "    hidden = Variable(torch.zeros(1, n_hidden))\n",
    "    output, next_hidden = rnn(input[0], hidden) # yezheng: strange: I though output should have size related with n_labels\n",
    "    print(output)\n",
    "else:\n",
    "    input = Variable(lineToTensor('Albert'))\n",
    "    hidden = Variable(torch.zeros(1, n_hidden))\n",
    "    output, next_hidden = rnn(input[0], hidden) # yezheng: strange: I though output should have size related with n_labels\n",
    "    print(output)\n",
    "\n",
    "\n",
    "# In[10]:\n",
    "\n",
    "\n",
    "def categoryFromOutput(output):\n",
    "    top_n, top_i = output.data.topk(1) # Tensor out of Variable with .data\n",
    "    category_i = top_i[0][0]\n",
    "    return all_categories[category_i], category_i\n",
    "print(categoryFromOutput(output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "category = fi / line = fintesti\n",
      "category = pk / line = parbanj\n",
      "category = fi / line = matkisto\n",
      "category = cn / line = zhongzhou xiang\n",
      "category = fr / line = la poueze\n",
      "category = de / line = caserio hoya de munoz\n",
      "category = af / line = metachina\n",
      "category = pk / line = goth aliu khan\n",
      "category = ir / line = bihar\n",
      "category = cn / line = changyaoxian\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "def randomChoice(l): return l[random.randint(0, len(l) - 1)]\n",
    "def randomTrainingExample():\n",
    "    category = randomChoice(all_categories)\n",
    "    line = randomChoice(category_lines[category])\n",
    "    category_tensor = Variable(torch.LongTensor([all_categories.index(category)]))\n",
    "    if tri_FLAG: line_tensor = Variable(trigramTensor(line,lms_dict_tri)) # yezheng \n",
    "    else: line_tensor = Variable(lineToTensor(line)) \n",
    "    return category, line, category_tensor, line_tensor\n",
    "for i in range(10):\n",
    "    category, line, category_tensor, line_tensor = randomTrainingExample()\n",
    "    print('category =', category, '/ line =', line)\n",
    "\n",
    "\n",
    "# In[12]:\n",
    "\n",
    "\n",
    "criterion = nn.NLLLoss() #Negative Log Likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n"
     ]
    }
   ],
   "source": [
    "def evaluate(line_tensor):\n",
    "    hidden = rnn.initHidden()\n",
    "    for i in range(line_tensor.size()[0]):\n",
    "        output, hidden = rnn(line_tensor[i], hidden)\n",
    "    return output\n",
    "\n",
    "def predict(input_line):\n",
    "#     print('\\n> %s' % input_line)\n",
    "    output = evaluate(Variable(trigramTensor(input_line, lms_dict_tri)))\n",
    "#     topv, topi = output.data.topk(1, 1, True)\n",
    "    guess, guess_i = categoryFromOutput(output)\n",
    "#     category_index = topi[0][0]\n",
    "#         print('(%.2f) %s' % (value, all_categories[category_index]))\n",
    "    return guess\n",
    "\n",
    "import csv\n",
    "del category\n",
    "# print(len(category_lines_val['de']))\n",
    "def Curr_Err_Rate_dev(): \n",
    "    global num_tot_val\n",
    "    num_err = 0\n",
    "    for catname in all_categories:\n",
    "        temp = num_err\n",
    "        num_err += sum([not catname == predict(cityname) for cityname in category_lines_val[catname]])\n",
    "#         print(catname,\"num_err\",num_err - temp)\n",
    "    return num_err*1.0/num_tot_val\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you set this too high, it might explode. If too low, it might not learn\n",
    "if tri_FLAG:\n",
    "    learning_rate = 0.001\n",
    "else:\n",
    "    learning_rate = 0.0004 \n",
    "# yezheng: 0.005 in the tutorial for their data\n",
    "def train(category_tensor, line_tensor):\n",
    "    hidden = rnn.initHidden()\n",
    "    rnn.zero_grad()\n",
    "#     print(\"line_tensor.size()[0]\",line_tensor.size()[0]) #yezheng: this should be all the way 1\n",
    "    for i in range(line_tensor.size()[0]): \n",
    "        output, hidden = rnn(line_tensor[i], hidden)\n",
    "    loss = criterion(output, category_tensor) #yezheng: nn.NLLLoss(output, label) # label = 0,1,\\ldots, 9\n",
    "    loss.backward()\n",
    "    # Add parameters' gradients to their values, multiplied by learning rate\n",
    "    for p in rnn.parameters(): \n",
    "        p.data.add_(-learning_rate, p.grad.data)\n",
    "#     torch.nn.Dropout(p=0.5, inplace=False) #yezheng\n",
    "    return output, loss.data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in num_err 79\n",
      "pk num_err 62\n",
      "fr num_err 55\n",
      "af num_err 84\n",
      "cn num_err 26\n",
      "za num_err 80\n",
      "fi num_err 64\n",
      "ir num_err 69\n",
      "de num_err 63\n",
      "5000 5% (0m 23s) 0.6467 kafr al maraziqah / af ✗ (fr)\n",
      "in num_err 58\n",
      "pk num_err 51\n",
      "fr num_err 57\n",
      "af num_err 74\n",
      "cn num_err 11\n",
      "za num_err 82\n",
      "fi num_err 56\n",
      "ir num_err 75\n",
      "de num_err 65\n",
      "10000 10% (0m 45s) 0.5878 via hernandez / de ✓\n",
      "in num_err 62\n",
      "pk num_err 42\n",
      "fr num_err 35\n",
      "af num_err 65\n",
      "cn num_err 21\n",
      "za num_err 49\n",
      "fi num_err 46\n",
      "ir num_err 63\n",
      "de num_err 55\n",
      "15000 15% (1m 11s) 0.4867 sedeyrehye pa'in / de ✗ (ir)\n",
      "in num_err 75\n",
      "pk num_err 35\n",
      "fr num_err 46\n",
      "af num_err 69\n",
      "cn num_err 16\n",
      "za num_err 42\n",
      "fi num_err 38\n",
      "ir num_err 71\n",
      "de num_err 58\n",
      "20000 20% (1m 42s) 0.5000 clieben / de ✓\n",
      "in num_err 60\n",
      "pk num_err 39\n",
      "fr num_err 40\n",
      "af num_err 71\n",
      "cn num_err 12\n",
      "za num_err 47\n",
      "fi num_err 36\n",
      "ir num_err 82\n",
      "de num_err 34\n",
      "25000 25% (2m 12s) 0.4678 huachiang / cn ✓\n",
      "in num_err 53\n",
      "pk num_err 56\n",
      "fr num_err 42\n",
      "af num_err 64\n",
      "cn num_err 14\n",
      "za num_err 38\n",
      "fi num_err 29\n",
      "ir num_err 66\n",
      "de num_err 39\n",
      "30000 30% (2m 41s) 0.4456 peruspohja / cn ✗ (fi)\n",
      "in num_err 55\n",
      "pk num_err 40\n",
      "fr num_err 45\n",
      "af num_err 45\n",
      "cn num_err 10\n",
      "za num_err 44\n",
      "fi num_err 50\n",
      "ir num_err 76\n",
      "de num_err 39\n",
      "35000 35% (3m 7s) 0.4489 ster kats / de ✗ (af)\n",
      "in num_err 59\n",
      "pk num_err 44\n",
      "fr num_err 47\n",
      "af num_err 56\n",
      "cn num_err 20\n",
      "za num_err 24\n",
      "fi num_err 38\n",
      "ir num_err 71\n",
      "de num_err 40\n",
      "40000 40% (3m 36s) 0.4433 ganani / pk ✓\n",
      "in num_err 69\n",
      "pk num_err 60\n",
      "fr num_err 22\n",
      "af num_err 66\n",
      "cn num_err 10\n",
      "za num_err 66\n",
      "fi num_err 48\n",
      "ir num_err 65\n",
      "de num_err 52\n",
      "45000 45% (4m 3s) 0.5089 grandcamp / fi ✗ (fr)\n",
      "in num_err 54\n",
      "pk num_err 45\n",
      "fr num_err 35\n",
      "af num_err 61\n",
      "cn num_err 12\n",
      "za num_err 45\n",
      "fi num_err 26\n",
      "ir num_err 65\n",
      "de num_err 49\n",
      "50000 50% (4m 31s) 0.4356 xigongfang / cn ✓\n",
      "in num_err 52\n",
      "pk num_err 47\n",
      "fr num_err 44\n",
      "af num_err 51\n",
      "cn num_err 7\n",
      "za num_err 29\n",
      "fi num_err 32\n",
      "ir num_err 79\n",
      "de num_err 48\n",
      "55000 55% (5m 2s) 0.4322 jurva / fi ✓\n",
      "in num_err 39\n",
      "pk num_err 43\n",
      "fr num_err 29\n",
      "af num_err 74\n",
      "cn num_err 17\n",
      "za num_err 50\n",
      "fi num_err 32\n",
      "ir num_err 48\n",
      "de num_err 51\n",
      "60000 60% (5m 36s) 0.4256 kasasevina / in ✓\n",
      "in num_err 55\n",
      "pk num_err 55\n",
      "fr num_err 62\n",
      "af num_err 41\n",
      "cn num_err 9\n",
      "za num_err 45\n",
      "fi num_err 28\n",
      "ir num_err 69\n",
      "de num_err 37\n",
      "65000 65% (6m 10s) 0.4456 zaplavskiye / za ✓\n",
      "in num_err 61\n",
      "pk num_err 65\n",
      "fr num_err 36\n",
      "af num_err 65\n",
      "cn num_err 9\n",
      "za num_err 42\n",
      "fi num_err 21\n",
      "ir num_err 41\n",
      "de num_err 42\n",
      "70000 70% (6m 41s) 0.4244 sar asiabe yusefi / fi ✓\n",
      "in num_err 42\n",
      "pk num_err 39\n",
      "fr num_err 27\n",
      "af num_err 65\n",
      "cn num_err 9\n",
      "za num_err 40\n",
      "fi num_err 32\n",
      "ir num_err 61\n",
      "de num_err 46\n",
      "75000 75% (7m 9s) 0.4011 villarejo de montalban / de ✓\n",
      "in num_err 62\n",
      "pk num_err 58\n",
      "fr num_err 53\n",
      "af num_err 29\n",
      "cn num_err 6\n",
      "za num_err 60\n",
      "fi num_err 32\n",
      "ir num_err 63\n",
      "de num_err 52\n",
      "80000 80% (7m 39s) 0.4611 kuz bengaser / za ✗ (af)\n",
      "in num_err 57\n",
      "pk num_err 44\n",
      "fr num_err 43\n",
      "af num_err 56\n",
      "cn num_err 8\n",
      "za num_err 27\n",
      "fi num_err 29\n",
      "ir num_err 48\n",
      "de num_err 53\n",
      "85000 85% (8m 15s) 0.4056 youchetou / cn ✓\n",
      "in num_err 56\n",
      "pk num_err 35\n",
      "fr num_err 32\n",
      "af num_err 58\n",
      "cn num_err 7\n",
      "za num_err 29\n",
      "fi num_err 30\n",
      "ir num_err 58\n",
      "de num_err 31\n",
      "90000 90% (8m 49s) 0.3733 manawwar / pk ✓\n",
      "in num_err 59\n",
      "pk num_err 61\n",
      "fr num_err 36\n",
      "af num_err 58\n",
      "cn num_err 14\n",
      "za num_err 26\n",
      "fi num_err 27\n",
      "ir num_err 45\n",
      "de num_err 28\n",
      "95000 95% (9m 27s) 0.3933 zara / za ✓\n",
      "in num_err 65\n",
      "pk num_err 55\n",
      "fr num_err 49\n",
      "af num_err 70\n",
      "cn num_err 8\n",
      "za num_err 41\n",
      "fi num_err 24\n",
      "ir num_err 38\n",
      "de num_err 30\n",
      "100000 100% (9m 51s) 0.4222 lifidezi / fi ✓\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import math\n",
    "\n",
    "if tri_FLAG:\n",
    "#     n_iters = 5000 # there are 3000 lines for each of 9 documents\n",
    "#     # n_iters_doc = 3\n",
    "#     print_every = 100\n",
    "#     plot_every = 100\n",
    "    n_iters = 100000 # there are 3000 lines for each of 9 documents\n",
    "    # n_iters_doc = 3\n",
    "    print_every = 5000\n",
    "    plot_every = 1000\n",
    "else:\n",
    "    n_iters = 100000 # there are 3000 lines for each of 9 documents\n",
    "    # n_iters_doc = 3\n",
    "    print_every = 5000\n",
    "    plot_every = 1000\n",
    "# Keep track of losses for plotting\n",
    "current_loss = 0\n",
    "all_losses = []\n",
    "all_accuracies = []\n",
    "def timeSince(since):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "start = time.time()\n",
    "num_err = 0\n",
    "pre_err_rate = 1\n",
    "for iter in range(1, n_iters + 1):\n",
    "# for iter in range(1, n_iters_docs + 1):\n",
    "    #-------------------------\n",
    "    category, line, category_tensor, line_tensor = randomTrainingExample()\n",
    "    #-------------------------  \n",
    "#     category_tensor = Variable(torch.LongTensor([all_categories.index(category)]))\n",
    "#     line_tensor = Variable(lineToTensor(line))\n",
    "    #-------------------------\n",
    "    output_real, loss = train(category_tensor, line_tensor)#yezheng:  what is output_real\n",
    "    current_loss += loss\n",
    "    all_losses.append(loss)\n",
    "    all_accuracies.append(1 - num_err*1.0/iter)\n",
    "    # Print iter number, loss, name and guess\n",
    "#         print(\"category\",category, \"line\",line,\"category_tensor\",category_tensor, \"line_tensor\",line_tensor )\n",
    "    guess, guess_i = categoryFromOutput(output_real) #yezheng: what is 'guess_i' -- the index while 'guess' is the name\n",
    "#         print(\"guess\",guess)\n",
    "#     if guess != category: num_err +=1\n",
    "    \n",
    "    if iter % print_every == 0:\n",
    "#         print(\"output_real\",output_real.size(),type(output_real))\n",
    "#         print(f\"category_tensor{category_tensor}\") # yezheng: this is true value\n",
    "        curr_err_rate = Curr_Err_Rate_dev()#num_err*1.0/iter\n",
    "        pre_err_rate = curr_err_rate\n",
    "        correct = '✓' if guess == category else '✗ (%s)' % category\n",
    "        print('%d %d%% (%s) %.4f %s / %s %s' % (iter, iter / n_iters * 100, timeSince(start), pre_err_rate, line, guess, correct))\n",
    "\n",
    "    # Add current loss avg to list of losses\n",
    "#     if iter % plot_every == 0:\n",
    "#         all_losses.append(current_loss / plot_every)\n",
    "#         current_loss = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5,0,'iter: iteration process')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "\n",
    "plt.figure()\n",
    "# plt.plot(all_losses)\n",
    "plt.plot(all_accuracies[1:])\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('iter: iteration process')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYwAAAEKCAYAAAAB0GKPAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4wLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvpW3flQAAFmlJREFUeJzt3X20XXV95/H3p4SAojVIMi4kaMCi\nbXRcBa8WHxBqKwJ2xKepUjoCHcWlYq2znArDzNBiW2txXC1TR6UOUrQDKlJlLBURQaZWxItACGIg\nPiCJWKIWLFqrwnf+2L+LhzTJ/eXh5Nzc+36tddbd+7d/Z+/vvvvkfrIfzt6pKiRJms3PTLoASdKu\nwcCQJHUxMCRJXQwMSVIXA0OS1MXAkCR1MTAkSV0MDElSFwNDktRl0aQL2FGWLl1aK1asmHQZkrRL\nue66675dVct6+s6bwFixYgXT09OTLkOSdilJbu/t6yEpSVIXA0OS1MXAkCR1MTAkSV0MDElSFwND\nktTFwJAkdTEwJEldDAxJUhcDQ5LUxcCQJHUxMCRJXQwMSVIXA0OS1MXAkCR1MTAkSV0MDElSFwND\nktTFwJAkdTEwJEldDAxJUhcDQ5LUxcCQJHUxMCRJXcYWGEnOTXJXktWbmZ4kZydZm2RVkkM2mv6z\nSdYl+fNx1ShJ6jfOPYzzgKO2MP1o4KD2Ohl410bT3wJcPZbKJElbbWyBUVVXA9/dQpdjgfNrcA2w\nJMm+AEmeAjwK+OS46pMkbZ1JnsPYD7hjZHwdsF+SnwH+B/CmiVQlSdqkuXjS+7XApVW1braOSU5O\nMp1kesOGDTuhNElauBZNcNnrgf1Hxpe3tqcDhyV5LfAwYHGSe6vq1I1nUFXnAOcATE1N1fhLlqSF\na5KBcQlwSpILgV8C7qmqO4HjZzokORGY2lRYSJJ2rrEFRpILgCOApUnWAWcAuwNU1buBS4FjgLXA\nD4CTxlWLJGn7jS0wquq4WaYX8LpZ+pzHcHmuJGnC5uJJb0nSHGRgSJK6GBiSpC4GhiSpi4EhSepi\nYEiSuhgYkqQuBoYkqYuBIUnqYmBIkroYGJKkLgaGJKmLgSFJ6mJgSJK6GBiSpC4GhiSpi4EhSepi\nYEiSuhgYkqQuBoYkqYuBIUnqYmBIkroYGJKkLgaGJKmLgSFJ6mJgSJK6GBiSpC4GhiSpi4EhSepi\nYEiSuhgYkqQuYwuMJOcmuSvJ6s1MT5Kzk6xNsirJIa39F5N8LsnNrf1l46pRktRvnHsY5wFHbWH6\n0cBB7XUy8K7W/gPgFVX1xPb+P02yZIx1SpI6LBrXjKvq6iQrttDlWOD8qirgmiRLkuxbVbeOzOOb\nSe4ClgF3j6tWSdLsJnkOYz/gjpHxda3tAUmeBiwGvrIT65IkbcKcPemdZF/g/cBJVXX/ZvqcnGQ6\nyfSGDRt2boGStMBMMjDWA/uPjC9vbST5WeBvgNOr6prNzaCqzqmqqaqaWrZs2ViLlaSFbpKBcQnw\nina11KHAPVV1Z5LFwF8znN+4aIL1SZJGjO2kd5ILgCOApUnWAWcAuwNU1buBS4FjgLUMV0ad1N76\n68CzgX2SnNjaTqyqG8ZVqyRpduO8Suq4WaYX8LpNtH8A+MC46pIkbZs5e9JbkjS3GBiSpC4GhiSp\ni4EhSepiYEiSuhgYkqQuBoYkqYuBIUnqYmBIkroYGJKkLgaGJKmLgSFJ6mJgSJK6GBiSpC5dgZHk\n4iTPT2LASNIC1RsA/wv4DeC2JH+c5AljrEmSNAd1BUZVfaqqjgcOAb4OfCrJ3yc5Kcnu4yxQkjQ3\ndB9iSrIPcCLwSuB64M8YAuTysVQmSZpTuh7RmuSvgScA7wf+XVXd2SZ9MMn0uIqTJM0dvc/0Pruq\nrtzUhKqa2oH1SJLmqN5DUiuTLJkZSbJ3kteOqSZJ0hzUGxivqqq7Z0aq6h+BV42nJEnSXNQbGLsl\nycxIkt2AxeMpSZI0F/Wew/gEwwnu97TxV7c2SdIC0RsYb2YIide08cuB946lIknSnNQVGFV1P/Cu\n9pIkLUC938M4CHgrsBLYc6a9qg4cU12SpDmm96T3+xj2Ln4C/DJwPvCBcRUlSZp7egPjIVV1BZCq\nur2qfg94/vjKkiTNNb0nvf+l3dr8tiSnAOuBh42vLEnSXNO7h/EG4KHAbwNPAX4TOGFLb0hybpK7\nkqzezPQkOTvJ2iSrkhwyMu2EJLe11xaXI0naOWYNjPYlvZdV1b1Vta6qTqqql1TVNbO89TzgqC1M\nPxo4qL1Opl2BleSRwBnALwFPA85IsvesayJJGqtZA6Oq7gOetbUzrqqrge9uocuxwPk1uAZYkmRf\n4HnA5VX13XYLksvZcvBIknaC3nMY1ye5BPgw8P2Zxqq6eDuWvR9wx8j4uta2ufax+N4Pf8ybL1o1\nrtlL0titWLoXbz7q58e+nN7A2BP4DvCckbYCticwtluSkxkOZ/GYxzxmm+Zx//3FVzbcuyPLkqSd\navfdup+Ft116v+l90hiWvR7Yf2R8eWtbDxyxUftVm6nrHOAcgKmpqdqWIpY8dDGffOPh2/JWSVpQ\ner/p/T6GPYoHqarf2o5lXwKckuRChhPc91TVnUkuA/5o5ET3kcBp27EcSdIO0HtI6uMjw3sCLwK+\nuaU3JLmAYU9haZJ1DFc+7Q5QVe8GLgWOAdYCPwBOatO+m+QtwBfarM6sqi2dPJck7QSp2vojOe1L\nfH9XVc/Y8SVtm6mpqZqe9vHikrQ1klzX+6jtbT1TchDwb7bxvZKkXVDvOYx/4sHnML7F8IwMSdIC\n0XuV1MPHXYgkaW7rOiSV5EVJHjEyviTJC8dXliRpruk9h3FGVd0zM1JVdzNc9SRJWiB6A2NT/Xov\nyZUkzQO9gTGd5B1JHtde7wCuG2dhkqS5pTcwXg/8CPggcCHwQ+B14ypKkjT39F4l9X3g1DHXIkma\nw3qvkro8yZKR8b3bPZ8kSQtE7yGppe3KKADag438prckLSC9gXF/kgceOJFkBZu4e60kaf7qvTT2\ndODvknwGCHAY7cFFkqSFofek9yeSTDGExPXAR4F/HmdhkqS5pffmg68E3sDw9LsbgEOBz/HgR7ZK\nkuax3nMYbwCeCtxeVb8MHAzcveW3SJLmk97A+GFV/RAgyR5V9WXgCeMrS5I01/Se9F7XvofxUeDy\nJP8I3D6+siRJc03vSe8XtcHfS3Il8AjgE2OrSpI052z1HWer6jPjKESSNLdt6zO9JUkLjIEhSepi\nYEiSuhgYkqQuBoYkqYuBIUnqYmBIkroYGJKkLgaGJKmLgSFJ6mJgSJK6jDUwkhyVZE2StUlO3cT0\nxya5IsmqJFclWT4y7U+S3JzkliRnJ8k4a5UkbdnYAiPJbsA7gaOBlcBxSVZu1O3twPlV9WTgTOCt\n7b3PAJ4JPBl4EsPDmw4fV62SpNmNcw/jacDaqvpqVf0IuBA4dqM+K4FPt+ErR6YXsCewGNgD2B34\nhzHWKkmaxTgDYz/gjpHxda1t1I3Ai9vwi4CHJ9mnqj7HECB3ttdlVXXLGGuVJM1i0ie93wQcnuR6\nhkNO64H7kvwc8AvAcoaQeU6SwzZ+c5KTk0wnmd6wYcPOrFuSFpxxBsZ6YP+R8eWt7QFV9c2qenFV\nHQyc3truZtjbuKaq7q2qe4G/BZ6+8QKq6pyqmqqqqWXLlo1rPSRJjDcwvgAclOSAJIuBlwOXjHZI\nsjTJTA2nAee24W8w7HksSrI7w96Hh6QkaYLGFhhV9RPgFOAyhj/2H6qqm5OcmeQFrdsRwJoktwKP\nAv6wtV8EfAW4ieE8x41V9X/HVaskaXapqknXsENMTU3V9PT0pMuQpF1Kkuuqaqqn76RPekuSdhEG\nhiSpi4EhSepiYEiSuhgYkqQuBoYkqYuBIUnqYmBIkroYGJKkLgaGJKmLgSFJ6mJgSJK6GBiSpC4G\nhiSpi4EhSepiYEiSuhgYkqQuBoYkqYuBIUnqYmBIkroYGJKkLgaGJKmLgSFJ6mJgSJK6GBiSpC4G\nhiSpi4EhSepiYEiSuhgYkqQuBoYkqctYAyPJUUnWJFmb5NRNTH9skiuSrEpyVZLlI9Mek+STSW5J\n8qUkK8ZZqyRpy8YWGEl2A94JHA2sBI5LsnKjbm8Hzq+qJwNnAm8dmXY+cFZV/QLwNOCucdUqSZrd\nOPcwngasraqvVtWPgAuBYzfqsxL4dBu+cmZ6C5ZFVXU5QFXdW1U/GGOtkqRZjDMw9gPuGBlf19pG\n3Qi8uA2/CHh4kn2AxwN3J7k4yfVJzmp7LJKkCZn0Se83AYcnuR44HFgP3AcsAg5r058KHAicuPGb\nk5ycZDrJ9IYNG3Za0ZK0EI0zMNYD+4+ML29tD6iqb1bVi6vqYOD01nY3w97IDe1w1k+AjwKHbLyA\nqjqnqqaqamrZsmXjWg9JEuMNjC8AByU5IMli4OXAJaMdkixNMlPDacC5I+9dkmQmBZ4DfGmMtUqS\nZjG2wGh7BqcAlwG3AB+qqpuTnJnkBa3bEcCaJLcCjwL+sL33PobDUVckuQkI8BfjqlWSNLtU1aRr\n2CGmpqZqenp60mVI0i4lyXVVNdXTd9InvSVJuwgDQ5LUxcCQJHUxMCRJXQwMSVIXA0OS1MXAkCR1\nMTAkSV0MDElSFwNDktTFwJAkdTEwJEldDAxJUhcDQ5LUxcCQJHUxMCRJXQwMSVIXA0OS1MXAkCR1\nMTAkSV0MDElSFwNDktTFwJAkdTEwJEldDAxJUpdU1aRr2CGSbABu345ZLAW+vYPK2VUstHVeaOsL\nrvNCsT3r/NiqWtbTcd4ExvZKMl1VU5OuY2daaOu80NYXXOeFYmets4ekJEldDAxJUhcD46fOmXQB\nE7DQ1nmhrS+4zgvFTllnz2FIkrq4hyFJ6rLgAyPJUUnWJFmb5NRJ17O1kuyf5MokX0pyc5I3tPZH\nJrk8yW3t596tPUnObuu7KskhI/M6ofW/LckJI+1PSXJTe8/ZSbLz1/TBkuyW5PokH2/jByT5fKvx\ng0kWt/Y92vjaNn3FyDxOa+1rkjxvpH3OfSaSLElyUZIvJ7klydMXwDZ+Y/tMr05yQZI959t2TnJu\nkruSrB5pG/t23dwyZlVVC/YF7AZ8BTgQWAzcCKycdF1buQ77Aoe04YcDtwIrgT8BTm3tpwJva8PH\nAH8LBDgU+HxrfyTw1fZz7za8d5t2beub9t6j58B6/yfg/wAfb+MfAl7eht8NvKYNvxZ4dxt+OfDB\nNryybe89gAPa52C3ufqZAP4SeGUbXgwsmc/bGNgP+BrwkJHte+J8287As4FDgNUjbWPfrptbxqz1\nTvofwoQ/lE8HLhsZPw04bdJ1bec6fQx4LrAG2Le17QusacPvAY4b6b+mTT8OeM9I+3ta277Al0fa\nH9RvQuu4HLgCeA7w8faP4dvAoo23K3AZ8PQ2vKj1y8bbeqbfXPxMAI9ofzyzUft83sb7AXe0P4KL\n2nZ+3nzczsAKHhwYY9+um1vGbK+Ffkhq5kM5Y11r2yW13fCDgc8Dj6qqO9ukbwGPasObW+ctta/b\nRPsk/Snwu8D9bXwf4O6q+kkbH63xgfVq0+9p/bf29zBJBwAbgPe1w3DvTbIX83gbV9V64O3AN4A7\nGbbbdczv7TxjZ2zXzS1jixZ6YMwbSR4GfAT4nar63ui0Gv4bMS8uh0vya8BdVXXdpGvZiRYxHLZ4\nV1UdDHyf4TDCA+bTNgZox9SPZQjLRwN7AUdNtKgJ2BnbdWuWsdADYz2w/8j48ta2S0myO0NY/FVV\nXdya/yHJvm36vsBdrX1z67yl9uWbaJ+UZwIvSPJ14EKGw1J/BixJsqj1Ga3xgfVq0x8BfIet/z1M\n0jpgXVV9vo1fxBAg83UbA/wq8LWq2lBVPwYuZtj283k7z9gZ23Vzy9iihR4YXwAOaldeLGY4WXbJ\nhGvaKu2qh/8N3FJV7xiZdAkwc7XECQznNmbaX9GuuDgUuKftml4GHJlk7/a/uyMZjvHeCXwvyaFt\nWa8YmddOV1WnVdXyqlrBsL0+XVXHA1cCL23dNl7fmd/DS1v/au0vb1fXHAAcxHCCcM59JqrqW8Ad\nSZ7Qmn4F+BLzdBs33wAOTfLQVtPMOs/b7TxiZ2zXzS1jyyZ1UmuuvBiuPLiV4YqJ0yddzzbU/yyG\n3clVwA3tdQzD8dsrgNuATwGPbP0DvLOt703A1Mi8fgtY214njbRPAavbe/6cjU6+TnDdj+CnV0kd\nyPCHYC3wYWCP1r5nG1/bph848v7T2zqtYeSqoLn4mQB+EZhu2/mjDFfDzOttDPw+8OVW1/sZrnSa\nV9sZuIDhHM2PGfYk/+PO2K6bW8ZsL7/pLUnqstAPSUmSOhkYkqQuBoYkqYuBIUnqYmBIkroYGJpz\nkvx9+7kiyW/s4HmfmeRX2/DvJHnoDpz3C5Os3NSypPnAy2o1ZyU5AnhTVf3aVrxnUf30XkOz9f06\nw7Xs396K+e9WVfdtZtp5DN8Luah3fjtK+2JWqur+WTtL28g9DM05Se5tg38MHJbkhgzPRtgtyVlJ\nvtCeB/Dq1v+IJP8vySUM3wbe0rzPS/LSJL/NcI+iK5Nc2aYdmeRzSb6Y5MPt/lwk+XqStyX5IvDv\nk7yq1XBjko+0byM/A3gBcFar93Ezy2rz+JV248CbMjwDYY+Ref9+W+ZNSX5+EzWfmORjSa7K8PyC\nM1r7igzPczif4ctZ+yc5rs1ndZK3jczjqLaMG5Nc0dr2arVc22o7trU/sbXd0H7PB7W+f9PevzrJ\ny7Zt62qXNulvc/rytfELuLf9PIL2Te42fjLwX9vwHgzffD6g9fs+cMBI30uBR29i3ucBL23DXweW\ntuGlwNXAXm38zcB/H+n3uyPz2Gdk+A+A128879Fxhm8h3wE8vrWfz3CTyJl5z7z/tcB7N1HziQzf\nBt4HeAhDOEwx3Bb7fuDQ1u/RDLfUWMZww8JPAy9s43fM/H746TeH/wj4zTa8hOFbz3sB/xM4vrUv\nbst8CfAXIzU9YtKfE187/+UehnYlRzLcS+cGhlu478NwbyCAa6vqazMdq+qYqvrmVsz7UIaH7Xy2\nzf8E4LEj0z84MvyktkdzE3A88MRZ5v0Ehhvp3drG/5LhwTkzZm4YeR1DCGzK5VX1nar659b/Wa39\n9qq6pg0/Fbiqhhv2/QT4q7acQ4GrZ34/VfXd1v9I4NS2vlcxBNtjgM8B/yXJm4HHtmXeBDy37Wkd\nVlX3zLLOmocWzd5FmjPC8L/xyx7UOJzr+P4OmPflVXXcZqaPzv884IVVdWOSExn2cLbHv7Sf97H5\nf5Mbn2ycGd+e9Q7wkqpas1H7LUk+DzwfuDTJq6vq0xkeCXoM8AdJrqiqM7dj2doFuYehueyfGB47\nO+My4DUZbudOksdneJDQjpj/NcAzk/xcm/deSR6/mfc9HLiz1XH8FuqdsQZYMTNv4D8An9nKWp+b\n4TnMD2E4zPTZTfS5Fjg8ydIkuzE8Ye0zbd2eneFurSR5ZOt/GfD6dsKcJAe3nwcCX62qsxnuYvrk\nJI8GflBVHwDOYri9uhYYA0Nz2Srgvnai9Y3AexlOan8xyWqGR1Fu8n/kSS5tf+S25BzgE0murKoN\nDOcKLkiyiuGwzL86Ad38N4ZDYp9luJvqjAuB/9xOID9uprGqfgicBHy4Hca6n+F51FvjWoZnnqwC\nPlJV0xt3qOF21qcy3AL8RuC6qvpYW7eTgYuT3MhPD6+9BdgdWJXk5jYO8OvA6nao6kkM51z+LXBt\nazuD4dyNFhgvq5XmuHbYa6qqTpl0LVrY3MOQJHVxD0OS1MU9DElSFwNDktTFwJAkdTEwJEldDAxJ\nUhcDQ5LU5f8Du5j1UZbvro8AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x109dc8a90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAS0AAAD+CAYAAABvPlPbAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x109d949b0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Keep track of correct guesses in a confusion matrix\n",
    "confusion = torch.zeros(n_categories, n_categories)\n",
    "n_confusion = 10000\n",
    "\n",
    "# Just return an output given a line\n",
    "\n",
    "\n",
    "# Go through a bunch of examples and record which are correctly guessed\n",
    "for i in range(n_confusion):\n",
    "    category, line, category_tensor, line_tensor = randomTrainingExample()\n",
    "    output = evaluate(line_tensor)\n",
    "    guess, guess_i = categoryFromOutput(output)\n",
    "    category_i = all_categories.index(category)\n",
    "    confusion[category_i][guess_i] += 1\n",
    "\n",
    "# Normalize by dividing every row by its sum\n",
    "for i in range(n_categories):\n",
    "    confusion[i] = confusion[i] / confusion[i].sum()\n",
    "\n",
    "# Set up plot\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "cax = ax.matshow(confusion.numpy())\n",
    "fig.colorbar(cax)\n",
    "\n",
    "# Set up axes\n",
    "ax.set_xticklabels([''] + all_categories, rotation=90)\n",
    "ax.set_yticklabels([''] + all_categories)\n",
    "\n",
    "# Force label at every tick\n",
    "ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "# sphinx_gallery_thumbnail_number = 2\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "words_test = []\n",
    "with open (\"cities_test.txt\",\"r\",encoding=\"ISO-8859-1\") as f:\n",
    "    reader = csv.reader(f)\n",
    "    for row in reader:\n",
    "        words_test.append(row[0])\n",
    "        \n",
    "pred_labels = [predict(word) for word in words_test]\n",
    "    \n",
    "output_file = open(\"labels.txt\",\"w\")\n",
    "for item in pred_labels:\n",
    "    output_file.write(\"%s\\n\" % item)\n",
    "    \n",
    "output_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
