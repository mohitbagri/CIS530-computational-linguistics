{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# http://pytorch.org/tutorials/intermediate/char_rnn_classification_tutorial.html\n",
    "from __future__ import unicode_literals, print_function, division\n",
    "from io import open\n",
    "import glob\n",
    "tri_FLAG = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train/in.txt 3000\n",
      "train/pk.txt 3000\n",
      "train/fr.txt 3000\n",
      "train/af.txt 3000\n",
      "train/cn.txt 3000\n",
      "train/za.txt 3000\n",
      "train/fi.txt 3000\n",
      "train/ir.txt 3000\n",
      "train/de.txt 3000\n"
     ]
    }
   ],
   "source": [
    "def findFiles(path): return glob.glob(path)\n",
    "\n",
    "# print(findFiles('data/names/*.txt'))\n",
    "# print(findFiles('train/*.txt'))\n",
    "\n",
    "import unicodedata\n",
    "import string\n",
    "\n",
    "all_letters = string.ascii_letters + \" .,;'\"\n",
    "n_letters = len(all_letters)\n",
    "\n",
    "# Turn a Unicode string to plain ASCII, thanks to http://stackoverflow.com/a/518232/2809427\n",
    "def unicodeToAscii(s):\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "        and c in all_letters\n",
    "    )\n",
    "\n",
    "# print(unicodeToAscii('Ślusàrski'))\n",
    "\n",
    "# Build the category_lines dictionary, a list of names per language\n",
    "category_lines = {}\n",
    "all_categories = [] # yezheng: this is a global variable\n",
    "\n",
    "# Read a file and split into lines\n",
    "def readLines(filename):\n",
    "    try: # yezheng -- tackle with \"ISO-8859-1\"\n",
    "        fd = open(filename, encoding='utf-8', errors='ignore')\n",
    "    except:\n",
    "        fd = open(filename, encoding=\"ISO-8859-1\")\n",
    "    lines = fd.read().strip().split('\\n')\n",
    "    return [unicodeToAscii(line) for line in lines]\n",
    "    fd.close()\n",
    "\n",
    "# for filename in findFiles('data/names/*.txt'):\n",
    "for filename in findFiles('train/*.txt'):\n",
    "    category = filename.split('/')[-1].split('.')[0]\n",
    "    all_categories.append(category)\n",
    "    lines = readLines(filename)\n",
    "    category_lines[category] = lines\n",
    "    print(filename,len(lines))\n",
    "\n",
    "n_categories = len(all_categories)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_categories=9 n_letters57\n"
     ]
    }
   ],
   "source": [
    "print(f\"n_categories={n_categories} n_letters{n_letters}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['yungming', 'xingzhuang', 'liren', 'hongjiaotian', 'guanrenling']\n",
      "abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ .,;'\n",
      "['in', 'pk', 'fr', 'af', 'cn', 'za', 'fi', 'ir', 'de']\n"
     ]
    }
   ],
   "source": [
    "# print(category_lines['Italian'][:5])\n",
    "print(category_lines['cn'][:5])\n",
    "print(all_letters)\n",
    "print(all_categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "# Find letter index from all_letters, e.g. \"a\" = 0\n",
    "def letterToIndex(letter): return all_letters.find(letter)\n",
    "\n",
    "# Just for demonstration, turn a letter into a <1 x n_letters> Tensor\n",
    "def letterToTensor(letter):\n",
    "    tensor = torch.zeros(1, n_letters)\n",
    "    tensor[0][letterToIndex(letter)] = 1\n",
    "    return tensor\n",
    "\n",
    "# Turn a line into a <line_length x 1 x n_letters>,\n",
    "# or an array of one-hot letter vectors\n",
    "def lineToTensor(line):\n",
    "    tensor = torch.zeros(len(line), 1, n_letters)\n",
    "    for li, letter in enumerate(line): \n",
    "        tensor[li][0][letterToIndex(letter)] = 1\n",
    "    return tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 513])\n",
      "torch.Size([5, 1, 513])\n"
     ]
    }
   ],
   "source": [
    "#yezheng: from HW5: evaluating trigram\n",
    "from collections import *\n",
    "from random import random\n",
    "import numpy as np\n",
    "def train_char_lm(fname, order=2, add_k=1):\n",
    "  ''' Trains a language model.\n",
    "  This code was borrowed from http://nbviewer.jupyter.org/gist/yoavg/d76121dfde2618422139\n",
    "  Inputs:\n",
    "    fname: Path to a text corpus.\n",
    "    order: The length of the n-grams.\n",
    "    add_k: k value for add-k smoothing. NOT YET IMPLMENTED\n",
    "\n",
    "  Returns:\n",
    "    A dictionary mapping from n-grams of length n to a list of tuples.\n",
    "    Each tuple consists of a possible net character and its probability.\n",
    "  '''\n",
    "  # TODO: Add your implementation of add-k smoothing.\n",
    "  #   data = open(fname).read()\n",
    "#-------------\n",
    "  lm = defaultdict(Counter)\n",
    "  fnameLst = fname\n",
    "  if isinstance(fname, str): fnameLst = [fname]\n",
    "  lm = defaultdict(Counter)\n",
    "#   print(fnameLst)\n",
    "  for fnm in fnameLst:\n",
    "      try: # yezheng -- tackle with \"ISO-8859-1\"\n",
    "            fd = open(fnm, encoding='utf-8', errors='ignore')\n",
    "      except:\n",
    "            fd = open(fnm, encoding=\"ISO-8859-1\")\n",
    "      AllChars = set()\n",
    "      for data in fd.readlines():\n",
    "          data = data.lower()\n",
    "          AllChars.update(data)\n",
    "          pad = \"~\" * order # yezheng: this is just setting beginning of a line -- just like <s><s> mentioned in chapter 4\n",
    "          data = pad + data\n",
    "          for i in range(len(data)-order):\n",
    "            history, char = data[i:i+order], data[i+order]\n",
    "            lm[history][char]+=1\n",
    "          del history\n",
    "          del char\n",
    "          del i\n",
    "      for his in lm.keys():\n",
    "        for ch in AllChars: lm[his][ch]+=0 \n",
    "      fd.close()\n",
    "#-------------\n",
    "  def normalize(counter): # input is a dictionary\n",
    "    s = float(sum(counter.values())) + add_k *len(counter)\n",
    "    return [(c,(cnt+add_k)/s) for c,cnt in counter.items()]\n",
    "  outlm = {hist:normalize(chars) for hist, chars in lm.items()}\n",
    "  return outlm\n",
    "\n",
    "# def perplexity_yezheng_string(cityname, lm, order=2):\n",
    "#   '''Computes the perplexity of a text file given the language model.\n",
    "#   Inputs:\n",
    "#     test_filename: path to text file\n",
    "#     lms: The output from calling train_char_lms.\n",
    "#     order: The length of the n-grams in the language model. #yezheng: order can be read from lm?\n",
    "#   Outputs:\n",
    "#     max_labels: a list of predicted labels\n",
    "#   '''\n",
    "#   #order = len(list(lm.keys())[0]) #yezheng: I think it should not be an argument\n",
    "#   pad = \"~\" * order\n",
    "#   data = pad + cityname\n",
    "#   data = data.lower()\n",
    "#   # TODO: YOUR CODE HERE\n",
    "#   # Daphne: make sure (num of characters > order)\n",
    "#   logPP = 0\n",
    "#   for i in range(len(data)-order):\n",
    "#     history, char = data[i:(i+order)], data[i+order]   \n",
    "#     if history not in lm:\n",
    "#       logPP += np.log2(8.0/len(lm)) # float(\"-inf\") # yezheng: deal with unknowns\n",
    "#     else:\n",
    "#       dict_temp = dict(lm[history])\n",
    "#       if char not in dict_temp:\n",
    "#         logPP += np.log2(8.0/len(lm)) #float(\"-inf\")  # yezheng: deal with unknowns\n",
    "#       else: logPP += np.log2(dict_temp[char])\n",
    "#   return logPP/len(data) #yezheng: we forget to divide this by len(data) in HW5\n",
    "\n",
    "import os\n",
    "lms_dict_tri = {}# a dictionary of lms\n",
    "for filename in os.listdir('train'):\n",
    "    filepath = ['train/' + filename,'val/' + filename]\n",
    "    lms_dict_tri[filename[:2]] = train_char_lm(filepath)  #, order=order, add_k = AddK\n",
    "\n",
    "def trigramTensor(line, lms_dict, order=2): # n_label*n_letters\n",
    "    tensor = torch.zeros(len(line), 1, n_categories*n_letters)\n",
    "    data = \"~\" *order + line\n",
    "    input_feature = []\n",
    "    for li in range(len(data)-order):\n",
    "        for idx_lm,lm_name in enumerate(lms_dict.keys()):\n",
    "            history, ch = data[li:(li+order)], data[li+order]   \n",
    "            lm = lms_dict[lm_name]\n",
    "            if history not in lm:\n",
    "              for j in range(n_letters): \n",
    "#                 print(\"tensor[li][0][idx_lm*n_letters + j]\",tensor[li][0][idx_lm*n_letters + j])\n",
    "                tensor[li][0][idx_lm*n_letters + j] =np.log2(8.0/len(lm)) \n",
    "            else:\n",
    "              dict_temp = dict(lm[history])\n",
    "              if ch not in dict_temp:\n",
    "                tensor[li][0][idx_lm*n_letters + letterToIndex(ch) ]= np.log2(8.0/len(lm)) #float(\"-inf\")  # yezheng: deal with unknowns\n",
    "              else:  \n",
    "                tensor[li][0][idx_lm*n_letters + letterToIndex(ch) ] = np.log2(dict_temp[ch])\n",
    "    return tensor\n",
    "\n",
    "# def lineToTensor(line):\n",
    "#     tensor = torch.zeros(len(line), 1, n_letters)\n",
    "#     for li, letter in enumerate(line): \n",
    "#         tensor[li][0][letterToIndex(letter)] = 1\n",
    "#     return tensor\n",
    "\n",
    "if tri_FLAG:\n",
    "    print(trigramTensor('J',lms_dict_tri).size())\n",
    "    print(trigramTensor('Jones',lms_dict_tri).size())\n",
    "else:\n",
    "    print(letterToTensor('J'))\n",
    "    print(lineToTensor('Jones').size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch import div as tchdiv\n",
    "\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(RNN, self).__init__()\n",
    "        self.input_size = input_size # yezheng\n",
    "        self.hidden_size = hidden_size\n",
    "        self.i2h = nn.Linear(input_size + hidden_size, hidden_size)\n",
    "        self.i2o = nn.Linear(input_size + hidden_size, output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "#         if tri_FLAG: print(\"tri debug\", input.size(),hidden.size())\n",
    "        combined = torch.cat((input, hidden), 1)\n",
    "        hidden = self.i2h(combined)\n",
    "        output = self.i2o(combined)\n",
    "        if tri_FLAG:\n",
    "#             output = tchdiv(self.softmax(output),self.input_size) # yezheng: should be divided by input_size\n",
    "            output = self.softmax(output) \n",
    "        else:\n",
    "            output = self.softmax(output) \n",
    "        #yezheng: self.softmax: transforming into \"probability\"\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self): return Variable(torch.zeros(1, self.hidden_size))\n",
    "\n",
    "n_hidden = 128\n",
    "#yezheng: initialization \n",
    "if tri_FLAG: \n",
    "    rnn = RNN(n_letters *n_categories, n_hidden, n_categories)  # yezheng: trigramTensor\n",
    "else: \n",
    "    rnn = RNN(n_letters, n_hidden, n_categories)  # yezheng: LineToTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "if tri_FLAG:\n",
    "    input = Variable(trigramTensor('A',lms_dict_tri))\n",
    "    hidden = Variable(torch.zeros(1, n_hidden))\n",
    "    output, next_hidden = rnn(input[0], hidden)\n",
    "else:\n",
    "    input = Variable(letterToTensor('A'))\n",
    "    hidden = Variable(torch.zeros(1, n_hidden))\n",
    "    output, next_hidden = rnn(input, hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      "-1.6594 -2.9258 -1.4797 -3.2812 -2.8253 -2.7443 -1.9373 -2.5889 -1.9101\n",
      "[torch.FloatTensor of size 1x9]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if tri_FLAG:\n",
    "    input = Variable(trigramTensor('Albert',lms_dict_tri))\n",
    "    hidden = Variable(torch.zeros(1, n_hidden))\n",
    "    output, next_hidden = rnn(input[0], hidden) # yezheng: strange: I though output should have size related with n_labels\n",
    "    print(output)\n",
    "else:\n",
    "    input = Variable(lineToTensor('Albert'))\n",
    "    hidden = Variable(torch.zeros(1, n_hidden))\n",
    "    output, next_hidden = rnn(input[0], hidden) # yezheng: strange: I though output should have size related with n_labels\n",
    "    print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('fr', 2)\n"
     ]
    }
   ],
   "source": [
    "def categoryFromOutput(output):\n",
    "    top_n, top_i = output.data.topk(1) # Tensor out of Variable with .data\n",
    "    category_i = top_i[0][0]\n",
    "    return all_categories[category_i], category_i\n",
    "print(categoryFromOutput(output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "category = in / line = lindum\n",
      "category = ir / line = sefteh\n",
      "category = fr / line = saintjuerylesazalate\n",
      "category = pk / line = jhakhar\n",
      "category = in / line = trinta reis\n",
      "category = cn / line = yezhuqiao\n",
      "category = fi / line = lohikoski\n",
      "category = in / line = chinchin grande\n",
      "category = de / line = kampung dendang\n",
      "category = af / line = ashikkheyl'\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "def randomChoice(l): return l[random.randint(0, len(l) - 1)]\n",
    "def randomTrainingExample():\n",
    "    category = randomChoice(all_categories)\n",
    "    line = randomChoice(category_lines[category])\n",
    "    category_tensor = Variable(torch.LongTensor([all_categories.index(category)]))\n",
    "    if tri_FLAG: line_tensor = Variable(trigramTensor(line,lms_dict_tri)) # yezheng \n",
    "    else: line_tensor = Variable(lineToTensor(line)) \n",
    "    return category, line, category_tensor, line_tensor\n",
    "for i in range(10):\n",
    "    category, line, category_tensor, line_tensor = randomTrainingExample()\n",
    "    print('category =', category, '/ line =', line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.NLLLoss() #Negative Log Likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you set this too high, it might explode. If too low, it might not learn\n",
    "if tri_FLAG:\n",
    "    learning_rate = 0.001\n",
    "else:\n",
    "    learning_rate = 0.0004 \n",
    "# yezheng: 0.005 in the tutorial for their data\n",
    "def train(category_tensor, line_tensor):\n",
    "    hidden = rnn.initHidden()\n",
    "    rnn.zero_grad()\n",
    "#     print(\"line_tensor.size()[0]\",line_tensor.size()[0]) #yezheng: this should be all the way 1\n",
    "    for i in range(line_tensor.size()[0]): \n",
    "        output, hidden = rnn(line_tensor[i], hidden)\n",
    "    loss = criterion(output, category_tensor) #yezheng: nn.NLLLoss(output, label) # label = 0,1,\\ldots, 9\n",
    "    loss.backward()\n",
    "    # Add parameters' gradients to their values, multiplied by learning rate\n",
    "    for p in rnn.parameters(): \n",
    "        p.data.add_(-learning_rate, p.grad.data)\n",
    "#     torch.nn.Dropout(p=0.5, inplace=False) #yezheng\n",
    "    return output, loss.data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000 5% (0m 21s) 0.6814 yoppingar / pk ✗ (in)\n",
      "10000 10% (0m 39s) 0.6353 cukurdere / fr ✗ (de)\n",
      "15000 15% (0m 57s) 0.5983 jhujh / af ✗ (pk)\n",
      "20000 20% (1m 14s) 0.5674 pozos zarcos / za ✓\n",
      "25000 25% (1m 32s) 0.5450 tregastelbourg / de ✗ (fr)\n",
      "30000 30% (1m 49s) 0.5270 mezza / za ✓\n",
      "35000 35% (2m 7s) 0.5115 carrer del mar / de ✓\n",
      "40000 40% (2m 25s) 0.5027 tatsientu / cn ✓\n",
      "45000 45% (2m 43s) 0.4955 birim / ir ✓\n",
      "50000 50% (3m 1s) 0.4877 qiujiapo / cn ✓\n",
      "55000 55% (3m 19s) 0.4786 dahanaisor / ir ✗ (af)\n",
      "60000 60% (3m 37s) 0.4711 baile an tochair / ir ✓\n",
      "65000 65% (3m 54s) 0.4644 balati / pk ✓\n",
      "70000 70% (4m 12s) 0.4585 chameshke poshte tang / in ✗ (ir)\n",
      "75000 75% (4m 30s) 0.4536 begi / ir ✓\n",
      "80000 80% (4m 47s) 0.4482 chiadede / de ✓\n",
      "85000 85% (5m 6s) 0.4438 baoguo / cn ✓\n",
      "90000 90% (5m 25s) 0.4393 basti shah din / pk ✓\n",
      "95000 95% (5m 43s) 0.4356 dehe delmohammed / af ✓\n",
      "100000 100% (6m 1s) 0.4319 navazan / za ✓\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import math\n",
    "\n",
    "if tri_FLAG:\n",
    "#     n_iters = 5000 # there are 3000 lines for each of 9 documents\n",
    "#     # n_iters_doc = 3\n",
    "#     print_every = 100\n",
    "#     plot_every = 100\n",
    "    n_iters = 100000 # there are 3000 lines for each of 9 documents\n",
    "    # n_iters_doc = 3\n",
    "    print_every = 5000\n",
    "    plot_every = 1000\n",
    "else:\n",
    "    n_iters = 100000 # there are 3000 lines for each of 9 documents\n",
    "    # n_iters_doc = 3\n",
    "    print_every = 5000\n",
    "    plot_every = 1000\n",
    "# Keep track of losses for plotting\n",
    "current_loss = 0\n",
    "all_losses = []\n",
    "def timeSince(since):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "start = time.time()\n",
    "num_err = 0\n",
    "for iter in range(1, n_iters + 1):\n",
    "# for iter in range(1, n_iters_docs + 1):\n",
    "    #-------------------------\n",
    "    category, line, category_tensor, line_tensor = randomTrainingExample()\n",
    "    #-------------------------  \n",
    "#     category_tensor = Variable(torch.LongTensor([all_categories.index(category)]))\n",
    "#     line_tensor = Variable(lineToTensor(line))\n",
    "    #-------------------------\n",
    "    output_real, loss = train(category_tensor, line_tensor)#yezheng:  what is output_real\n",
    "    current_loss += loss\n",
    "    # Print iter number, loss, name and guess\n",
    "#         print(\"category\",category, \"line\",line,\"category_tensor\",category_tensor, \"line_tensor\",line_tensor )\n",
    "    guess, guess_i = categoryFromOutput(output_real) #yezheng: what is 'guess_i' -- the index while 'guess' is the name\n",
    "#         print(\"guess\",guess)\n",
    "    if guess != category: num_err +=1\n",
    "    if iter % print_every == 0:\n",
    "#         print(\"output_real\",output_real.size())\n",
    "#         print(f\"category_tensor{category_tensor}\") # yezheng: this is true value\n",
    "        correct = '✓' if guess == category else '✗ (%s)' % category\n",
    "        print('%d %d%% (%s) %.4f %s / %s %s' % (iter, iter / n_iters * 100, timeSince(start), num_err*1.0/iter, line, guess, correct))\n",
    "\n",
    "    # Add current loss avg to list of losses\n",
    "#     if iter % plot_every == 0:\n",
    "#         all_losses.append(current_loss / plot_every)\n",
    "#         current_loss = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1136a1e10>]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(all_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYYAAAD8CAYAAABzTgP2AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nNQyDJKlhGCRJjf8FFDYZsBaypoYAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x109ec2710>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAS0AAAD+CAYAAABvPlPbAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4wLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvpW3flQAAGUJJREFUeJzt3XmUXGWZx/HvrzsJCUsyYJR932Jk\nUCCIIg6LqGFmDM6wHBB0QDQygHDE/aiIzmHOyIx4XFCMMw6CCwpHMRxZZg6CSABJWDUJOwgRB42E\nJYQs3fXMH/d2qOrqdN3qurfq3s7vc8493VV163nfdFc/ee97730fRQRmZlXR1+sOmJm1w0nLzCrF\nScvMKsVJy8wqxUnLzCrFScvMKsVJy8wqxUnLzCrFScvMKmWjS1qSThvhuX/rRV/MRiJpL0k3Svpd\n+nhfSZ/tdb/KYqNLWsAxkk4aeiDpYuDVPexPqUj6R0kPS3pe0guSXpT0Qs5t7CPpeEnvG9ryjD8O\nfAf4NLAOICLuB07oaY9KZEKvO9ADxwDzJdWA2cBzEdE0+iobSTdGxNskfSkiPllgUxcC74qIpUUE\nl/R54DBgJnAtcBRwK3BZEe1V1KYRcaek+ucGetWZsill0pJ0MLALdf2LiI4+1JK2qnv4AeDnJH8s\nX5C0VUQ820n8Ltg2/bnMkXQF0PCJjoi7c2rnmaISVupY4PXAPRFxqqStge8X2F4VLZe0OxAAko4F\n/tjbLpWHyrbKg6TLgd2Be4HB9OmIiLM7jPs4yYdAw74ONbBbh/H7gcsi4qSWO48t/rHAacAhwKJh\nL0dEHJFTO18FtgGuBtbUNfDTnOIvjIgDJd0FHA68CDwQEXvnEX88kLQbMA84GFgBPA6cFBG/72nH\nSqKMI61ZwMzIOZtGxK4AkqYAZ5D88Qfwa+CSHOIPStpZ0qSIWNtpvBHiXwVcJelzwDeAvYDJ1CXe\nnEwFVgHvqG8eyCVpAQsl/RXJvM1dwEpgQU6xC5f+57Q4ImYUEPvcuofXAjeRzDu/RDKtcVHebVZR\nGZPW70j+py9qOPw94AXga+nj96TPHZ9D7MeABZLmk3zQAIiIPD9s/wfcAuxAMhp9E3Ab8Lac4vcB\n50TEcwCStgS+nFNsgBnAwRFxiaTrgWnAmZ0GlfSJiLhQ0tcZIZF3OlKvizMo6UFJO0XEk3nErLNF\n+nVv4ECSKQwB7wXuzLmtyipj0poOLJF0J42HJ3Nyir9PRMyse3yTpCWdBJR0eUS8F5gDfIXkD3+L\n0d81ZmeTfKDviIjDJc0A/jXH+PsOJSyAiFghab8c4+8CfFLSgRHxBQBJs3KI+0mSkwiPkhxSFWlL\nYHH6Ga3/z6mjz2jdz+MWYP+IeDF9fD7wi05ijydlTFrnFxz/bklviog7ACQdRPMcUbsOkLQd8CTw\n9U472MLqiFgtCUmbRMQDkvKcD+qTtGVErID1JzDy/Jw8RzIq/Jqka4CTc4r7TPo7OJXk7KRG370j\nnyswNsDWQP0Uw9r0OaOESSsiflVwEwcAt0kaGtrvBDwo6bdJ87HvGGJeAtwI7EpjAhya7O9okn+Y\nZemc0NXA/0paAeQ5Qftl4HZJV6aPjwMuyDG+ImIAOEPSKSRncLfMIe63SH4Hu5HMla1vj5x/B134\njF4G3CnpZ+njdwOXFtxmZZTm7KGkWyPiEEkv0jgnIZJkMjWndnYe7fVOztBI+lZE/PNY3z+G9g4l\nmRO6Ps/Jf0kzgaGzkb+MiI4On4fF/lBEfLvu8QHAmRHx/pziF/Y76NZnNG1rf+Ct6cNbIuKevGJX\nXWmSlplZFhvjbTxmVmGlTlqS5jr++I3fjTYcf/wpddICiv6FOX5v43ejDccfZ8qetMzMGnR1In7T\nLTeJadttmnn/VSvWsOmWm2Tf/+FJbfVnbe1lJvVNyf6GwcHW+9THj9VM0uS23tPO72NdrGZim/E1\nOfvPc+3AKiZNyP77AoiXV7e1/zrWMJHsfWrXWOJrQvYrgdr+DAHUatnjt/kZerm2krWxuqNr1N55\n+Gbxl2ezfdbvun/NDRExu5P22tXV67Smbbcpp/7o8MLi3/P3OxUWG6D2bNEXWkOsK3YFEu29e6Hx\na/cVuUAEoCKvGU30bzW90Pixur3E3o47Vs7vOMbyZwf5zQ07ZNp34raPFvvDGkHpLi41s14LBiP7\naLDbnLTMrEEAtdwXD8mPk5aZNalR3pGWzx6aWYMgWBe1TFsrkmanS/k8IulTI7y+k6SbJN0j6X5J\nf9sqppOWmTUIYJDItI0mXTDxYpI6ADOBE9P7Wut9FvhJROxHUrzjm63656RlZk1qRKathTcCj0TE\nY+kN/VcARw/bJ0hWy4Xk5v+nWwX1nJaZNQhgMPv1gtMl1S/HNC8i5qXfbw88VffaMuCgYe8/H/gf\nSR8GNgOObNVgW0lL0m0RcXA77zGz6mljGn55RHSy8uyJwKUR8WVJbwYul7RPxIYnzNpKWk5YZuNf\nZJivyugPwI51j3dIn6t3Gkn9USLidkmTSZZc/9OGgrY1pyVpZfr1MEk3S7pK0gOSfiB14VJlMytc\nBKzLuLWwENhT0q6SJpFMtA+/ZP9J0qIskl5LUmHqz6MF7WROaz/gdSQTZwuAt5AsndsgXVpjLsDU\nbdu8R8vMekAM5rDEfkQMSDoLuAHoB74bEYslfRFYFBHzgY8C35H0EZLptFNalQ/sJGndGRHLACTd\nS1JlpSlppZNy8wC2fd2W5b3M1syA9Ir4nP5SI+JakhqO9c+dV/f9EpIBT2adJK01dd8PdhjLzEok\nj5FWUZxozKxBcnGpk5aZVUQA66K81523e8nD5unXm4Gb654/K9demVnPBGKwxDfLeKRlZk1q4cND\nM6sIz2mZWcWIwfEyp2Vm41+ycqmTlplVRIRYG/297sYGdTVprXp0Mncfv1dh8WuXFVflBCDe9nKh\n8QH699i10Pjx8O8Ljd+32WaFxq+99FKh8QEG9tiu0Ph9CxcXFnuUxRHaUvOclplVRTIR78NDM6sM\nT8SbWYV4It7MKmfQF5eaWVUEYl2UNzWUt2dm1hOeiDezSglU6sPD8qZTM+uZGn2ZtlYyVJj+iqR7\n0+0hSc+1iumRlpk1iCCXSx7qKky/naTm4UJJ89MlltO24iN1+3+YpPbEqDrqmaQnJE3vJIaZlUsy\nEd+faWshS4XpeicCP2oV1CMtM2vSxkR8pxWmAZC0M7Ar8MtWDWZKWpJ2Aa4H7gL2BxYD76t7fQrw\nU+CnEfGdLDHNrJwCtbMIYKcVpoecAFwVEYOtdmzn8HBv4JsR8VrgBeCM9PnNgWuAH42UsCTNlbRI\n0qK1g6vaaM7MemWQvkxbC1kqTA85gQyHhtBe0noqIhak338fOCT9/ufAf0fEZSO9KSLmRcSsiJg1\nqX/TNpozs15I6h72ZdpayFJhGkkzgC2B27P0r52kNbx849DjBcBsSeW9sMPM2pBUmM6yjSYiBoCh\nCtNLgZ8MVZiWNKdu1xOAK1pVlh7SzkT8TpLeHBG3A+8hqSa9H3Beul3MK4eMZlZRSQmxfBYBbFVh\nOn18fjsx2xlpPQicKWkpyVDuW3WvnQNMkXRhO42bWflEKK/Dw0K0M9IaiIiThz23S933p3beHTMr\nA6+nZWaVkaynVd4p6kxJKyKeAPYptitmVg5eudTMKiS55KHiIy0z23gM3XtYVk5aZtbEa8SnYlI/\na7ebVlj8/iOfKCw2wLRfb1VofIAX3/lMofFrq9cUGl8TC/5IdeEa5gmLHy80fszco7DYeuhXHcdI\nlqbx4aGZVYjntMysMpJVHnx4aGYVkdzG46RlZpXhkZaZVUzlr4g3s42Hzx6aWeX48NDMKqPNNeK7\nzknLzBoEMFDikdaYeibpbElLJf0g7w6ZWe/ltQhgqwrT6T7HS1oiabGkH7aKOdaR1hnAkRGxrK7h\nCema0GZWZZHP4WGWCtOS9gQ+DbwlIlZIek2ruG2PtCRdAuwGXCfpeUmXS1oAXN5uLDMrn6FFALNs\nLWSpMP1B4OKIWAEQEX9qFbTtpBURpwNPA4cDXwFmkoy6Thxp/4a6h2tfarc5M+uBWjraarWRVpiu\n2+bWhRmpwvT2w5raC9hL0gJJd0ia3apveUzEz4+Ilzf0Yloiex7A1Kk7ZCoRZGa90+YigJ1WmJ4A\n7AkcRlLM9RZJfx0Rz432hk55+GQ2jgRioJbL2cMsFaaXAb+JiHXA45IeIkliCzcUtLznNc2sZ3Ka\n08pSYfpqklEWkqaTHC4+NlpQX6dlZo0in/W0ImJA0lCF6X7gu0MVpoFFETE/fe0dkpYAg8DHI+Iv\no8UdU9KKiF3Sb88fy/vNrLzyLGzRqsJ0RARwbrpl4pGWmTXxbTxmVhmBGMxnIr4QTlpm1sTraZlZ\nZUROE/FFcdIysybhpGVm1eH1tNbTy2uZtPip1juO1Yzdi4sNvHDYqNe85eLBb+xfaPwZH11caPy+\nV7+q0Pjx7Abv7sjNyiNmFBp/0+vuKyx2rMmnGK9HWmZWGREwWHPSMrMK8dlDM6uMwIeHZlYpnog3\ns4qJEq9856RlZk18eGhmlZGcPfS9h2ZWIWU+PMwlnUo6Lq2DeFMe8cystyKUaeuFvEZapwEfjIhb\nc4pnZj0S9C4hZdF20pJ0Ncli9ZOBrwLbAIcA/5UWYvx4vl00s24r8dHhmA4P3x8RBwCzgLNJKsgu\nAk4aKWE11D2sbbDSmJmVRUDUlGlrRdJsSQ9KekTSp0Z4/RRJf5Z0b7p9oFXMsRweni3pH9LvdyQp\n97NB9XUPp018TZkTuJml8jg8lNRPMqh5O0mpsIXp0diSYbv+OCLOyhq3raQl6TDgSODNEbFK0s0k\nh4lmNo7kdPbwjcAjEfEYgKQrgKOB4UmrLe0eHk4DVqQJawbwpk4aN7PyGbr3MOPZw+lD0z/pNrcu\n1PZA/VpUy9LnhjtG0v2SrpK04wivN2j38PB64HRJS4EHgTvafL+ZlV0A2Q8Pl0fErA5auwb4UUSs\nkfQh4HvAEaO9oa2kFRFrgKNGeOmwduKYWbnldHj4B5J57yE7pM/VtdNQmPU/gQtbBS3vtfpm1iPZ\nzhxmOHu4ENhT0q6SJgEnAPMbWpK2rXs4B1jaKqhv4zGzZjmMtCJiQNJZwA1AP/DdiFgs6YvAooiY\nT3I1whxgAHgWOKVVXCctM2sU+a3yEBHXAtcOe+68uu8/DXy6nZhOWmbWrMRXVDppmdkIxtG9h2a2\nEaj1ugMb1t2k1d8H0zYvLHztoWLrEsbAQKHxAfY+p7iaeADXPf6bQuPP3qmTS3Za68bvoMi6hJBf\nbcKRg+cxg04712l1nUdaZtakzIsAOmmZWTMnLTOrFB8emlmVyCMtM6uMEGRY4K9XnLTMrJlHWmZW\nKU5aZlYpTlpmVhm+uNTMqqbyZw8lvQ/4GEkOvh8YBF4gKSO2DfCJiLiqqE6aWZdVOWlJeh3wWeDg\niFguaSvgImBbkiKtM0hWI3TSMhsnqj7SOgK4MiKWA0TEs5IAro6IGrBE0tYbenNanWMuwOQJUzvv\nsZkVr8RzWp2sEV9/q/oG/4URMS8iZkXErEn9Uzpozsy6ItrYWmhVYbpuv2MkhaSWy4RkSVq/BI6T\n9Ko0+FYZ3mNmVZZD0qqrMH0UMBM4UdLMEfbbAjgHyLRuUsukFRGLgQuAX0m6j2Q+y8zGMdWybS2s\nrzAdEWuBoQrTw/0L8CVgdZa+ZTp7GBHfIymiuKHXi1vZz8y6L/tE/HRJi+oez4uIeen3I1WYPqj+\nzZL2B3aMiF9I+niWBn2dlpk1ULR19nDMFaYl9ZEcuZ3SzvuctMysWT5nD1tVmN4C2Ae4Ob0iYRtg\nvqQ5EVE/emvgpGVmzfK5Tmt9hWmSZHUC8J71TUQ8D0wfeizpZuBjoyUs6OySBzMbp4YOEVtto4mI\nAWCowvRS4CdDFabTqtJj4pGWmTWKTGcGs4VqUWF62POHZYnppGVmzSp+G0+uVCvwp9HfX1xsIDnZ\nUay+zTcrNP7sXQ9qvVMH9ri90PA8/MbBYhsA+vbYpdgGnlleWGityOlvwEnLzKqkzDdMeyLezCrF\nIy0za1bikZaTlpk1yvHsYRGctMysmUdaZlYVotwT8U5aZtbMScvMKqO9VR66zknLzJp5It7MqsQj\nLTOrlqonLUmnA6enD6cBTwAPAAcCU4CrIuLzRXTQzLosY6WdXsm6RvwlwCWSJpJU57kIWJDWQOwH\nbpS0b0TcP/y9jXUPt8iv52ZWmDIfHrZ77+FXgV9GxDXA8ZLuBu4BXkdSIqhJY93DTTvrrZl1R051\nD4uQeU5L0inAzsBZ6fKpHwMOjIgVki4FJhfSQzPrujLfxpNppCXpAJIkdXJE1ICpwEvA85K2JinG\naGbjQRcrTEs6XdJvJd0r6daRirkOl3WkdRawFXBTWjVjEclh4QMkdc0WZIxjZiWndOs4zisVpt9O\nUvNwoaT5EbGkbrcfpnPmpOvGXwTMHi1u1on4U8fUazOrpnzmq9ZXmAaQNFRhen3SiogX6vbfLEvL\nvk7LzJq0cfawowrTAJLOBM4FJgFHtGrQScvMmnWhwvT6piIuBi6W9B7gs8A/jba/k5aZNcpvEcBW\nFaaHuwL4VqugXiPezJrlc/ZwfYVpSZNIKkzPr99B0p51D/8OeLhVUI+0zKxJHlfER8SApKEK0/3A\nd4cqTAOLImI+yXWfRwLrgBW0ODSELietwSkTeeENWxcWf4uJBf9z/vinYuMDmlrsrU7x7IpC4z96\n6JRC4z90SUfTJ5m89rzfFxq/9vyLhcWOwbxKQ+cUpkWF6Yg4p92YHmmZWZMy33vopGVmjQIvAmhm\n1eHCFmZWPU5aZlYlivJmLSctM2s0HlYuNbONi+e0zKxSyrwIoJOWmTUr8UhrzPceSjpb0lJJK0Za\nkdDMKiqtMJ1l64VORlpnAEdGxLK8OmNmJTHeRlqSLgF2A66T9BFJ38i3W2bWK0MXl5Z1pDWmpBUR\npwNPA4eT3Jm9QZLmSlokadG6NSvH0pyZdZlqkWnrhcLX06qvezhxk82Lbs7MOpVjNZ4i+OyhmTXx\nJQ9mVi0lnoh30jKzJmW+In7Mc1oRsUtELI+ISyPirDw7ZWY9FEBEtq2FDBWmz5W0RNL9km6UtHOr\nmC5sYWZNVMu2jRrjlQrTRwEzgRNHKHt/DzArIvYFrgIubNU3Jy0za5DjdVrrK0xHxFqSEmFH1+8Q\nETdFxKr04R0kZcZG5aRlZo2yHhomh4fTh67DTLe5dZFGqjC9/SgtnwZc16p7nog3syZtTMR3XGEa\nQNLJwCzg0Fb7OmmZWbN8zh5mqjCd1j38DHBoRKxpFdSHh2bWJKc5rSwVpvcDvg3MiYhMhUW7OtLq\nf2kNU28vsBDm5E2Kiw0MrlrVeqcO9e2wTaHxNWFisfGnTC40/t5n3lNofIBtby323/D0EcX92WlA\nnQcJYLDzoVbGCtP/DmwOXCkJ4MmImDNaXB8emlmTvC4uzVBh+sh2YzppmVkzV+Mxsyop8208Tlpm\n1sglxMysSgQoh4n4ojhpmVkTV5g2s+rw4aGZVUu2ZWd6pZO6h7fl2REzK48yV+MZ80grIg4e/pyk\nCREx0FmXzKznxulIa2X69TBJv5Y0H1iSW8/MrDciOXuYZeuFvOa09gf2iYjHh7+Qrq8zF2Byv0uI\nmVVCeQdauSWtO0dKWJDUPQTmAUyb9JoS/yjMbMjGcMnDSznFMbMy2AiSlpmNFwG4WKuZVYWI8Xl4\nGBGbp19vBm7OqT9mVga18g61PNIys0Y+PDSzqinz4aELW5hZs+x1D0clabakByU9IulTI7z+N5Lu\nljQg6dgsXXPSMrNh2irWukGS+oGLgaOAmcCJkmYO2+1J4BTgh1l758NDM2uUUzUe4I3AIxHxGICk\nK4CjqbvdLyKeSF/LPIvmpGVmTdqY05ouaVHd43npXTAA2wNP1b22DDio0751N2n19UGBdfEGn3q6\nsNgAfbvtXGh8gHjsycLbKFJtZbE3R0St+AniP5786kLj7/6r4j6n97w3t9pfWfdcHhGz8mk0G4+0\nzKxRAPn85/AHYMe6xzukz3XEE/FmNkw+E/HAQmBPSbtKmgScAMzvtHdOWmbWLIeklS4IehZwA7AU\n+ElELJb0RUlzACQdKGkZcBzwbUmLW3XNh4dm1iiAwXwuiY+Ia4Frhz13Xt33C0kOGzNz0jKzYQKi\nvPfxOGmZWbMS38bjpGVmjfI7e1gIJy0zazaeRlqSzgdWRsR/5N8dMyuF8ZS0zGyci4DBwV73YoMy\nXacl6TOSHpJ0K7B3+tzukq6XdFda93BGoT01s+7JaWmaIrQcaUk6gORK1jek+98N3EVSFuz0iHhY\n0kHAN4EjRnj/K3UPJ2yRX8/NrDgVPzx8K/CziFgFkFaSngwcDFwpaWi/TUZ6c0Pdw8nblPcnYWap\nGJdnD/uA5yLiDXl2xsxKICBKfHFpljmtW4B3S5oiaQvgXcAq4HFJxwEo8foC+2lm3TRYy7b1QMuk\nFRF3Az8G7gOuI7lzG+Ak4DRJ9wGLSVYkNLOqi0hKiGXZeiDT4WFEXABcMMJLs/PtjpmVQsUn4s1s\nIxMu1mpm1dG7a7CycNIys0a+YdrMqiSAKPFtPE5aZtYovAigmVVMN0q1jZWiixNukv4M/L6Nt0wH\nlhfUHcfvffxutLGxxd85Ijoq3Cjp+rTdLJZHRFcvfepq0mqXpEVFFoJ0/N7G70Ybjj/+uISYmVWK\nk5aZVUrZk9Y8xx/X8bvRhuOPM6We0zIzG67sIy0zswZOWmZWKU5aZlYpTlpmVilOWmZWKf8PBX4a\nn6CXZpUAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1136cb780>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Keep track of correct guesses in a confusion matrix\n",
    "confusion = torch.zeros(n_categories, n_categories)\n",
    "n_confusion = 10000\n",
    "\n",
    "# Just return an output given a line\n",
    "def evaluate(line_tensor):\n",
    "    hidden = rnn.initHidden()\n",
    "    for i in range(line_tensor.size()[0]):\n",
    "        output, hidden = rnn(line_tensor[i], hidden)\n",
    "    return output\n",
    "\n",
    "# Go through a bunch of examples and record which are correctly guessed\n",
    "for i in range(n_confusion):\n",
    "    category, line, category_tensor, line_tensor = randomTrainingExample()\n",
    "    output = evaluate(line_tensor)\n",
    "    guess, guess_i = categoryFromOutput(output)\n",
    "    category_i = all_categories.index(category)\n",
    "    confusion[category_i][guess_i] += 1\n",
    "\n",
    "# Normalize by dividing every row by its sum\n",
    "for i in range(n_categories):\n",
    "    confusion[i] = confusion[i] / confusion[i].sum()\n",
    "\n",
    "# Set up plot\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "cax = ax.matshow(confusion.numpy())\n",
    "fig.colorbar(cax)\n",
    "\n",
    "# Set up axes\n",
    "ax.set_xticklabels([''] + all_categories, rotation=90)\n",
    "ax.set_yticklabels([''] + all_categories)\n",
    "\n",
    "# Force label at every tick\n",
    "ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "\n",
    "# sphinx_gallery_thumbnail_number = 2\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
