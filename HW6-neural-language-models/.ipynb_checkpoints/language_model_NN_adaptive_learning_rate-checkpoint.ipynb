{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# http://pytorch.org/tutorials/intermediate/char_rnn_classification_tutorial.html\n",
    "from __future__ import unicode_literals, print_function, division\n",
    "from io import open\n",
    "import glob\n",
    "tri_FLAG = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def findFiles(path): return glob.glob(path)\n",
    "\n",
    "# print(findFiles('data/names/*.txt'))\n",
    "# print(findFiles('train/*.txt'))\n",
    "\n",
    "import unicodedata\n",
    "import string\n",
    "\n",
    "all_letters = string.ascii_letters + \" .,;'\"\n",
    "n_letters = len(all_letters)\n",
    "\n",
    "# Turn a Unicode string to plain ASCII, thanks to http://stackoverflow.com/a/518232/2809427\n",
    "def unicodeToAscii(s):\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "        and c in all_letters\n",
    "    )\n",
    "\n",
    "# print(unicodeToAscii('Ślusàrski'))\n",
    "\n",
    "# Build the category_lines dictionary, a list of names per language\n",
    "category_lines = {}\n",
    "all_categories = [] # yezheng: this is a global variable\n",
    "\n",
    "# Read a file and split into lines\n",
    "def readLines(filename):\n",
    "    try: # yezheng -- tackle with \"ISO-8859-1\"\n",
    "        fd = open(filename, encoding='utf-8', errors='ignore')\n",
    "    except:\n",
    "        fd = open(filename, encoding=\"ISO-8859-1\")\n",
    "    lines = fd.read().strip().split('\\n')\n",
    "    return [unicodeToAscii(line) for line in lines]\n",
    "    fd.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train/in.txt 3000\n",
      "train/pk.txt 3000\n",
      "train/fr.txt 3000\n",
      "train/af.txt 3000\n",
      "train/cn.txt 3000\n",
      "train/za.txt 3000\n",
      "train/fi.txt 3000\n",
      "train/ir.txt 3000\n",
      "train/de.txt 3000\n",
      "n_categories=9 n_letters57\n",
      "dict_keys(['in', 'pk', 'fr', 'af', 'cn', 'za', 'fi', 'ir', 'de'])\n"
     ]
    }
   ],
   "source": [
    "# for filename in findFiles('data/names/*.txt'):\n",
    "for filename in findFiles('train/*.txt'):\n",
    "    category = filename.split('/')[-1].split('.')[0]\n",
    "    all_categories.append(category)\n",
    "    lines = readLines(filename)\n",
    "    category_lines[category] = lines\n",
    "    print(filename,len(lines))\n",
    "\n",
    "n_categories = len(all_categories)\n",
    "\n",
    "\n",
    "\n",
    "category_lines_val = {}\n",
    "global num_tot_val\n",
    "num_tot_val = 0\n",
    "for filename in findFiles('val/*.txt'):\n",
    "    category = filename.split('/')[-1].split('.')[0]\n",
    "    lines = readLines(filename)\n",
    "    category_lines_val[category] = lines\n",
    "    num_tot_val += len(lines)\n",
    "\n",
    "print(f\"n_categories={n_categories} n_letters{n_letters}\")\n",
    "print(category_lines_val.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['yungming', 'xingzhuang', 'liren', 'hongjiaotian', 'guanrenling']\n",
      "abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ .,;'\n",
      "['in', 'pk', 'fr', 'af', 'cn', 'za', 'fi', 'ir', 'de']\n"
     ]
    }
   ],
   "source": [
    "print(category_lines['cn'][:5])\n",
    "print(all_letters)\n",
    "print(all_categories)\n",
    "\n",
    "\n",
    "# In[5]:\n",
    "\n",
    "\n",
    "import torch\n",
    "# Find letter index from all_letters, e.g. \"a\" = 0\n",
    "def letterToIndex(letter): return all_letters.find(letter)\n",
    "\n",
    "# Just for demonstration, turn a letter into a <1 x n_letters> Tensor\n",
    "def letterToTensor(letter):\n",
    "    tensor = torch.zeros(1, n_letters)\n",
    "    tensor[0][letterToIndex(letter)] = 1\n",
    "    return tensor\n",
    "\n",
    "# Turn a line into a <line_length x 1 x n_letters>,\n",
    "# or an array of one-hot letter vectors\n",
    "def lineToTensor(line):\n",
    "    line = line.lower()\n",
    "    tensor = torch.zeros(len(line), 1, n_letters)\n",
    "    for li, letter in enumerate(line): \n",
    "        tensor[li][0][letterToIndex(letter)] = 1\n",
    "    return tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 513])\n",
      "torch.Size([5, 1, 513])\n"
     ]
    }
   ],
   "source": [
    "#yezheng: from HW5: evaluating trigram\n",
    "from collections import *\n",
    "from random import random\n",
    "import numpy as np\n",
    "def train_char_lm(fname, order=2, add_k=1):\n",
    "  ''' Trains a language model.\n",
    "  This code was borrowed from http://nbviewer.jupyter.org/gist/yoavg/d76121dfde2618422139\n",
    "  Inputs:\n",
    "    fname: Path to a text corpus.\n",
    "    order: The length of the n-grams.\n",
    "    add_k: k value for add-k smoothing. NOT YET IMPLMENTED\n",
    "\n",
    "  Returns:\n",
    "    A dictionary mapping from n-grams of length n to a list of tuples.\n",
    "    Each tuple consists of a possible net character and its probability.\n",
    "  '''\n",
    "  # TODO: Add your implementation of add-k smoothing.\n",
    "  #   data = open(fname).read()\n",
    "#-------------\n",
    "  lm = defaultdict(Counter)\n",
    "  fnameLst = fname\n",
    "  if isinstance(fname, str): fnameLst = [fname]\n",
    "  lm = defaultdict(Counter)\n",
    "#   print(fnameLst)\n",
    "  for fnm in fnameLst:\n",
    "      try: # yezheng -- tackle with \"ISO-8859-1\"\n",
    "            fd = open(fnm, encoding='utf-8', errors='ignore')\n",
    "      except:\n",
    "            fd = open(fnm, encoding=\"ISO-8859-1\")\n",
    "      AllChars = set()\n",
    "      for data in fd.readlines():\n",
    "          data = data.lower()\n",
    "          AllChars.update(data)\n",
    "          pad = \"~\" * order # yezheng: this is just setting beginning of a line -- just like <s><s> mentioned in chapter 4\n",
    "          data = pad + data\n",
    "          for i in range(len(data)-order):\n",
    "            history, char = data[i:i+order], data[i+order]\n",
    "            lm[history][char]+=1\n",
    "          del history\n",
    "          del char\n",
    "          del i\n",
    "      for his in lm.keys():\n",
    "        for ch in AllChars: lm[his][ch]+=0 \n",
    "      fd.close()\n",
    "#-------------\n",
    "  def normalize(counter): # input is a dictionary\n",
    "    s = float(sum(counter.values())) + add_k *len(counter)\n",
    "    return [(c,(cnt+add_k)/s) for c,cnt in counter.items()]\n",
    "  outlm = {hist:normalize(chars) for hist, chars in lm.items()}\n",
    "  return outlm\n",
    "\n",
    "# def perplexity_yezheng_string(cityname, lm, order=2):\n",
    "#   '''Computes the perplexity of a text file given the language model.\n",
    "#   Inputs:\n",
    "#     test_filename: path to text file\n",
    "#     lms: The output from calling train_char_lms.\n",
    "#     order: The length of the n-grams in the language model. #yezheng: order can be read from lm?\n",
    "#   Outputs:\n",
    "#     max_labels: a list of predicted labels\n",
    "#   '''\n",
    "#   #order = len(list(lm.keys())[0]) #yezheng: I think it should not be an argument\n",
    "#   pad = \"~\" * order\n",
    "#   data = pad + cityname\n",
    "#   data = data.lower()\n",
    "#   # TODO: YOUR CODE HERE\n",
    "#   # Daphne: make sure (num of characters > order)\n",
    "#   logPP = 0\n",
    "#   for i in range(len(data)-order):\n",
    "#     history, char = data[i:(i+order)], data[i+order]   \n",
    "#     if history not in lm:\n",
    "#       logPP += np.log2(8.0/len(lm)) # float(\"-inf\") # yezheng: deal with unknowns\n",
    "#     else:\n",
    "#       dict_temp = dict(lm[history])\n",
    "#       if char not in dict_temp:\n",
    "#         logPP += np.log2(8.0/len(lm)) #float(\"-inf\")  # yezheng: deal with unknowns\n",
    "#       else: logPP += np.log2(dict_temp[char])\n",
    "#   return logPP/len(data) #yezheng: we forget to divide this by len(data) in HW5\n",
    "\n",
    "import os\n",
    "lms_dict_tri = {}# a dictionary of lms\n",
    "for filename in os.listdir('train'):\n",
    "    filepath = ['train/' + filename,'val/' + filename]\n",
    "    lms_dict_tri[filename[:2]] = train_char_lm(filepath)  #, order=order, add_k = AddK\n",
    "\n",
    "def trigramTensor(line, lms_dict, order=2): # n_label*n_letters\n",
    "    tensor = torch.zeros(len(line), 1, n_categories*n_letters)\n",
    "    data = \"~\" *order + line\n",
    "    input_feature = []\n",
    "    for li in range(len(data)-order):\n",
    "        for idx_lm,lm_name in enumerate(lms_dict.keys()):\n",
    "            history, ch = data[li:(li+order)], data[li+order]   \n",
    "            lm = lms_dict[lm_name]\n",
    "            if history not in lm:\n",
    "              for j in range(n_letters): \n",
    "#                 print(\"tensor[li][0][idx_lm*n_letters + j]\",tensor[li][0][idx_lm*n_letters + j])\n",
    "                tensor[li][0][idx_lm*n_letters + j] =np.log2(8.0/len(lm)) \n",
    "            else:\n",
    "              dict_temp = dict(lm[history])\n",
    "              if ch not in dict_temp:\n",
    "                tensor[li][0][idx_lm*n_letters + letterToIndex(ch) ]= np.log2(8.0/len(lm)) #float(\"-inf\")  # yezheng: deal with unknowns\n",
    "              else:  \n",
    "                tensor[li][0][idx_lm*n_letters + letterToIndex(ch) ] = np.log2(dict_temp[ch])\n",
    "    return tensor\n",
    "\n",
    "# def lineToTensor(line):\n",
    "#     tensor = torch.zeros(len(line), 1, n_letters)\n",
    "#     for li, letter in enumerate(line): \n",
    "#         tensor[li][0][letterToIndex(letter)] = 1\n",
    "#     return tensor\n",
    "\n",
    "if tri_FLAG:\n",
    "    print(trigramTensor('J',lms_dict_tri).size())\n",
    "    print(trigramTensor('Jones',lms_dict_tri).size())\n",
    "else:\n",
    "    print(letterToTensor('J'))\n",
    "    print(lineToTensor('Jones').size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch import div as tchdiv\n",
    "\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(RNN, self).__init__()\n",
    "        self.input_size = input_size # yezheng\n",
    "        self.hidden_size = hidden_size\n",
    "        self.i2h = nn.Linear(input_size + hidden_size, hidden_size)\n",
    "        self.i2o = nn.Linear(input_size + hidden_size, output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "#         if tri_FLAG: print(\"tri debug\", input.size(),hidden.size())\n",
    "        combined = torch.cat((input, hidden), 1)\n",
    "        hidden = self.i2h(combined)\n",
    "        output = self.i2o(combined)\n",
    "        if tri_FLAG:\n",
    "#             output = tchdiv(self.softmax(output),self.input_size) # yezheng: should be divided by input_size\n",
    "            output = self.softmax(output)\n",
    "        else:\n",
    "            output = self.softmax(output) \n",
    "        #yezheng: self.softmax: transforming into \"probability\"\n",
    "        return output, hidden\n",
    "    def initHidden(self): return Variable(torch.zeros(1, self.hidden_size))\n",
    "\n",
    "n_hidden = 128\n",
    "#yezheng: initialization \n",
    "if tri_FLAG: \n",
    "    rnn = RNN(n_letters *n_categories, n_hidden, n_categories)  # yezheng: trigramTensor\n",
    "else: \n",
    "    rnn = RNN(n_letters, n_hidden, n_categories)  # yezheng: LineToTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "if tri_FLAG:\n",
    "    input = Variable(trigramTensor('A',lms_dict_tri))\n",
    "    hidden = Variable(torch.zeros(1, n_hidden))\n",
    "    output, next_hidden = rnn(input[0], hidden)\n",
    "else:\n",
    "    input = Variable(letterToTensor('A'))\n",
    "    hidden = Variable(torch.zeros(1, n_hidden))\n",
    "    output, next_hidden = rnn(input, hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      "-2.9573 -2.5347 -2.8450 -1.7781 -2.4904 -1.8170 -2.1881 -1.7433 -2.2144\n",
      "[torch.FloatTensor of size 1x9]\n",
      "\n",
      "('ir', 7)\n"
     ]
    }
   ],
   "source": [
    "if tri_FLAG:\n",
    "    input = Variable(trigramTensor('Albert',lms_dict_tri))\n",
    "    hidden = Variable(torch.zeros(1, n_hidden))\n",
    "    output, next_hidden = rnn(input[0], hidden) # yezheng: strange: I though output should have size related with n_labels\n",
    "    print(output)\n",
    "else:\n",
    "    input = Variable(lineToTensor('Albert'))\n",
    "    hidden = Variable(torch.zeros(1, n_hidden))\n",
    "    output, next_hidden = rnn(input[0], hidden) # yezheng: strange: I though output should have size related with n_labels\n",
    "    print(output)\n",
    "\n",
    "\n",
    "# In[10]:\n",
    "\n",
    "\n",
    "def categoryFromOutput(output):\n",
    "    top_n, top_i = output.data.topk(1) # Tensor out of Variable with .data\n",
    "    category_i = top_i[0][0]\n",
    "    return all_categories[category_i], category_i\n",
    "print(categoryFromOutput(output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "category = fr / line = grand buc\n",
      "category = ir / line = shirvanskaya\n",
      "category = za / line = tzalm\n",
      "category = fr / line = saintgriede\n",
      "category = fi / line = barde sefid\n",
      "category = fi / line = fiauana\n",
      "category = ir / line = zekarria kala\n",
      "category = cn / line = kuangfahsu\n",
      "category = cn / line = siangtu\n",
      "category = cn / line = fanzhaicun\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "def randomChoice(l): return l[random.randint(0, len(l) - 1)]\n",
    "def randomTrainingExample():\n",
    "    category = randomChoice(all_categories)\n",
    "    line = randomChoice(category_lines[category])\n",
    "    category_tensor = Variable(torch.LongTensor([all_categories.index(category)]))\n",
    "    if tri_FLAG: line_tensor = Variable(trigramTensor(line,lms_dict_tri)) # yezheng \n",
    "    else: line_tensor = Variable(lineToTensor(line)) \n",
    "    return category, line, category_tensor, line_tensor\n",
    "for i in range(10):\n",
    "    category, line, category_tensor, line_tensor = randomTrainingExample()\n",
    "    print('category =', category, '/ line =', line)\n",
    "\n",
    "\n",
    "# In[12]:\n",
    "\n",
    "\n",
    "criterion = nn.NLLLoss() #Negative Log Likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(line_tensor):\n",
    "    hidden = rnn.initHidden()\n",
    "    for i in range(line_tensor.size()[0]):\n",
    "        output, hidden = rnn(line_tensor[i], hidden)\n",
    "    return output\n",
    "\n",
    "def predict(input_line):\n",
    "#     print('\\n> %s' % input_line)\n",
    "    output = evaluate(Variable(trigramTensor(input_line, lms_dict_tri)))\n",
    "#     topv, topi = output.data.topk(1, 1, True)\n",
    "    guess, guess_i = categoryFromOutput(output)\n",
    "#     category_index = topi[0][0]\n",
    "#         print('(%.2f) %s' % (value, all_categories[category_index]))\n",
    "    return guess\n",
    "\n",
    "import csv\n",
    "del category\n",
    "# print(len(category_lines_val['de']))\n",
    "def Curr_Err_Rate_dev(): \n",
    "    global num_tot_val\n",
    "    num_err = 0\n",
    "    for catname in all_categories:\n",
    "        temp = num_err\n",
    "        num_err += sum([not catname == predict(cityname) for cityname in category_lines_val[catname]])\n",
    "#         print(catname,\"num_err\",num_err - temp)\n",
    "    return num_err*1.0/num_tot_val\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you set this too high, it might explode. If too low, it might not learn\n",
    "if tri_FLAG:\n",
    "    learning_rate = 0.001\n",
    "else:\n",
    "    learning_rate = 0.0004 \n",
    "# yezheng: 0.005 in the tutorial for their data\n",
    "def train(category_tensor, line_tensor, lr):\n",
    "    hidden = rnn.initHidden()\n",
    "    rnn.zero_grad()\n",
    "#     print(\"line_tensor.size()[0]\",line_tensor.size()[0]) #yezheng: this should be all the way 1\n",
    "    for i in range(line_tensor.size()[0]): \n",
    "        output, hidden = rnn(line_tensor[i], hidden)\n",
    "    loss = criterion(output, category_tensor) #yezheng: nn.NLLLoss(output, label) # label = 0,1,\\ldots, 9\n",
    "    loss.backward()\n",
    "    # Add parameters' gradients to their values, multiplied by learning rate\n",
    "    for p in rnn.parameters(): \n",
    "        p.data.add_(-lr, p.grad.data)\n",
    "#     torch.nn.Dropout(p=0.5, inplace=False) #yezheng\n",
    "    return output, loss.data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000 5% (0m 21s) 0.6433 bruillelezmarchiennes / fr ✓\n",
      "10000 10% (0m 45s) 0.5700 zabarnica / fi ✗ (za)\n",
      "15000 15% (1m 5s) 0.5156 ladeira de cima / fi ✗ (ir)\n",
      "20000 20% (1m 30s) 0.5111 dahane urkhan / pk ✗ (af)\n",
      "25000 25% (1m 52s) 0.4922 suffisant dorp / fi ✓\n",
      "30000 30% (2m 14s) 0.4933 saintmauricesaintgermain / fr ✗ (in)\n",
      "35000 35% (2m 35s) 0.4933 jugut / fr ✗ (pk)\n",
      "40000 40% (2m 58s) 0.4933 qozagaci / za ✓\n",
      "45000 45% (3m 20s) 0.4933 mareuge / fr ✓\n",
      "50000 50% (3m 50s) 0.4933 notredamedevaux / fr ✓\n",
      "55000 55% (4m 17s) 0.4933 sawiran kidul / af ✗ (ir)\n",
      "60000 60% (4m 44s) 0.4933 yagnyatin / in ✓\n",
      "65000 65% (5m 13s) 0.4933 comunidad pallina grande / de ✓\n",
      "70000 70% (5m 41s) 0.4933 el llano de la cruz / de ✓\n",
      "75000 75% (6m 6s) 0.4933 harzrigi / af ✗ (de)\n",
      "80000 80% (6m 29s) 0.4933 surkhave / za ✗ (af)\n",
      "85000 85% (6m 54s) 0.4933 goth ramzan mehar / pk ✓\n",
      "90000 90% (7m 19s) 0.4933 banza ia zulo / za ✓\n",
      "95000 95% (7m 43s) 0.4933 kirchheim an der weinstrasse / fr ✗ (de)\n",
      "100000 100% (8m 9s) 0.4933 msingizane / za ✓\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import math\n",
    "\n",
    "if tri_FLAG:\n",
    "#     n_iters = 5000 # there are 3000 lines for each of 9 documents\n",
    "#     # n_iters_doc = 3\n",
    "#     print_every = 100\n",
    "#     plot_every = 100\n",
    "    n_iters = 100000 # there are 3000 lines for each of 9 documents\n",
    "    # n_iters_doc = 3\n",
    "    print_every = 5000\n",
    "    plot_every = 1000\n",
    "else:\n",
    "    n_iters = 100000 # there are 3000 lines for each of 9 documents\n",
    "    # n_iters_doc = 3\n",
    "    print_every = 5000\n",
    "    plot_every = 1000\n",
    "# Keep track of losses for plotting\n",
    "current_loss = 0\n",
    "all_losses = []\n",
    "all_accuracies = []\n",
    "def timeSince(since):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "start = time.time()\n",
    "num_err = 0\n",
    "pre_err_rate = 1\n",
    "lr_temp = learning_rate\n",
    "for iter in range(1, n_iters + 1):\n",
    "# for iter in range(1, n_iters_docs + 1):\n",
    "    #-------------------------\n",
    "    category, line, category_tensor, line_tensor = randomTrainingExample()\n",
    "    #-------------------------  \n",
    "#     category_tensor = Variable(torch.LongTensor([all_categories.index(category)]))\n",
    "#     line_tensor = Variable(lineToTensor(line))\n",
    "    #-------------------------\n",
    "    output_real, loss = train(category_tensor, line_tensor, lr_temp )#yezheng:  what is output_real\n",
    "    current_loss += loss\n",
    "    all_losses.append(loss)\n",
    "    all_accuracies.append(1 - num_err*1.0/iter)\n",
    "    # Print iter number, loss, name and guess\n",
    "#         print(\"category\",category, \"line\",line,\"category_tensor\",category_tensor, \"line_tensor\",line_tensor )\n",
    "    guess, guess_i = categoryFromOutput(output_real) #yezheng: what is 'guess_i' -- the index while 'guess' is the name\n",
    "#         print(\"guess\",guess)\n",
    "#     if guess != category: num_err +=1\n",
    "    \n",
    "    if iter % print_every == 0:\n",
    "#         print(\"output_real\",output_real.size(),type(output_real))\n",
    "#         print(f\"category_tensor{category_tensor}\") # yezheng: this is true value\n",
    "        curr_err_rate = Curr_Err_Rate_dev()#num_err*1.0/iter\n",
    "        lr_temp = learning_rate* max( pre_err_rate - curr_err_rate,0)* 3\n",
    "        pre_err_rate = curr_err_rate\n",
    "        correct = '✓' if guess == category else '✗ (%s)' % category\n",
    "        print('%d %d%% (%s) %.4f %s / %s %s' % (iter, iter / n_iters * 100, timeSince(start), pre_err_rate, line, guess, correct))\n",
    "\n",
    "    # Add current loss avg to list of losses\n",
    "#     if iter % plot_every == 0:\n",
    "#         all_losses.append(current_loss / plot_every)\n",
    "#         current_loss = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5,0,'iter: iteration process')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "\n",
    "plt.figure()\n",
    "# plt.plot(all_losses)\n",
    "plt.plot(all_accuracies[1:])\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('iter: iteration process')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYwAAAEKCAYAAAAB0GKPAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nUhcDQ5LU5f8Du5j1UZbvro8AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10a188240>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAS0AAAD+CAYAAABvPlPbAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1143c5be0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Keep track of correct guesses in a confusion matrix\n",
    "confusion = torch.zeros(n_categories, n_categories)\n",
    "n_confusion = 10000\n",
    "\n",
    "# Just return an output given a line\n",
    "\n",
    "\n",
    "# Go through a bunch of examples and record which are correctly guessed\n",
    "for i in range(n_confusion):\n",
    "    category, line, category_tensor, line_tensor = randomTrainingExample()\n",
    "    output = evaluate(line_tensor)\n",
    "    guess, guess_i = categoryFromOutput(output)\n",
    "    category_i = all_categories.index(category)\n",
    "    confusion[category_i][guess_i] += 1\n",
    "\n",
    "# Normalize by dividing every row by its sum\n",
    "for i in range(n_categories):\n",
    "    confusion[i] = confusion[i] / confusion[i].sum()\n",
    "\n",
    "# Set up plot\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "cax = ax.matshow(confusion.numpy())\n",
    "fig.colorbar(cax)\n",
    "\n",
    "# Set up axes\n",
    "ax.set_xticklabels([''] + all_categories, rotation=90)\n",
    "ax.set_yticklabels([''] + all_categories)\n",
    "\n",
    "# Force label at every tick\n",
    "ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "# sphinx_gallery_thumbnail_number = 2\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "words_test = []\n",
    "with open (\"cities_test.txt\",\"r\",encoding=\"ISO-8859-1\") as f:\n",
    "    reader = csv.reader(f)\n",
    "    for row in reader:\n",
    "        words_test.append(row[0])\n",
    "        \n",
    "pred_labels = [predict(word) for word in words_test]\n",
    "    \n",
    "output_file = open(\"labels.txt\",\"w\")\n",
    "for item in pred_labels:\n",
    "    output_file.write(\"%s\\n\" % item)\n",
    "    \n",
    "output_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
