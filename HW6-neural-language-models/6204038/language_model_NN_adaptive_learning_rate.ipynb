{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# http://pytorch.org/tutorials/intermediate/char_rnn_classification_tutorial.html\n",
    "from __future__ import unicode_literals, print_function, division\n",
    "from io import open\n",
    "import glob\n",
    "tri_FLAG = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def findFiles(path): return glob.glob(path)\n",
    "\n",
    "# print(findFiles('data/names/*.txt'))\n",
    "# print(findFiles('train/*.txt'))\n",
    "\n",
    "import unicodedata\n",
    "import string\n",
    "\n",
    "all_letters = string.ascii_letters + \" .,;'\"\n",
    "n_letters = len(all_letters)\n",
    "\n",
    "# Turn a Unicode string to plain ASCII, thanks to http://stackoverflow.com/a/518232/2809427\n",
    "def unicodeToAscii(s):\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "        and c in all_letters\n",
    "    )\n",
    "\n",
    "# print(unicodeToAscii('Ślusàrski'))\n",
    "\n",
    "# Build the category_lines dictionary, a list of names per language\n",
    "category_lines = {}\n",
    "all_categories = [] # yezheng: this is a global variable\n",
    "\n",
    "# Read a file and split into lines\n",
    "def readLines(filename):\n",
    "    try: # yezheng -- tackle with \"ISO-8859-1\"\n",
    "        fd = open(filename, encoding='utf-8', errors='ignore')\n",
    "    except:\n",
    "        fd = open(filename, encoding=\"ISO-8859-1\")\n",
    "    lines = fd.read().strip().split('\\n')\n",
    "    return [unicodeToAscii(line) for line in lines]\n",
    "    fd.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train/in.txt 3000\n",
      "train/pk.txt 3000\n",
      "train/fr.txt 3000\n",
      "train/af.txt 3000\n",
      "train/cn.txt 3000\n",
      "train/za.txt 3000\n",
      "train/fi.txt 3000\n",
      "train/ir.txt 3000\n",
      "train/de.txt 3000\n",
      "n_categories=9 n_letters57\n",
      "dict_keys(['in', 'pk', 'fr', 'af', 'cn', 'za', 'fi', 'ir', 'de'])\n"
     ]
    }
   ],
   "source": [
    "global num_tot_train\n",
    "num_tot_train = 0\n",
    "for filename in findFiles('train/*.txt'):\n",
    "    category = filename.split('/')[-1].split('.')[0]\n",
    "    all_categories.append(category)\n",
    "    lines = readLines(filename)\n",
    "    num_tot_train += len(lines)\n",
    "    category_lines[category] = lines\n",
    "    print(filename,len(lines))\n",
    "\n",
    "n_categories = len(all_categories)\n",
    "\n",
    "\n",
    "\n",
    "category_lines_val = {}\n",
    "global num_tot_val\n",
    "num_tot_val = 0\n",
    "for filename in findFiles('val/*.txt'):\n",
    "    category = filename.split('/')[-1].split('.')[0]\n",
    "    lines = readLines(filename)\n",
    "    category_lines_val[category] = lines\n",
    "    num_tot_val += len(lines)\n",
    "\n",
    "print(f\"n_categories={n_categories} n_letters{n_letters}\")\n",
    "print(category_lines_val.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['yungming', 'xingzhuang', 'liren', 'hongjiaotian', 'guanrenling']\n",
      "abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ .,;'\n",
      "['in', 'pk', 'fr', 'af', 'cn', 'za', 'fi', 'ir', 'de']\n"
     ]
    }
   ],
   "source": [
    "print(category_lines['cn'][:5])\n",
    "print(all_letters)\n",
    "print(all_categories)\n",
    "\n",
    "\n",
    "# In[5]:\n",
    "\n",
    "\n",
    "import torch\n",
    "# Find letter index from all_letters, e.g. \"a\" = 0\n",
    "def letterToIndex(letter): return all_letters.find(letter)\n",
    "\n",
    "# Just for demonstration, turn a letter into a <1 x n_letters> Tensor\n",
    "def letterToTensor(letter):\n",
    "    tensor = torch.zeros(1, n_letters)\n",
    "    tensor[0][letterToIndex(letter)] = 1\n",
    "    return tensor\n",
    "\n",
    "# Turn a line into a <line_length x 1 x n_letters>,\n",
    "# or an array of one-hot letter vectors\n",
    "def lineToTensor(line):\n",
    "    line = line.lower()\n",
    "    tensor = torch.zeros(len(line), 1, n_letters)\n",
    "    for li, letter in enumerate(line): \n",
    "        tensor[li][0][letterToIndex(letter)] = 1\n",
    "    return tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 513])\n",
      "torch.Size([5, 1, 513])\n"
     ]
    }
   ],
   "source": [
    "#yezheng: from HW5: evaluating trigram\n",
    "from collections import *\n",
    "from random import random\n",
    "import numpy as np\n",
    "def train_char_lm(fname, order=2, add_k=1):\n",
    "  ''' Trains a language model.\n",
    "  This code was borrowed from http://nbviewer.jupyter.org/gist/yoavg/d76121dfde2618422139\n",
    "  Inputs:\n",
    "    fname: Path to a text corpus.\n",
    "    order: The length of the n-grams.\n",
    "    add_k: k value for add-k smoothing. NOT YET IMPLMENTED\n",
    "\n",
    "  Returns:\n",
    "    A dictionary mapping from n-grams of length n to a list of tuples.\n",
    "    Each tuple consists of a possible net character and its probability.\n",
    "  '''\n",
    "  # TODO: Add your implementation of add-k smoothing.\n",
    "  #   data = open(fname).read()\n",
    "#-------------\n",
    "  lm = defaultdict(Counter)\n",
    "  fnameLst = fname\n",
    "  if isinstance(fname, str): fnameLst = [fname]\n",
    "  lm = defaultdict(Counter)\n",
    "#   print(fnameLst)\n",
    "  for fnm in fnameLst:\n",
    "      try: # yezheng -- tackle with \"ISO-8859-1\"\n",
    "            fd = open(fnm, encoding='utf-8', errors='ignore')\n",
    "      except:\n",
    "            fd = open(fnm, encoding=\"ISO-8859-1\")\n",
    "      AllChars = set()\n",
    "      for data in fd.readlines():\n",
    "          data = data.lower()\n",
    "          AllChars.update(data)\n",
    "          pad = \"~\" * order # yezheng: this is just setting beginning of a line -- just like <s><s> mentioned in chapter 4\n",
    "          data = pad + data\n",
    "          for i in range(len(data)-order):\n",
    "            history, char = data[i:i+order], data[i+order]\n",
    "            lm[history][char]+=1\n",
    "          del history\n",
    "          del char\n",
    "          del i\n",
    "      for his in lm.keys():\n",
    "        for ch in AllChars: lm[his][ch]+=0 \n",
    "      fd.close()\n",
    "#-------------\n",
    "  def normalize(counter): # input is a dictionary\n",
    "    s = float(sum(counter.values())) + add_k *len(counter)\n",
    "    return [(c,(cnt+add_k)/s) for c,cnt in counter.items()]\n",
    "  outlm = {hist:normalize(chars) for hist, chars in lm.items()}\n",
    "  return outlm\n",
    "\n",
    "# def perplexity_yezheng_string(cityname, lm, order=2):\n",
    "#   '''Computes the perplexity of a text file given the language model.\n",
    "#   Inputs:\n",
    "#     test_filename: path to text file\n",
    "#     lms: The output from calling train_char_lms.\n",
    "#     order: The length of the n-grams in the language model. #yezheng: order can be read from lm?\n",
    "#   Outputs:\n",
    "#     max_labels: a list of predicted labels\n",
    "#   '''\n",
    "#   #order = len(list(lm.keys())[0]) #yezheng: I think it should not be an argument\n",
    "#   pad = \"~\" * order\n",
    "#   data = pad + cityname\n",
    "#   data = data.lower()\n",
    "#   # TODO: YOUR CODE HERE\n",
    "#   # Daphne: make sure (num of characters > order)\n",
    "#   logPP = 0\n",
    "#   for i in range(len(data)-order):\n",
    "#     history, char = data[i:(i+order)], data[i+order]   \n",
    "#     if history not in lm:\n",
    "#       logPP += np.log2(8.0/len(lm)) # float(\"-inf\") # yezheng: deal with unknowns\n",
    "#     else:\n",
    "#       dict_temp = dict(lm[history])\n",
    "#       if char not in dict_temp:\n",
    "#         logPP += np.log2(8.0/len(lm)) #float(\"-inf\")  # yezheng: deal with unknowns\n",
    "#       else: logPP += np.log2(dict_temp[char])\n",
    "#   return logPP/len(data) #yezheng: we forget to divide this by len(data) in HW5\n",
    "\n",
    "import os\n",
    "lms_dict_tri = {}# a dictionary of lms\n",
    "for filename in os.listdir('train'):\n",
    "    filepath = ['train/' + filename,'val/' + filename]\n",
    "    lms_dict_tri[filename[:2]] = train_char_lm(filepath)  #, order=order, add_k = AddK\n",
    "\n",
    "def trigramTensor(line, lms_dict, order=2): # n_label*n_letters\n",
    "    tensor = torch.zeros(len(line), 1, n_categories*n_letters)\n",
    "    data = \"~\" *order + line\n",
    "    input_feature = []\n",
    "    for li in range(len(data)-order):\n",
    "        for idx_lm,lm_name in enumerate(lms_dict.keys()):\n",
    "            history, ch = data[li:(li+order)], data[li+order]   \n",
    "            lm = lms_dict[lm_name]\n",
    "            if history not in lm:\n",
    "              for j in range(n_letters): \n",
    "#                 print(\"tensor[li][0][idx_lm*n_letters + j]\",tensor[li][0][idx_lm*n_letters + j])\n",
    "                tensor[li][0][idx_lm*n_letters + j] =np.log2(8.0/len(lm)) \n",
    "            else:\n",
    "              dict_temp = dict(lm[history])\n",
    "              if ch not in dict_temp:\n",
    "                tensor[li][0][idx_lm*n_letters + letterToIndex(ch) ]= np.log2(8.0/len(lm)) #float(\"-inf\")  # yezheng: deal with unknowns\n",
    "              else:  \n",
    "                tensor[li][0][idx_lm*n_letters + letterToIndex(ch) ] = np.log2(dict_temp[ch])\n",
    "    return tensor\n",
    "\n",
    "# def lineToTensor(line):\n",
    "#     tensor = torch.zeros(len(line), 1, n_letters)\n",
    "#     for li, letter in enumerate(line): \n",
    "#         tensor[li][0][letterToIndex(letter)] = 1\n",
    "#     return tensor\n",
    "\n",
    "if tri_FLAG:\n",
    "    print(trigramTensor('J',lms_dict_tri).size())\n",
    "    print(trigramTensor('Jones',lms_dict_tri).size())\n",
    "else:\n",
    "    print(letterToTensor('J'))\n",
    "    print(lineToTensor('Jones').size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch import div as tchdiv\n",
    "\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(RNN, self).__init__()\n",
    "        self.input_size = input_size # yezheng\n",
    "        self.hidden_size = hidden_size\n",
    "        self.i2h = nn.Linear(input_size + hidden_size, hidden_size)\n",
    "        self.i2o = nn.Linear(input_size + hidden_size, output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "#         if tri_FLAG: print(\"tri debug\", input.size(),hidden.size())\n",
    "        combined = torch.cat((input, hidden), 1)\n",
    "        hidden = self.i2h(combined)\n",
    "        output = self.i2o(combined)\n",
    "        if tri_FLAG:\n",
    "#             output = tchdiv(self.softmax(output),self.input_size) # yezheng: should be divided by input_size\n",
    "            output = self.softmax(output)\n",
    "        else:\n",
    "            output = self.softmax(output) \n",
    "        #yezheng: self.softmax: transforming into \"probability\"\n",
    "        return output, hidden\n",
    "    def initHidden(self): return Variable(torch.zeros(1, self.hidden_size))\n",
    "\n",
    "n_hidden = 128\n",
    "#yezheng: initialization \n",
    "if tri_FLAG: \n",
    "    rnn = RNN(n_letters *n_categories, n_hidden, n_categories)  # yezheng: trigramTensor\n",
    "else: \n",
    "    rnn = RNN(n_letters, n_hidden, n_categories)  # yezheng: LineToTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "if tri_FLAG:\n",
    "    input = Variable(trigramTensor('A',lms_dict_tri))\n",
    "    hidden = Variable(torch.zeros(1, n_hidden))\n",
    "    output, next_hidden = rnn(input[0], hidden)\n",
    "else:\n",
    "    input = Variable(letterToTensor('A'))\n",
    "    hidden = Variable(torch.zeros(1, n_hidden))\n",
    "    output, next_hidden = rnn(input, hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      "-2.6115 -2.5378 -2.2183 -2.1613 -2.7277 -1.4902 -2.6263 -2.0751 -2.0025\n",
      "[torch.FloatTensor of size 1x9]\n",
      "\n",
      "('za', 5)\n"
     ]
    }
   ],
   "source": [
    "if tri_FLAG:\n",
    "    input = Variable(trigramTensor('Albert',lms_dict_tri))\n",
    "    hidden = Variable(torch.zeros(1, n_hidden))\n",
    "    output, next_hidden = rnn(input[0], hidden) # yezheng: strange: I though output should have size related with n_labels\n",
    "    print(output)\n",
    "else:\n",
    "    input = Variable(lineToTensor('Albert'))\n",
    "    hidden = Variable(torch.zeros(1, n_hidden))\n",
    "    output, next_hidden = rnn(input[0], hidden) # yezheng: strange: I though output should have size related with n_labels\n",
    "    print(output)\n",
    "\n",
    "\n",
    "# In[10]:\n",
    "\n",
    "\n",
    "def categoryFromOutput(output):\n",
    "    top_n, top_i = output.data.topk(1) # Tensor out of Variable with .data\n",
    "    category_i = top_i[0][0]\n",
    "    return all_categories[category_i], category_i\n",
    "print(categoryFromOutput(output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "category = ir / line = raqeyveh\n",
      "category = za / line = polotnyanyy zavod\n",
      "category = in / line = sjorring\n",
      "category = fr / line = burbach\n",
      "category = pk / line = kotla jafar\n",
      "category = pk / line = basti haji ghulam muhammad\n",
      "category = pk / line = goth wali muhammad zaur\n",
      "category = cn / line = hutangli\n",
      "category = af / line = chirasarkhune\n",
      "category = pk / line = kot mahoi\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "def randomChoice(l): return l[random.randint(0, len(l) - 1)]\n",
    "def randomTrainingExample():\n",
    "    category = randomChoice(all_categories)\n",
    "    line = randomChoice(category_lines[category])\n",
    "    category_tensor = Variable(torch.LongTensor([all_categories.index(category)]))\n",
    "    if tri_FLAG: line_tensor = Variable(trigramTensor(line,lms_dict_tri)) # yezheng \n",
    "    else: line_tensor = Variable(lineToTensor(line)) \n",
    "    return category, line, category_tensor, line_tensor\n",
    "for i in range(10):\n",
    "    category, line, category_tensor, line_tensor = randomTrainingExample()\n",
    "    print('category =', category, '/ line =', line)\n",
    "\n",
    "\n",
    "# In[12]:\n",
    "\n",
    "\n",
    "criterion = nn.NLLLoss() #Negative Log Likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(line_tensor):\n",
    "    hidden = rnn.initHidden()\n",
    "    for i in range(line_tensor.size()[0]):\n",
    "        output, hidden = rnn(line_tensor[i], hidden)\n",
    "    return output\n",
    "\n",
    "def predict(input_line):\n",
    "#     print('\\n> %s' % input_line)\n",
    "    output = evaluate(Variable(trigramTensor(input_line, lms_dict_tri)))\n",
    "#     topv, topi = output.data.topk(1, 1, True)\n",
    "    guess, guess_i = categoryFromOutput(output)\n",
    "#     category_index = topi[0][0]\n",
    "#         print('(%.2f) %s' % (value, all_categories[category_index]))\n",
    "    return guess\n",
    "\n",
    "import csv\n",
    "del category\n",
    "# print(len(category_lines_val['de']))\n",
    "# def Curr_Err_Rate_dev():\n",
    "#     global num_tot_val\n",
    "#     num_err = 0\n",
    "#     for catname in all_categories:\n",
    "#         temp = num_err\n",
    "#         num_err += sum([not catname == predict(cityname) for cityname in category_lines_val[catname]])\n",
    "\n",
    "confusion = torch.zeros(n_categories, n_categories)\n",
    "def Curr_Err_Rate_dev():\n",
    "    global num_tot_val\n",
    "    global num_tot_train\n",
    "    num_err = 0\n",
    "    for category_i in range(n_categories):\n",
    "        catname = all_categories[category_i]\n",
    "        num_err += sum([not catname == predict(city_name) for city_name in category_lines_val[catname]])\n",
    "    return num_err*1.0/num_tot_val\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you set this too high, it might explode. If too low, it might not learn\n",
    "if tri_FLAG:\n",
    "    learning_rate = 0.001#0.001 best #0.0015 -- oscillate a lot\n",
    "else:\n",
    "    learning_rate = 0.0004 \n",
    "# yezheng: 0.005 in the tutorial for their data\n",
    "def train(category_tensor, line_tensor, lr):\n",
    "    hidden = rnn.initHidden()\n",
    "    rnn.zero_grad()\n",
    "#     print(\"line_tensor.size()[0]\",line_tensor.size()[0]) #yezheng: this should be all the way 1\n",
    "    for i in range(line_tensor.size()[0]): \n",
    "        output, hidden = rnn(line_tensor[i], hidden)\n",
    "    loss = criterion(output, category_tensor) #yezheng: nn.NLLLoss(output, label) # label = 0,1,\\ldots, 9\n",
    "    loss.backward()\n",
    "    # Add parameters' gradients to their values, multiplied by learning rate\n",
    "    for p in rnn.parameters(): \n",
    "        p.data.add_(-lr, p.grad.data)\n",
    "#     torch.nn.Dropout(p=0.5, inplace=False) #yezheng\n",
    "    return output, loss.data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000 5% (0m 20s) 0.7322 lomza stara / za ✓\n",
      "10000 10% (0m 41s) 0.6944 palinget / fr ✗ (in)\n",
      "15000 15% (1m 2s) 0.6856 rzczawa / za ✓\n",
      "20000 20% (1m 25s) 0.6633 changgou / cn ✓\n",
      "25000 25% (1m 50s) 0.6522 korin kanbari / pk ✗ (in)\n",
      "30000 30% (2m 12s) 0.6578 penggongshan / cn ✓\n",
      "35000 35% (2m 35s) 0.6333 pemar / pk ✗ (fi)\n",
      "40000 40% (2m 56s) 0.6344 mubarakshahi / pk ✓\n",
      "45000 45% (3m 17s) 0.6156 merraveza / za ✓\n",
      "50000 50% (3m 40s) 0.6122 tinoire / fr ✗ (in)\n",
      "55000 55% (4m 2s) 0.5967 bar bast / fr ✗ (ir)\n",
      "60000 60% (4m 27s) 0.5756 jinan zhen / cn ✓\n",
      "65000 65% (4m 54s) 0.5767 antsifitse / fr ✗ (fi)\n",
      "70000 70% (5m 20s) 0.5767 tsyganskaya zarya / af ✗ (za)\n",
      "75000 75% (5m 45s) 0.5511 godejorden / de ✓\n",
      "80000 80% (6m 12s) 0.5556 vuorilahti / pk ✗ (fi)\n",
      "85000 85% (6m 40s) 0.5478 bozalkanly / fi ✗ (za)\n",
      "90000 90% (7m 7s) 0.5456 qingshuihu / cn ✓\n",
      "95000 95% (7m 29s) 0.5278 valleiry / fr ✓\n",
      "100000 100% (7m 51s) 0.5144 myza puytele / fr ✗ (za)\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import math\n",
    "\n",
    "n_iters = 100000 # there are 3000 lines for each of 9 documents\n",
    "n_iters_doc = 3\n",
    "print_every = 5000\n",
    "plot_every = 1000\n",
    "# Keep track of losses for plotting\n",
    "current_loss = 0\n",
    "small_losses = []\n",
    "all_losses = []\n",
    "all_accuracies = []\n",
    "def timeSince(since):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "start = time.time()\n",
    "num_err = 0\n",
    "pre_err_rate = 1\n",
    "Repeat = 30\n",
    "lr_temp = learning_rate\n",
    "for iter in range(1, n_iters + 1):\n",
    "# for iter in range(1, n_iters_docs + 1):\n",
    "#     for period in range(Repeat): # train the NN averagingly\n",
    "#         for in range(3000/Repeat):\n",
    "#         category_tensor = Variable(torch.LongTensor([all_categories.index(category)]))\n",
    "#         line_tensor = Variable(trigramTensor(line,lms_dict_tri)) # yezheng \n",
    "        #-------------------------\n",
    "        category, line, category_tensor, line_tensor = randomTrainingExample()\n",
    "\n",
    "        #-------------------------  \n",
    "    #     category_tensor = Variable(torch.LongTensor([all_categories.index(category)]))\n",
    "    #     line_tensor = Variable(lineToTensor(line))\n",
    "        #-------------------------\n",
    "        output_real, loss = train(category_tensor, line_tensor, lr_temp )#yezheng:  what is output_real\n",
    "        current_loss += loss\n",
    "        all_losses.append(loss)\n",
    "        # Print iter number, loss, name and guess\n",
    "    #         print(\"category\",category, \"line\",line,\"category_tensor\",category_tensor, \"line_tensor\",line_tensor )\n",
    "        guess, guess_i = categoryFromOutput(output_real) #yezheng: what is 'guess_i' -- the index while 'guess' is the name\n",
    "    #         print(\"guess\",guess)\n",
    "    #     if guess != category: num_err +=1\n",
    "\n",
    "        if iter % print_every == 0:\n",
    "            all_accuracies.append(1 - Curr_Err_Rate_dev())\n",
    "    #         print(\"output_real\",output_real.size(),type(output_real))\n",
    "    #         print(f\"category_tensor{category_tensor}\") # yezheng: this is true value\n",
    "            curr_err_rate = Curr_Err_Rate_dev()#num_err*1.0/iter\n",
    "#             lr_temp = learning_rate* max( pre_err_rate - curr_err_rate,0.1)* 3\n",
    "            pre_err_rate = curr_err_rate\n",
    "            correct = '✓' if guess == category else '✗ (%s)' % category\n",
    "            small_losses.append(loss)\n",
    "            print('%d %d%% (%s) %.4f %s / %s %s' % (iter, iter / n_iters * 100, timeSince(start), pre_err_rate, line, guess, correct))\n",
    "\n",
    "# small_losses.append(learning_rate)\n",
    "# learning_rate_loss = [learning_rate,current_loss]\n",
    "# all_losses.append(learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all_losses [1.4387309551239014, 2.7489874362945557, 0.2140227109193802, 0.7557604312896729, 1.2496644258499146]\n"
     ]
    }
   ],
   "source": [
    "# print('small losses',small_losses)\n",
    "# print('final loss',current_loss)\n",
    "# print('learning_rate',learning_rate)\n",
    "print('all_losses',all_losses[-5:])\n",
    "\n",
    "import csv\n",
    "with open('small_losses.csv', 'a') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow([learning_rate] + small_losses )\n",
    "with open('learning_rate_loss.csv', 'a') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow([learning_rate,current_loss])\n",
    "with open('all_losses.csv', 'a') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow([learning_rate] + all_losses)\n",
    "\n",
    "    # Add current loss avg to list of losses\n",
    "#     if iter % plot_every == 0:\n",
    "#         all_losses.append(current_loss / plot_every)\n",
    "#         current_loss = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep track of correct guesses in a confusion matrix\n",
    "confusion = torch.zeros(n_categories, n_categories)\n",
    "n_confusion = 10000\n",
    "\n",
    "# Just return an output given a line\n",
    "\n",
    "\n",
    "# Go through a bunch of examples and record which are correctly guessed\n",
    "for i in range(n_confusion):\n",
    "    category, line, category_tensor, line_tensor = randomTrainingExample()\n",
    "    output = evaluate(line_tensor)\n",
    "    guess, guess_i = categoryFromOutput(output)\n",
    "    category_i = all_categories.index(category)\n",
    "    confusion[category_i][guess_i] += 1\n",
    "\n",
    "# Normalize by dividing every row by its sum\n",
    "for i in range(n_categories):\n",
    "    confusion[i] = confusion[i] / confusion[i].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYsAAAEKCAYAAADjDHn2AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\navX/AXxa59XpAEA+AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10a62a9b0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAS0AAAD+CAYAAABvPlPbAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4wLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvpW3flQAAGJ5JREFUeJzt3XuQXGWZx/HvbyaZJBIugSAici8g\nRkSBsCDicpGVsJbILpfipguikQWEgkVZSlTcXbaUXbRQEYy7LuIFlNSKoeSiC0TkTriIQrioIOIF\nDQQCuU1m5tk/zulU9/Rk+vT0Od2nO79P1anp09P9nDczPU/e8573vI8iAjOzbtHX6QaYmTXDScvM\nuoqTlpl1FSctM+sqTlpm1lWctMysqzhpmVlXcdIys67ipGVmXWWDS1qSTh3juc91oi1mY5G0q6Rb\nJf0y3d9D0oWdbldZbHBJCzhK0omVHUmXA1t2sD2lIunvJT0t6RVJyyW9Kml5zsfYXdKxkj5Y2fKM\n3wO+DlwArAWIiEeB4zraohKZ1OkGdMBRwEJJI8Bc4OWIqOt9lY2kWyPi3ZI+HxHnF3ioS4D3RcSS\nIoJL+gxwEDAbuBE4HLgTuLqI43Wp10XE/ZKqnxvqVGPKppRJS9L+wA5UtS8iWvpQS9q8avfDwA9J\n/lg+K2nziHiplfhtsHX6czlC0rVAzSc6Ih7K6TgvFJWwUkcDbwMejohTJG0FfLvA43WjpZJ2BgJA\n0tHAHzvbpPJQ2VZ5kPQtYGfgEWA4fToi4qwW4z5D8iHQqK+VA+zUYvx+4OqIOLHhiycW/2jgVOAA\nYPGob0dEHJLTcS4D3gBcD6ypOsD/5hT/gYjYR9KDwMHAq8ATEbFbHvF7gaSdgPnA/sAy4BngxIj4\nbUcbVhJl7GnNAWZHztk0InYEkDQNOJ3kjz+AnwFX5hB/WNL2kgYiYrDVeGPEXwAskPQp4CvArsBU\nqhJvTjYBVgLvqT48kEvSAh6QtBnJuM2DwGvAXTnFLlz6n9NjETGrgNjnVu3eCNxOMu68gmRY4wt5\nH7MblTFp/ZLkf/qiusPfBJYDX0r3T0ifOzaH2L8B7pK0kOSDBkBE5Plh+xNwB/Amkt7ofsDdwLtz\nit8HnB0RLwNImgFcmlNsgFnA/hFxpaSbgU2BM1oNKukTEXGJpC8zRiJvtadeFWdY0pOStouI5/KI\nWWXj9OtuwD4kQxgCPgDcn/OxulYZk9ZM4HFJ91N7enJETvF3j4jZVfu3S3q8lYCSvhURHwCOAL5I\n8oe/8fjvmrCzSD7Q90bEwZJmAf+eY/w9KgkLICKWSdozx/g7AOdL2iciPgsgaU4Occ8nuYjwa5JT\nqiLNAB5LP6PV/zm19Bmt+nncAewVEa+m+xcBP2oldi8pY9K6qOD4D0naLyLuBZC0L/VjRM3aW9Ib\ngeeAL7fawAZWR8RqSUiaEhFPSMpzPKhP0oyIWAbrLmDk+Tl5maRX+CVJNwAn5RT3hfR3cArJ1UmN\n//KWfKrA2ABbAdVDDIPpc0YJk1ZE/LTgQ+wN3C2p0rXfDnhS0i+Sw8ceE4h5JXArsCO1CbAy2N/S\nIP8oz6djQtcDP5G0DMhzgPZS4B5J16X7xwAX5xhfETEEnC7pZJIruDNyiHsFye9gJ5KxsnXHI+ff\nQRs+o1cD90v6Qbp/JHBVwcfsGqW5eijpzog4QNKr1I5JiCSZbJLTcbYf7/utXKGRdEVE/ONE3z+B\n4x1IMiZ0c56D/5JmA5WrkbdFREunz6NifzQivla1vzdwRkR8KKf4hf0O2vUZTY+1F/CudPeOiHg4\nr9jdrjRJy8wsiw3xNh4z62KlTlqS5jl+78ZvxzEcv/eUOmkBRf/CHL+z8dtxDMfvMWVPWmZmNdo6\nED9p6kYxsPHmjV+YGlq9gklTN8r8+skrhhu/qMrg0AoGJmWPH03O/Fk7tJLJk17X1Hu0JvtFwMGR\n1Qz0TW0qfkybkj3+2hUMTM7+8wHQquYuYg7GagbUxL+hr7n/ZwdHVjHQN62p9zAykj1+s+2Hpv4N\nzbZ/1fByBodXtTRH7bCDN4oXX8r2t/Tgo2tuiYi5rRyvWW2dpzWw8ebMOvKcwuJveV+xE6Fjcn+h\n8QH6fv27QuOvfdvOhcaf/MtnC42vjZr7T2AiYsXKQuMX+W+4+0/XtBxj6UvD3HfLmzK9dvLWv57Z\n8gGbVLrJpWbWacFwZO9ttpuTlpnVCGAk98VD8uOkZWZ1RnBPy8y6RBCs9emhmXWLAIZ9emhm3cRj\nWmbWNQIYLvFCCk3N1JN0d1ENMbPyGMm4dUJTPa2I2L+ohphZOQRR6jGtZntar6VfD5K0SNICSU9I\n+o5GVZY0s+4UAWszbp3QypjWnsBbgD+QlIB6J8nSuTXSpTXmAUyensequmZWLDFc6BL7rWlllYf7\nI+L5iBghKWW1w1gvioj5ETEnIuY0c/OzmXVGACORbeuEVnpaa6oeD7cYy8xKpMw9LScaM6uRTC51\n0jKzLhHA2ijv+qDNTnmYnn5dBCyqev7MXFtlZh0TiOESL2pc3paZWceMhDJtjUiaK+lJSb+S9M9j\nfH87SbdLeljSo5L+tlFMJy0zq1EZ08qyjUdSP3A5cDgwGzg+LQRc7ULg+xGxJ3Ac8NVG7fOYlpmN\nIobzGdP6K+BXEfEbAEnXAu8HqiuWB1CpzL0pybzPcTlpmVmNZOXSzElrpqTFVfvzI2J++ngboLro\nwfPAvqPefxHwY0kfAzYCDm10QCctM6sRIQYjcxGXpRExp4XDHQ9cFRGXSnoH8C1Ju6eT1sfU1qQ1\n+aVVbHnNzwuLP3TDFoXFBuif+0Kh8QHYabtCw0966KlC49NfbMWiWLu20PgA9Bc71Duy9MXigg8N\n5RJmJJ95Wr8Htq3af1P6XLVTgbkAEXGPpKnATODP6wvqgXgzq5EMxPdl2hp4ANhF0o6SBkgG2heO\nes1zwLsBJL0ZmAr8ZbygPj00s1HyGYiPiCFJZwK3AP3ANyLiMUn/AiyOiIXAPwFfl3QOSb48ORpU\nkHbSMrMaTQ7Ejx8r4kbgxlHPfbrq8eMkK8Rk5qRlZnWGM0wc7RQnLTOrEYi1Ud7UUN6WmVlHVAbi\ny8pJy8xqBPLpoZl1l7wG4ovgpGVmNSLI697DQrSUtCQ9C8yJiKX5NMfMOi0ZiC/2zoZWuKdlZnXK\nPBCfqWWSdqiqb7gkrXf4uqrvT5N0k6SPFNdUM2uHINsCgFkWASxCM+l0N+CrEfFmYDlwevr8dOAG\n4JqI+ProN0maJ2mxpMWDsbrlBptZ8XK697AQzRz1dxFxV/r428AB6eMfAv8TEVeP9abquocDmtpC\nU82sHZK6h32Ztk5o5qijb2Ks7N8FzJVU3okdZtaEbEstd6rMWDNJa7t0kS6AE4A708efBpaRrAVt\nZl0uKSHWn2nrhGaS1pPAGZKWADOAK6q+dzYwTdIleTbOzNovQqU+PWxmysNQRJw06rkdqh6f0npz\nzKwMenZyqZn1nmQ9rfIOUWdKpxHxbETsXnRjzKwMkpVLs2wNIzUu1vpFSY+k21OSXm4U0z0tM6uR\nTHlovadVVaz1b0jKhz0gaWG6WmlyrIhzql7/MWDPRnGdtMysRo73HmYp1lrteOAzjYI6aZlZnTYW\nawVA0vbAjsBtjQ7Y3qQloSlTCgs/6ciGp8Mt0Y9nFhofgPevt9xbLmKw2LqBfVtMLzT+yIqVhcYH\niMHBQuP3b7N1ccGfn9xyiGRpmsynh60Wa604DlgQEcONXuielpnVyelm6CzFWiuOA87IEtRJy8xq\nJKs85DJPa12xVpJkdRzJ3TQ1JM0imbB+T5agTlpmViO5jadtxVohSWbXNirSWuGkZWaj5NbTalis\nNd2/qJmYTlpmVqfMM+KdtMysRpNXD9vOScvM6nRqBYcsnLTMrEZljfiyctIysxoBDJW4pzWhlkk6\nK63K8528G2RmndcriwBWOx04NCKerzwhaVJEDOXTLDPrmA6WB8ui6aQl6UpgJ+AmSdsBC9P950ju\n0jazLlb2RQCbTloRcZqkucDBwJnA+4ADImLVWK+XNA+YBzC1r9ibac0sHz3V0xrDwvUlLEjqHgLz\nATadtGWmafpm1jl5LQJYlDyS1oocYphZSQRiaKS8Vw895cHM6vTUmJaZ9bjowdPDiNghfXhRbi0x\ns1LYEMa0zKzHOGmZWdcIxHCJB+LL2zIz65gRlGlrpFGx1vQ1x0p6XNJjkr7bKKZ7WmZWI3IaiM9S\nrFXSLsAFwDsjYpmk1zeK656WmdWJUKatgXXFWiNiEKgUa632EeDyiFiWHDca1tBz0jKzUZIbprNs\npMVaq7Z5VYHGKta6zaiD7QrsKukuSfemtwiOq72nhxGFFsJcfcCbC4sNMGXuLwqND/DcJ/YuNP72\nlz5SaPyYsUmh8fumDBQaH4D+XErCr1e8tKy44EMNa51mkqEXVdFqsdZJwC7AQSR1Ee+Q9NaIWG/l\nZY9pmVmNCBgeaVux1ueB+yJiLfCMpKdIktgD6wvq00Mzq5PT1cN1xVolDZDUN1w46jXXk/SykDST\n5HTxN+MFdU/LzGoETZ0erj9OtmKttwDvkfQ4MAx8PCJeHC+uk5aZjZLfyqWNirWmVaXPTbdMnLTM\nrE62AvWd4aRlZnXyOD0sipOWmdVIrh6W9xqdk5aZ1Snz6WEu6VTSMWkdxNvziGdmnZXTbTyFyKun\ndSrwkYi4M6d4ZtYhQecSUhYTqXt4Pcks16nAZcAbgAOA/07v4P54vk00s3Yr8dnhhHpaH4qIlyRN\nI5nxeiBwCHBeRCwe/eKauofaqJW2mlk7BEQ+t/EUYiJJ6yxJf5c+3pbkPqH1qql72D+zzAnczFI9\nc3oo6SDgUOAdEbFS0iKS00Qz6yFlvnrYbE9rU2BZmrBmAfsV0CYz66C87j0sSrNTHm4GJklaAnwO\nuDf/JplZRwUQyrZ1QFM9rYhYAxw+xrcOyqU1ZlYKvXR6aGY9Tz139dDMel2Je1rlvSvSzDoj8ruN\np1HdQ0knS/qLpEfS7cONYrqnZWb1cuhpZal7mPpeRJyZNa57WmY2BmXcxpWl7mHTnLTMrN5Ixq31\nuocAR0l6VNICSduO8f0a7T897CsuT065teCafsP51JQbz7b/dk+h8W/6/cOFxj9smz0Ljd+Wa/Eq\n+MqZivsbyOUzWpmnlU2rdQ9vAK6JiDWSPgp8k+Re5vVyT8vM6kRk2xpoWPcwIl5M538C/BfQsFqx\nk5aZ1YuM2/ga1j2UtHXV7hHAkkZBffXQzOq1r+7hWZKOAIaAl4CTG8V10jKzOspp6DBD3cMLgAua\niemkZWa1QuDbeMysq5T4Nh4nLTOr56RlZl3FScvMukZzk0vbzknLzOrkdfWwCJmSlqQPAueR5OBH\ngWFgOTCHpO7hJyJiQVGNNLM26+akJektwIXA/hGxVNLmwBeArUmKtM4imeXqpGXWI7q9p3UIcF1E\nLAVIC7UCXB8RI8DjkrZa35tdrNWsC5V4TKuVew/XVD1e778wIuZHxJyImDMgl0g0K72s9x12qDeW\nJWndBhwjaQuA9PTQzHpZiZNWw9PD9AbHi4GfShoGil2Qycw6TiOdbsH6Zbp6GBHfJFmca33fn55b\ni8ys87p8IN7MNiCK7r96aGYbmhJfPXTSMrN6Je5pebllM6tTOUVstDWM06BYa9XrjpIUkhoWyXDS\nMrNakVw9zLKNp6pY6+HAbOB4SbPHeN3GwNnAfVma56RlZvXymaeVtVjrvwKfB1ZnaVp7x7QiYO3a\n4uL39xcXG5g0c4tC4wOF19x7716HFRp/2qJCw7PmsFeKPQAQw8VOUurbfLPCYmtpTn/S2ce0Zkpa\nXLU/PyLmp4/HKta6b/WbJe0FbBsRP5L08SwH9EC8mdVpYsrDhIu1SuojWXzh5Gbe59NDMytKo2Kt\nGwO7A4skPQvsByxsNBjvnpaZ1ctnysO6Yq0kyeo44IR1h4h4BZhZ2Ze0CDgvIhYzDve0zKxWTlcP\nI2IIqBRrXQJ8v1KsNS3QOiHuaZlZvTYVax31/EFZYjppmVkN4XsPzazbOGmZWdfwKg9m1nW6fRFA\nM9uwuKdlZt2l25OWpNOA09LdTYFngSeAfYBpwIKI+EwRDTSzNutg0Yossq4RfyVwpaTJJNV5vgDc\nldZA7AdulbRHRDw6+r2ue2jWfcp8etjsjPjLgNsi4gbgWEkPkVTneQvJejl1auoeMqW11ppZe3Rz\nCbEKSScD2wNnpvcSnQfsExHLJF0FuBKrWY8ocwmxTD0tSXuTJKmTImIE2ARYAbwiaSuSlQnNrBeU\nvMJ01p7WmcDmwO1KFqlbTHJa+ATJIl93FdI6M2s7pVtZZR2IP6XohphZiZR4IN7ztMysTpmvHjpp\nmVm9EictLwJoZrVyWgQQGtc9lHSapF9IekTSnWOVGBvNScvM6uVw9TBj3cPvRsRbI+LtwCUkE9fH\n5aRlZnVyqjDdsO5hRCyv2t2IDCem7R3TmtRP32abFhY+ZmxSWGwovh4egJa/Vmj8oT+9UGj8OHJG\nofGfuOythcYHmHXu44XG18BAccH7cpqskM+YVsO6hwCSzgDOBQaAQxoFdU/LzOo00dOaKWlx1Tav\n2WNFxOURsTNwPnBho9f76qGZ1QqaWQRwvGKtjeoejnYtcEWjA7qnZWY1KoUtchjTWlf3UNIASd3D\nhTXHknap2n0v8HSjoO5pmVm9HMa0ImJIUqXuYT/wjUrdQ2BxRCwkWYDhUGAtsAz4h0ZxnbTMrI4i\nn5H4RnUPI+LsZmM6aZlZrV5YudTMNiy+99DMukqZFwF00jKzeiXuaU14yoOksyQtkbRsrBshzaxL\nZZzu0KlTyFZ6WqcDh0bE83k1xsxKotd6WpKuBHYCbpJ0jqSv5NssM+uUHCeXFmJCSSsiTgP+ABxM\nMiFsvSTNq9yXNDiyaiKHM7M200hk2jqh8Nt4auoe9k0r+nBm1qoeqcZjZhsQT3kws+5S4oF4Jy0z\nq9OTM+IjYof04VXpZma9IICcbpgugntaZlbHY1pm1jUq87TKyknLzGpFlPr00Mstm1mdvGbEZyjW\neq6kxyU9KulWSds3iumkZWb12les9WFgTkTsASwgKdg6LictM6vTxmKtt0fEynT3XpKKPeNq/5iW\nciomOYbV2xRbrHXakj8WGh9gcJc3Fhp/8prBQuOzxWaFhp/9uT8XGh9g+Xt2LzT+9P8rsBjs0HDr\nMQIYzjymNVPS4qr9+RExP32cqVhrlVOBmxod0APxZlaniauH49U9zH486SRgDnBgo9c6aZlZvXyu\nHmYq1pqWEPskcGBErGkU1GNaZlanjcVa9wS+BhwREZnO/Z20zKxWTkvTRMQQUCnWugT4fqVYq6Qj\n0pf9BzAduE7SI5IWrifcOj49NLMaApR9IH5cGYq1HtpsTCctM6uTV4XpIjhpmVktV5g2s+7So/ce\nSro7z4aYWXmUuRpPK4sA7j/6OUmT0isGZtbNerSn9Vr69SBJP0svVRZ4f4KZtUUkVw+zbJ2Q15jW\nXsDuEfHM6G9ImgfMA5jaPz2nw5lZocrb0cotad0/VsKCpO4hMB9g04HXl/hHYWYVG8KUhxU5xTGz\nMtgAkpaZ9YoAXNjCzLqFiN48PYyI6enXRcCinNpjZmUwUt6ulntaZlbLp4dm1m168vTQzHqYk5aZ\ndY8evWHazHpUpRpPlq2BDMVa/1rSQ5KGJB2dpXlOWmZWRxGZtnFjZCvW+hxwMvDdrG1r8+mhoL+/\nsOhT73+6sNgAq/bbtdD4AFMfGvNuqNzEYLF1D/tWrCo0frThUnyhdQmBX11YXF3F1Zf9JJ9A+Zwe\nrivWCiCpUqx13Q84Ip5Nv5f5F+sxLTOrFcBIR4q1ZuKkZWajNDUQn0ux1mY4aZlZvTYWa22Wk5aZ\n1QpgOJexw3XFWkmS1XHACa0G9dVDMxslIEaybeNFyVCsVdI+kp4HjgG+JumxRq1zT8vM6uU0uTRD\nsdYHSE4bM3PSMrNazV09bDsnLTOrV+LbeJpOWpIuAl6LiP/MvzlmVgq9lLTMrMdFwPBwp1uxXpmu\nHkr6pKSnJN0J7JY+t7OkmyU9mNY9nFVoS82sfSKybR3QsKclaW+S+RVvT1//EPAgSVmw0yLiaUn7\nAl8FDhnj/VV1DzfOr+VmVpwuPz18F/CDiFgJkFaSngrsD1wnqfK6KWO9ubbu4Vbl/UmYWSp68uph\nH/ByRLw9z8aYWQkERIOJo52UZUzrDuBISdMkbQy8D1gJPCPpGAAl3lZgO82snYZHsm0d0DBpRcRD\nwPeAnwM3kdxPBHAicKqknwOPkayTY2bdLiIpIZZl64BMp4cRcTFw8Rjfmptvc8ysFLp8IN7MNjDt\nWCF2opy0zGyUclfjcdIys1q+YdrMukkAUeLbeJy0zKxWRMMF/jrJScvM6kSJTw8VbRxwk/QX4LdN\nvGUmsLSg5jh+5+O34xgbWvztI2LLVg4o6eb0uFksjYi2Tn1qa9JqlqTFRZYncvzOxm/HMRy/97iw\nhZl1FSctM+sqZU9a8xu/xPG7OH47juH4PabUY1pmZqOVvadlZlbDScvMuoqTlpl1FSctM+sqTlpm\n1lX+HwbxRYEvKRysAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x114665668>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "  \n",
    "plt.figure()\n",
    "# plt.plot(all_losses)\n",
    "plt.plot(all_accuracies,'g^')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('iter: iteration process')\n",
    "\n",
    "\n",
    "\n",
    "# Set up plot\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "cax = ax.matshow(confusion.numpy())\n",
    "fig.colorbar(cax)\n",
    "\n",
    "# Set up axes\n",
    "ax.set_xticklabels([''] + all_categories, rotation=90)\n",
    "ax.set_yticklabels([''] + all_categories)\n",
    "\n",
    "# Force label at every tick\n",
    "ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "# sphinx_gallery_thumbnail_number = 2\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "words_test = []\n",
    "with open (\"cities_test.txt\",\"r\",encoding=\"ISO-8859-1\") as f:\n",
    "    reader = csv.reader(f)\n",
    "    for row in reader:\n",
    "        words_test.append(row[0])\n",
    "        \n",
    "pred_labels = [predict(word) for word in words_test]\n",
    "    \n",
    "output_file = open(\"../labels.txt\",\"w\")\n",
    "for item in pred_labels:\n",
    "    output_file.write(\"%s\\n\" % item)\n",
    "    \n",
    "output_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
