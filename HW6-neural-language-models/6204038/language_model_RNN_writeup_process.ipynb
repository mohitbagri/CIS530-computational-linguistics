{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# http://pytorch.org/tutorials/intermediate/char_rnn_classification_tutorial.html\n",
    "from __future__ import unicode_literals, print_function, division\n",
    "from io import open\n",
    "import glob\n",
    "tri_FLAG = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def findFiles(path): return glob.glob(path)\n",
    "\n",
    "# print(findFiles('data/names/*.txt'))\n",
    "# print(findFiles('train/*.txt'))\n",
    "\n",
    "import unicodedata\n",
    "import string\n",
    "\n",
    "all_letters = string.ascii_letters + \" .,;'\"\n",
    "n_letters = len(all_letters)\n",
    "\n",
    "# Turn a Unicode string to plain ASCII, thanks to http://stackoverflow.com/a/518232/2809427\n",
    "def unicodeToAscii(s):\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "        and c in all_letters\n",
    "    )\n",
    "\n",
    "# print(unicodeToAscii('Ślusàrski'))\n",
    "\n",
    "# Build the category_lines dictionary, a list of names per language\n",
    "category_lines = {}\n",
    "all_categories = [] # yezheng: this is a global variable\n",
    "\n",
    "# Read a file and split into lines\n",
    "def readLines(filename):\n",
    "    try: # yezheng -- tackle with \"ISO-8859-1\"\n",
    "        fd = open(filename, encoding='utf-8', errors='ignore')\n",
    "    except:\n",
    "        fd = open(filename, encoding=\"ISO-8859-1\")\n",
    "    lines = fd.read().strip().split('\\n')\n",
    "    return [unicodeToAscii(line) for line in lines]\n",
    "    fd.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train/in.txt 3000\n",
      "train/pk.txt 3000\n",
      "train/fr.txt 3000\n",
      "train/af.txt 3000\n",
      "train/cn.txt 3000\n",
      "train/za.txt 3000\n",
      "train/fi.txt 3000\n",
      "train/ir.txt 3000\n",
      "train/de.txt 3000\n",
      "n_categories=9 n_letters57\n",
      "dict_keys(['in', 'pk', 'fr', 'af', 'cn', 'za', 'fi', 'ir', 'de'])\n"
     ]
    }
   ],
   "source": [
    "global num_tot_train\n",
    "num_tot_train = 0\n",
    "for filename in findFiles('train/*.txt'):\n",
    "    category = filename.split('/')[-1].split('.')[0]\n",
    "    all_categories.append(category)\n",
    "    lines = readLines(filename)\n",
    "    num_tot_train += len(lines)\n",
    "    category_lines[category] = lines\n",
    "    print(filename,len(lines))\n",
    "\n",
    "n_categories = len(all_categories)\n",
    "\n",
    "category_lines_val = {}\n",
    "global num_tot_val\n",
    "num_tot_val = 0\n",
    "for filename in findFiles('val/*.txt'):\n",
    "    category = filename.split('/')[-1].split('.')[0]\n",
    "    lines = readLines(filename)\n",
    "    category_lines_val[category] = lines\n",
    "    num_tot_val += len(lines)\n",
    "\n",
    "print(f\"n_categories={n_categories} n_letters{n_letters}\")\n",
    "print(category_lines_val.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['yungming', 'xingzhuang', 'liren', 'hongjiaotian', 'guanrenling']\n",
      "abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ .,;'\n",
      "['in', 'pk', 'fr', 'af', 'cn', 'za', 'fi', 'ir', 'de']\n"
     ]
    }
   ],
   "source": [
    "print(category_lines['cn'][:5])\n",
    "print(all_letters)\n",
    "print(all_categories)\n",
    "\n",
    "\n",
    "# In[5]:\n",
    "\n",
    "\n",
    "import torch\n",
    "# Find letter index from all_letters, e.g. \"a\" = 0\n",
    "def letterToIndex(letter): return all_letters.find(letter)\n",
    "\n",
    "# Just for demonstration, turn a letter into a <1 x n_letters> Tensor\n",
    "def letterToTensor(letter):\n",
    "    tensor = torch.zeros(1, n_letters)\n",
    "    tensor[0][letterToIndex(letter)] = 1\n",
    "    return tensor\n",
    "\n",
    "# Turn a line into a <line_length x 1 x n_letters>,\n",
    "# or an array of one-hot letter vectors\n",
    "def lineToTensor(line):\n",
    "    line = line.lower()\n",
    "    tensor = torch.zeros(len(line), 1, n_letters)\n",
    "    for li, letter in enumerate(line): \n",
    "        tensor[li][0][letterToIndex(letter)] = 1\n",
    "    return tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 513])\n",
      "torch.Size([5, 1, 513])\n"
     ]
    }
   ],
   "source": [
    "#yezheng: from HW5: evaluating trigram\n",
    "from collections import *\n",
    "from random import random\n",
    "import numpy as np\n",
    "def train_char_lm(fname, order=2, add_k=1):\n",
    "  ''' Trains a language model.\n",
    "  This code was borrowed from http://nbviewer.jupyter.org/gist/yoavg/d76121dfde2618422139\n",
    "  Inputs:\n",
    "    fname: Path to a text corpus.\n",
    "    order: The length of the n-grams.\n",
    "    add_k: k value for add-k smoothing. NOT YET IMPLMENTED\n",
    "\n",
    "  Returns:\n",
    "    A dictionary mapping from n-grams of length n to a list of tuples.\n",
    "    Each tuple consists of a possible net character and its probability.\n",
    "  '''\n",
    "  # TODO: Add your implementation of add-k smoothing.\n",
    "  #   data = open(fname).read()\n",
    "#-------------\n",
    "  lm = defaultdict(Counter)\n",
    "  fnameLst = fname\n",
    "  if isinstance(fname, str): fnameLst = [fname]\n",
    "  lm = defaultdict(Counter)\n",
    "#   print(fnameLst)\n",
    "  for fnm in fnameLst:\n",
    "      try: # yezheng -- tackle with \"ISO-8859-1\"\n",
    "            fd = open(fnm, encoding='utf-8', errors='ignore')\n",
    "      except:\n",
    "            fd = open(fnm, encoding=\"ISO-8859-1\")\n",
    "      AllChars = set()\n",
    "      for data in fd.readlines():\n",
    "          data = data.lower()\n",
    "          AllChars.update(data)\n",
    "          pad = \"~\" * order # yezheng: this is just setting beginning of a line -- just like <s><s> mentioned in chapter 4\n",
    "          data = pad + data\n",
    "          for i in range(len(data)-order):\n",
    "            history, char = data[i:i+order], data[i+order]\n",
    "            lm[history][char]+=1\n",
    "          del history\n",
    "          del char\n",
    "          del i\n",
    "      for his in lm.keys():\n",
    "        for ch in AllChars: lm[his][ch]+=0 \n",
    "      fd.close()\n",
    "#-------------\n",
    "  def normalize(counter): # input is a dictionary\n",
    "    s = float(sum(counter.values())) + add_k *len(counter)\n",
    "    return [(c,(cnt+add_k)/s) for c,cnt in counter.items()]\n",
    "  outlm = {hist:normalize(chars) for hist, chars in lm.items()}\n",
    "  return outlm\n",
    "\n",
    "import os\n",
    "lms_dict_tri = {}# a dictionary of lms\n",
    "for filename in os.listdir('train'):\n",
    "    filepath = ['train/' + filename,'val/' + filename]\n",
    "    lms_dict_tri[filename[:2]] = train_char_lm(filepath)  #, order=order, add_k = AddK\n",
    "\n",
    "def trigramTensor(line, lms_dict, order=2): # n_label*n_letters\n",
    "    tensor = torch.zeros(len(line), 1, n_categories*n_letters)\n",
    "    data = \"~\" *order + line\n",
    "    input_feature = []\n",
    "    for li in range(len(data)-order):\n",
    "        for idx_lm,lm_name in enumerate(lms_dict.keys()):\n",
    "            history, ch = data[li:(li+order)], data[li+order]   \n",
    "            lm = lms_dict[lm_name]\n",
    "            if history not in lm:\n",
    "              for j in range(n_letters): \n",
    "#                 print(\"tensor[li][0][idx_lm*n_letters + j]\",tensor[li][0][idx_lm*n_letters + j])\n",
    "                tensor[li][0][idx_lm*n_letters + j] =np.log2(8.0/len(lm)) \n",
    "            else:\n",
    "              dict_temp = dict(lm[history])\n",
    "              if ch not in dict_temp:\n",
    "                tensor[li][0][idx_lm*n_letters + letterToIndex(ch) ]= np.log2(8.0/len(lm)) #float(\"-inf\")  # yezheng: deal with unknowns\n",
    "              else:  \n",
    "                tensor[li][0][idx_lm*n_letters + letterToIndex(ch) ] = np.log2(dict_temp[ch])\n",
    "    return tensor\n",
    "\n",
    "# def lineToTensor(line):\n",
    "#     tensor = torch.zeros(len(line), 1, n_letters)\n",
    "#     for li, letter in enumerate(line): \n",
    "#         tensor[li][0][letterToIndex(letter)] = 1\n",
    "#     return tensor\n",
    "\n",
    "if tri_FLAG:\n",
    "    print(trigramTensor('J',lms_dict_tri).size())\n",
    "    print(trigramTensor('Jones',lms_dict_tri).size())\n",
    "else:\n",
    "    print(letterToTensor('J'))\n",
    "    print(lineToTensor('Jones').size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch import div as tchdiv\n",
    "\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(RNN, self).__init__()\n",
    "        self.input_size = input_size # yezheng\n",
    "        self.hidden_size = hidden_size\n",
    "        self.i2h = nn.Linear(input_size + hidden_size, hidden_size)\n",
    "        self.i2o = nn.Linear(input_size + hidden_size, output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "#         if tri_FLAG: print(\"tri debug\", input.size(),hidden.size())\n",
    "        combined = torch.cat((input, hidden), 1)\n",
    "        hidden = self.i2h(combined)\n",
    "        output = self.i2o(combined)\n",
    "        if tri_FLAG:\n",
    "#             output = tchdiv(self.softmax(output),self.input_size) # yezheng: should be divided by input_size\n",
    "            output = self.softmax(output)\n",
    "        else:\n",
    "            output = self.softmax(output) \n",
    "        #yezheng: self.softmax: transforming into \"probability\"\n",
    "        return output, hidden\n",
    "    def initHidden(self): \n",
    "        return Variable(torch.zeros(1, self.hidden_size))\n",
    "\n",
    "n_hidden = 128\n",
    "#yezheng: initialization \n",
    "if tri_FLAG: \n",
    "    rnn = RNN(n_letters *n_categories, n_hidden, n_categories)  # yezheng: trigramTensor\n",
    "else: \n",
    "    rnn = RNN(n_letters, n_hidden, n_categories)  # yezheng: LineToTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "if tri_FLAG:\n",
    "    input = Variable(trigramTensor('A',lms_dict_tri))\n",
    "    hidden = Variable(torch.zeros(1, n_hidden))\n",
    "    output, next_hidden = rnn(input[0], hidden)\n",
    "else:\n",
    "    input = Variable(letterToTensor('A'))\n",
    "    hidden = Variable(torch.zeros(1, n_hidden))\n",
    "    output, next_hidden = rnn(input, hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      "-2.8605 -2.4055 -1.8240 -3.0926 -2.8605 -2.0789 -1.5782 -2.1698 -1.9454\n",
      "[torch.FloatTensor of size 1x9]\n",
      "\n",
      "('fi', 6)\n"
     ]
    }
   ],
   "source": [
    "if tri_FLAG:\n",
    "    input = Variable(trigramTensor('Albert',lms_dict_tri))\n",
    "    hidden = Variable(torch.zeros(1, n_hidden))\n",
    "    output, next_hidden = rnn(input[0], hidden) # yezheng: strange: I though output should have size related with n_labels\n",
    "    print(output)\n",
    "else:\n",
    "    input = Variable(lineToTensor('Albert'))\n",
    "    hidden = Variable(torch.zeros(1, n_hidden))\n",
    "    output, next_hidden = rnn(input[0], hidden) # yezheng: strange: I though output should have size related with n_labels\n",
    "    print(output)\n",
    "\n",
    "\n",
    "# In[10]:\n",
    "\n",
    "\n",
    "def categoryFromOutput(output):\n",
    "    top_n, top_i = output.data.topk(1) # Tensor out of Variable with .data\n",
    "    category_i = top_i[0][0]\n",
    "    return all_categories[category_i], category_i\n",
    "print(categoryFromOutput(output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "category = de / line = halle\n",
      "category = cn / line = kohou\n",
      "category = fi / line = jewfish\n",
      "category = pk / line = wadunwala\n",
      "category = de / line = lille agersende\n",
      "category = fr / line = le courroy\n",
      "category = fi / line = winfields mill\n",
      "category = za / line = zadni porici\n",
      "category = ir / line = matibiri\n",
      "category = ir / line = baluchestan do\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "def randomChoice(l): return l[random.randint(0, len(l) - 1)]\n",
    "def randomTrainingExample():\n",
    "    category = randomChoice(all_categories)\n",
    "    line = randomChoice(category_lines[category])\n",
    "    category_tensor = Variable(torch.LongTensor([all_categories.index(category)]))\n",
    "    if tri_FLAG: line_tensor = Variable(trigramTensor(line,lms_dict_tri)) # yezheng \n",
    "    else: line_tensor = Variable(lineToTensor(line)) \n",
    "    return category, line, category_tensor, line_tensor\n",
    "for i in range(10):\n",
    "    category, line, category_tensor, line_tensor = randomTrainingExample()\n",
    "    print('category =', category, '/ line =', line)\n",
    "\n",
    "\n",
    "# In[12]:\n",
    "\n",
    "\n",
    "criterion = nn.NLLLoss() #Negative Log Likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(line_tensor):\n",
    "    hidden = rnn.initHidden()\n",
    "    for i in range(line_tensor.size()[0]):\n",
    "        output, hidden = rnn(line_tensor[i], hidden)\n",
    "    return output\n",
    "\n",
    "def predict(input_line):\n",
    "#     print('\\n> %s' % input_line)\n",
    "    output = evaluate(Variable(trigramTensor(input_line, lms_dict_tri)))\n",
    "#     topv, topi = output.data.topk(1, 1, True)\n",
    "    guess, guess_i = categoryFromOutput(output)\n",
    "#     category_index = topi[0][0]\n",
    "#         print('(%.2f) %s' % (value, all_categories[category_index]))\n",
    "    return guess\n",
    "\n",
    "import csv\n",
    "del category\n",
    "\n",
    "confusion = torch.zeros(n_categories, n_categories)\n",
    "def Curr_Err_Rate_dev():\n",
    "    global num_tot_val\n",
    "    global num_tot_train\n",
    "    num_err = 0\n",
    "    for category_i in range(n_categories):\n",
    "        catname = all_categories[category_i]\n",
    "        num_err += sum([not catname == predict(city_name) for city_name in category_lines_val[catname]])\n",
    "    return num_err*1.0/num_tot_val\n",
    "\n",
    "def Curr_Loss_dev():\n",
    "    cummulative_loss = 0\n",
    "    for category_i in range(n_categories):\n",
    "        catname = all_categories[category_i]\n",
    "        category_tensor = Variable(torch.LongTensor([all_categories.index(catname)])) # notice I cannot just use category_i\n",
    "#         for city_name in category_lines_val[catname]:\n",
    "#             output = evaluate(Variable(trigramTensor(input_line, lms_dict_tri)))\n",
    "#             loss = criterion(output, category_tensor)\n",
    "#         cummulative_loss += loss.data[0]\n",
    "        cummulative_loss += sum(criterion(evaluate(Variable(trigramTensor(city_name, lms_dict_tri))), category_tensor).data[0] for city_name in category_lines_val[catname])\n",
    "    return cummulative_loss/num_tot_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you set this too high, it might explode. If too low, it might not learn\n",
    "if tri_FLAG:\n",
    "    learning_rate = 0.005 # not good for 0.0015\n",
    "else:\n",
    "    learning_rate = 0.0004 \n",
    "# yezheng: 0.005 in the tutorial for their data\n",
    "def train(category_tensor, line_tensor, lr):\n",
    "    hidden = rnn.initHidden()\n",
    "    rnn.zero_grad()\n",
    "#     print(\"line_tensor.size()[0]\",line_tensor.size()[0]) #yezheng: this should be all the way 1\n",
    "    for i in range(line_tensor.size()[0]): \n",
    "        output, hidden = rnn(line_tensor[i], hidden)\n",
    "    loss = criterion(output, category_tensor)\n",
    "    loss.backward()\n",
    "    # Add parameters' gradients to their values, multiplied by learning rate\n",
    "    for p in rnn.parameters(): \n",
    "        p.data.add_(-lr, p.grad.data)\n",
    "#     torch.nn.Dropout(p=0.5, inplace=False) #yezheng\n",
    "    return output, loss.data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000 6% (0m 30s) 0.7278 sinping / cn ✓\n",
      "10000 12% (0m 58s) 0.8744 saintlaurentdumottay / in ✓\n",
      "15000 18% (1m 21s) 0.6444 boyaweza / za ✓\n",
      "20000 24% (1m 45s) 0.7844 hachiari dulal / pk ✓\n",
      "25000 30% (2m 7s) 0.6244 fischner phillips condominium / in ✗ (fi)\n",
      "30000 37% (2m 30s) 0.7722 batonceau / fr ✓\n",
      "35000 43% (2m 52s) 0.6489 gav bandeh / ir ✓\n",
      "40000 49% (3m 14s) 0.6578 nader shah kowt / af ✓\n",
      "45000 55% (3m 36s) 0.6178 rheinshagen / de ✓\n",
      "50000 61% (3m 58s) 0.6344 youganping / cn ✓\n",
      "55000 67% (4m 20s) 0.7956 binya / in ✓\n",
      "60000 74% (4m 43s) 0.6833 ocean view / za ✓\n",
      "65000 80% (5m 5s) 0.6811 nookhab manqi khan / pk ✓\n",
      "70000 86% (5m 27s) 0.6289 firq / fi ✓\n",
      "75000 92% (5m 49s) 0.5956 villedevantchaumont / fr ✓\n",
      "80000 98% (6m 11s) 0.6789 bun sik / ir ✓\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import math\n",
    "\n",
    "\n",
    "n_iters_docs = 3 # there are 3000 lines for each of 9 documents\n",
    "n_iters = n_iters_docs * 3000*n_categories # n_iters = 100000 \n",
    "print_every = 5000\n",
    "plot_every = 1000\n",
    "# Keep track of losses for plotting\n",
    "current_loss = 0\n",
    "all_losses = []\n",
    "all_accuracies = []\n",
    "printed_losses_dev = []\n",
    "printed_losses_train = []\n",
    "def timeSince(since):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "start = time.time()\n",
    "num_err = 0\n",
    "pre_err_rate = 1\n",
    "lr_temp = learning_rate\n",
    "n_blocks = 30\n",
    "# idx_blocks = [ range(i*int(3000/n_blocks),(i+1)*int(3000/n_blocks)) for i in range(n_blocks)]\n",
    "iter = 0\n",
    "for iter_doc in range(1, n_iters_docs + 1):\n",
    "    for i in range(n_blocks):\n",
    "        for catname in all_categories:\n",
    "            category_tensor = Variable(torch.LongTensor([all_categories.index(catname)]))\n",
    "            for line in category_lines[catname][i*int(3000/n_blocks):(i+1)*int(3000/n_blocks)]:\n",
    "                line_tensor = Variable(trigramTensor(line,lms_dict_tri)) # yezheng \n",
    "                #-------------------------\n",
    "                output_real, loss = train(category_tensor, line_tensor, lr_temp )#yezheng:  what is output_real\n",
    "                current_loss += loss\n",
    "                all_losses.append(loss)\n",
    "                guess, guess_i = categoryFromOutput(output_real) #yezheng: what is 'guess_i' -- the index while 'guess' is the name\n",
    "                iter += 1\n",
    "                if iter % print_every == 0:\n",
    "                    all_accuracies.append(1 - Curr_Err_Rate_dev())\n",
    "                    printed_losses_dev.append(Curr_Loss_dev())\n",
    "                    printed_losses_train.append(loss)\n",
    "                    curr_err_rate = Curr_Err_Rate_dev()#num_err*1.0/iter\n",
    "                    lr_temp = learning_rate* max( pre_err_rate - curr_err_rate,0.1)* 3\n",
    "                    pre_err_rate = curr_err_rate\n",
    "                    correct = '✓' if guess == catname else '✗ (%s)' % catname\n",
    "                    print('%d %d%% (%s) %.4f %s / %s %s' % (iter, iter / n_iters * 100, timeSince(start), pre_err_rate, line, guess, correct))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "81000\n"
     ]
    }
   ],
   "source": [
    "print(iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5,0,'iter: iteration process')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "\n",
    "plt.figure()\n",
    "# plt.plot(all_losses)\n",
    "\n",
    "x = [i for i in range(len(printed_losses_train))]\n",
    "plt.plot(x,printed_losses_train ,'g^', label = 'train set')\n",
    "plt.plot(x,printed_losses_dev,'b*', label = 'validation set')\n",
    "plt.legend(loc='upper right')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('iter: iteration process')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXwAAAEKCAYAAAARnO4WAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nwBcRCYQCX0QkEAp8EZFA/B92jJrO83wBiQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10a1777b8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAS0AAAEBCAYAAAAtjfOjAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nJK2IaJUkrYholf8Pk4eAHSFL5D0AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11356c1d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Keep track of correct guesses in a confusion matrix\n",
    "confusion = torch.zeros(n_categories, n_categories)\n",
    "n_confusion = 10000\n",
    "\n",
    "# Just return an output given a line\n",
    "\n",
    "\n",
    "# Go through a bunch of examples and record which are correctly guessed\n",
    "for i in range(n_confusion):\n",
    "    category, line, category_tensor, line_tensor = randomTrainingExample()\n",
    "    output = evaluate(line_tensor)\n",
    "    guess, guess_i = categoryFromOutput(output)\n",
    "    category_i = all_categories.index(category)\n",
    "    confusion[category_i][guess_i] += 1\n",
    "\n",
    "# Normalize by dividing every row by its sum\n",
    "for i in range(n_categories):\n",
    "    confusion[i] = confusion[i] / confusion[i].sum()\n",
    "\n",
    "# Set up plot\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "cax = ax.matshow(confusion.numpy())\n",
    "fig.colorbar(cax)\n",
    "\n",
    "# Set up axes\n",
    "ax.set_xticklabels([''] + all_categories, rotation=90)\n",
    "ax.set_yticklabels([''] + all_categories)\n",
    "\n",
    "# Force label at every tick\n",
    "ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "# sphinx_gallery_thumbnail_number = 2\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "words_test = []\n",
    "with open (\"cities_test.txt\",\"r\",encoding=\"ISO-8859-1\") as f:\n",
    "    reader = csv.reader(f)\n",
    "    for row in reader:\n",
    "        words_test.append(row[0])\n",
    "        \n",
    "pred_labels = [predict(word) for word in words_test]\n",
    "    \n",
    "output_file = open(\"../labels.txt\",\"w\")\n",
    "for item in pred_labels:\n",
    "    output_file.write(\"%s\\n\" % item)\n",
    "    \n",
    "output_file.close()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
