{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################################\n",
    "## ASSIGNMENT 2 CODE SKELETON\n",
    "## RELEASED: 1/17/2018\n",
    "## DUE: 1/24/2018\n",
    "## DESCRIPTION: In this assignment, you will explore the\n",
    "## text classification problem of identifying complex words.\n",
    "## We have provided the following skeleton for your code,\n",
    "## with several helper functions, and all the required\n",
    "## functions you need to write.\n",
    "#############################################################\n",
    "\n",
    "from collections import defaultdict\n",
    "import gzip\n",
    "import re\n",
    "#-------\n",
    "#yezheng:\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import matplotlib.pyplot as plt\n",
    "from string import ascii_lowercase\n",
    "letter_sele = 'aeiou-'#'aeiouthn' #ascii_lowercase#\n",
    "#### 1. Evaluation Metrics ####\n",
    "\n",
    "## Input: y_pred, a list of length n with the predicted labels,\n",
    "## y_true, a list of length n with the true labels\n",
    "\n",
    "## Calculates the precision of the predicted labels\n",
    "def get_precision(y_pred, y_true, debug = False):\n",
    "    # deal with npdarray\n",
    "    y_pred = list(y_pred)\n",
    "    y_true = list(y_true)\n",
    "    #---------\n",
    "    n = len(y_pred);\n",
    "    y_pred = [0 if v is None else v for v in y_pred]# deal with None type\n",
    "    y_true = [0 if v is None else v for v in y_true]# deal with None type\n",
    "    true_positive = sum(y_pred[i]* y_true[i] for i in range(n))\n",
    "    if (0 == sum(y_pred)): return 0\n",
    "    return true_positive*1.0/sum(y_pred)\n",
    "    \n",
    "## Calculates the recall of the predicted labels\n",
    "def get_recall(y_pred, y_true):\n",
    "    # deal with npdarray\n",
    "    y_pred = list(y_pred)\n",
    "    y_true = list(y_true)\n",
    "    #---------\n",
    "    n = len(y_pred);\n",
    "    y_pred = list(map(int,[1 == l for l in y_pred]))# deal with None type\n",
    "    y_true = list(map(int,[1 == l for l in y_true]))# deal with None type\n",
    "    true_positive = sum(y_pred[i]*y_true[i] for i in range(n))\n",
    "    if 0 == sum(y_true): return 0\n",
    "    return true_positive*1.0/sum(y_true)\n",
    "    \n",
    "\n",
    "## Calculates the f-score of the predicted labels\n",
    "def get_fscore(y_pred, y_true):\n",
    "    precision = get_precision(y_pred, y_true);\n",
    "    if (0 == precision): return 0\n",
    "    recall= get_recall(y_pred, y_true);\n",
    "    if (0 == recall): return 0\n",
    "    beta = 1.0;\n",
    "    # print(\"get_fscore:\",(beta**2*precision+recall))\n",
    "    fscore = (beta**2+1)*precision*recall/(beta**2*precision+recall);\n",
    "    return fscore\n",
    "\n",
    "#### 2. Complex Word Identification ####\n",
    "\n",
    "## Loads in the words and labels of one of the datasets\n",
    "def load_file(data_file, debug = False):\n",
    "    labels = []   \n",
    "    words = []\n",
    "    Lst_data = data_file\n",
    "    if isinstance(data_file, str): Lst_data = [data_file] # isinstance(data_file,basestring) for python2\n",
    "    for fname in Lst_data:\n",
    "        with open(fname, 'rt', encoding=\"utf8\") as f:\n",
    "            Lines = f.readlines()\n",
    "            Lines = Lines[1:]\n",
    "            num_data = len(Lines)# remove first ele in Lines, remove last one \n",
    "            Lst_pos1 = [line[:-1].find('\\t') for line in Lines]\n",
    "            words += [Lines[i][:Lst_pos1[i]] for i in range(num_data)]\n",
    "            Lst_pos2 = [Lines[i][(Lst_pos1[i]+1):-1].find('\\t')+Lst_pos1[i]+1 for i in range(num_data )]\n",
    "            for i in range(num_data ):\n",
    "                # if debug:\n",
    "                #     print(i,\"----\",Lines[i])\n",
    "                #     print(\"#########\",Lines[i][(Lst_pos1[i]+1):Lst_pos2[i]],\"#######\")\n",
    "                if (re.match(\"^\\d+?(\\.\\d+)?$\",Lines[i][(Lst_pos1[i]+1):Lst_pos2[i]])): labels.append(int(Lines[i][(Lst_pos1[i]+1):Lst_pos2[i]]))\n",
    "                else: labels.append(0) #None\n",
    "    # if debug: print(\"load file DEBUG:\",len(words),len(labels),labels)\n",
    "        # labels = [int(Lines[i+1][(Lst_pos1[i]+1):Lst_pos2[i]]) for i in range(num_data)] \n",
    "    return words, labels\n",
    "\n",
    "#-------------\n",
    "# yezheng: default one\n",
    "# ## Loads in the words and labels of one of the datasets\n",
    "# def load_file(data_file):\n",
    "#     words = []\n",
    "#     labels = []   \n",
    "#     with open(data_file, 'rt', encoding=\"utf8\") as f:\n",
    "#         i = 0\n",
    "#         for line in f:\n",
    "#             if i > 0:\n",
    "#                 line_split = line[:-1].split(\"\\t\")\n",
    "#                 words.append(line_split[0])\n",
    "#                 labels.append(int(line_split[1]))\n",
    "#             i += 1\n",
    "#     return words, labels\n",
    "#-------------\n",
    "### 2.1: A very simple baseline\n",
    "\n",
    "## Labels every word complex\n",
    "def all_complex(data_file):\n",
    "    ## YOUR CODE HERE...\n",
    "    words,labels = load_file(data_file)\n",
    "    y_pred = [1] * len(words)\n",
    "    precision = get_precision(y_pred, labels)\n",
    "    recall = get_recall(y_pred, labels)\n",
    "    fscore = get_fscore(y_pred, labels)\n",
    "    return precision, recall, fscore\n",
    "\n",
    "\n",
    "### 2.2: Word length thresholding\n",
    "\n",
    "## Finds the best length threshold by f-score, and uses this threshold to\n",
    "## classify the training and development set\n",
    "def word_length_threshold(training_file, development_file, plot_flag = False):\n",
    "    ## YOUR CODE HERE\n",
    "    # print(\"load training file\")\n",
    "    words,labels = load_file(training_file, True)\n",
    "    Dict_len = defaultdict(set)\n",
    "    for w in words: Dict_len[len(w)].add(w)\n",
    "    # Evaluation depending on fscore\n",
    "    Min = min(Dict_len)\n",
    "    Max = max(Dict_len)\n",
    "    # for Thres in range(Min,Max): \n",
    "    #     # print(list(map(int,[len(w) < Thres for w in words])))\n",
    "    #     print(get_precision(list(map(int,[len(w) < Thres for w in words])), labels ) ) \n",
    "    #----------\n",
    "    #yezheng: binary search is possible to improve the serach\n",
    "    #plotting precision vs. recall \n",
    "    ThreRange =  range(Min,Max+1)\n",
    "    FscoreL = dict([(get_fscore(list(map(int,[len(w) > Thres for w in words])), labels),Thres) for Thres in ThreRange])\n",
    "    Thres_opt = FscoreL[max(FscoreL)]\n",
    "    if plot_flag:\n",
    "        print(\"Range of thresholds:\",Min,\"to\",Max, \" with optimal threshold:\", Thres_opt+1)\n",
    "        precisionL = [get_precision(list(map(int,[len(w) > Thres for w in words])), labels)for Thres in ThreRange]\n",
    "        recallL = [get_recall(list(map(int,[len(w) > Thres for w in words])), labels) for Thres in ThreRange]\n",
    "        Ret_plot = [precisionL,recallL]\n",
    "        plt.plot(recallL, precisionL,'^r', label = 'Train')\n",
    "        plt.title('word_length_threshold: precision-recall curve')\n",
    "        plt.xlabel('Recall')\n",
    "        plt.ylabel('Precision')\n",
    "\n",
    "    Pred_opt = list(map(int,[len(w)> Thres_opt for w in words]))\n",
    "    training_performance = [get_precision(Pred_opt, labels), get_recall(Pred_opt, labels), get_fscore(Pred_opt, labels)]\n",
    "    del words\n",
    "    del labels\n",
    "    # print(\"load development file\")\n",
    "    words,labels = load_file(development_file)\n",
    "    Pred_opt = list(map(int,[len(w)> Thres_opt for w in words]))\n",
    "    if plot_flag: \n",
    "        precisionL = [get_precision(list(map(int,[len(w) > Thres for w in words])), labels)for Thres in ThreRange]\n",
    "        recallL = [get_recall(list(map(int,[len(w) > Thres for w in words])), labels) for Thres in ThreRange]\n",
    "        plt.plot(recallL, precisionL,'^g', label = 'Dev')\n",
    "        plt.legend(loc='upper right')\n",
    "        plt.show()\n",
    "        Ret_plot += [precisionL,recallL]\n",
    "    development_performance = [get_precision(Pred_opt, labels), get_recall(Pred_opt, labels), get_fscore(Pred_opt, labels)]\n",
    "    if plot_flag: return training_performance, development_performance,Ret_plot\n",
    "    return training_performance, development_performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 2.3: Word frequency thresholding\n",
    "## Loads Google NGram counts\n",
    "def load_ngram_counts(ngram_counts_file): \n",
    "   # counts = defaultdict(int) \n",
    "   counts = {}\n",
    "   with gzip.open(ngram_counts_file, 'rt') as f: \n",
    "       for line in f:\n",
    "           token, count = line.strip().split('\\t') \n",
    "           if token[0].islower(): \n",
    "               counts[token] = int(count) \n",
    "   return counts\n",
    "\n",
    "# Finds the best frequency threshold by f-score, and uses this threshold to\n",
    "## classify the training and development set\n",
    "def word_frequency_threshold(training_file, development_file, counts, plot_flag = False):\n",
    "    words,labels = load_file(training_file)\n",
    "#     Min = min(counts[w] for w in words if w in counts)\n",
    "#     Max_ = max(counts[w] for w in words if w in counts)\n",
    "#     Max = Max_#int((Max_ - Min) /(10**7)) + Min # manually do the test\n",
    "#     ThreRange = range(Min,Max, int((Max-Min)/10))\n",
    "    Min =int(19903896) #19903996#<- 19903896#<-19903906# <-19902396 #<- 19881406 #<- 19802396\n",
    "    Max = 2* 19903996  - 19903896 \n",
    "#     print(\"Max - Min\",Max - Min) #158020\n",
    "    ThreRange = range(Min,Max) # int((Max-Min)/3000)\n",
    "#     ThreRange = range(Min,Max)\n",
    "    FscoreL = dict([(get_fscore(list(map(int,[counts[w]< Thres if w in counts else 0 for w in words])), labels) ,Thres) for Thres in ThreRange])\n",
    "    Thres_opt = FscoreL[max(FscoreL)]\n",
    "    if plot_flag:\n",
    "#         print(\"Range of thresholds:\",Min,\"to\",Max, \" with optimal threshold:\", Thres_opt) # Dev has larger range\n",
    "        Min = 137\n",
    "        Max = 1120679362\n",
    "        ThreRange = range(Min,Max,int((Max-Min)/1000))\n",
    "        precisionL = [get_precision(list(map(int,[counts[w]< Thres if w in counts else 0 for w in words])), labels)  for Thres in ThreRange]\n",
    "        recallL = [get_recall(list(map(int,[counts[w]< Thres if w in counts else 0 for w in words])), labels)  for Thres in ThreRange]\n",
    "#         print(\"precisionL\",precisionL)\n",
    "#         print(\"recallL\", recallL)\n",
    "        plt.plot(recallL, precisionL,'.b',label = \"Train\")\n",
    "        plt.title('word_frequency_threshold: precision-recall curve')\n",
    "        plt.xlabel('Recall')\n",
    "        plt.ylabel('Precision')\n",
    "        Ret_plot = [precisionL,recallL]\n",
    "    labels_pred = list(map(int,[counts[w]< Thres_opt if w in counts else 0 for w in words]))\n",
    "    training_performance = [get_precision(labels_pred,labels), get_recall(labels_pred,labels), get_fscore(labels_pred,labels)]\n",
    "    words,labels = load_file(development_file, True)\n",
    "#     Min = min(counts[w] for w in words if w in counts)\n",
    "#     Max_ = max(counts[w] for w in words if w in counts)\n",
    "#     Max = Max_#int((Max_ - Min) /(10**7)) + Min # manually do the test\n",
    "#     ThreRange = range(Min,Max, int((Max-Min)/10))\n",
    "#     ThreRange = range(Min,Max, int((Max-Min)/1000))\n",
    "    labels_pred = list(map(int,[counts[w]< Thres_opt if w in counts else 0 for w in words]))\n",
    "    if plot_flag:\n",
    "        Min = 137\n",
    "        Max = 1120679362\n",
    "        ThreRange = range(Min,Max,int((Max-Min)/1000))\n",
    "        print(\"Range of thresholds:\",Min,\"to\",Max, \" with optimal threshold:\", Thres_opt)\n",
    "        precisionL = [get_precision(list(map(int,[counts[w]< Thres if w in counts else 0 for w in words])), labels)  for Thres in ThreRange]\n",
    "        recallL = [get_recall(list(map(int,[counts[w]< Thres if w in counts else 0 for w in words])), labels)  for Thres in ThreRange]\n",
    "#         print(\"precisionL\",precisionL)\n",
    "#         print(\"recallL\", recallL)\n",
    "        plt.plot(recallL, precisionL,'.y',label = \"Dev\")\n",
    "        plt.legend(loc='upper right')\n",
    "        plt.show()\n",
    "        Ret_plot += [precisionL,recallL]\n",
    "    development_performance = [get_precision(labels_pred,labels), get_recall(labels_pred,labels), get_fscore(labels_pred,labels)]\n",
    "    if plot_flag: return training_performance, development_performance,Ret_plot\n",
    "    return training_performance, development_performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 2.4: Naive Bayes\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "## Trains a Naive Bayes classifier using length and frequency features\n",
    "def naive_bayes(training_file, development_file, counts):    \n",
    "    words,labels = load_file(training_file)\n",
    "    labels_np = np.array(labels)\n",
    "    X_features = np.array([[1.0*len(w), counts[w]] if w in counts else [1.0*len(w),0] for w in words])\n",
    "    scaler = sklearn.preprocessing.StandardScaler(); scaler.fit(X_features); X_features = scaler.transform(X_features)\n",
    "    clf = GaussianNB(); clf.fit(X_features, labels_np)\n",
    "    Y_pred_np = clf.predict(X_features)\n",
    "\n",
    "    training_performance = [get_precision(labels_np, Y_pred_np), get_recall(labels_np, Y_pred_np), get_fscore(labels_np, Y_pred_np)]\n",
    "    words, labels = load_file(development_file)\n",
    "    labels_np = np.array(labels)\n",
    "    X_features = np.array([[1.0*len(w), counts[w]] if w in counts else [1.0*len(w),0] for w in words])\n",
    "    scaler = sklearn.preprocessing.StandardScaler(); scaler.fit(X_features); X_features = scaler.transform(X_features)\n",
    "    Y_pred_np = clf.predict(X_features)\n",
    "    development_performance = [get_precision(labels_np, Y_pred_np), get_recall(labels_np, Y_pred_np), get_fscore(labels_np, Y_pred_np)]\n",
    "    return training_performance, development_performance\n",
    "\n",
    "### 2.5: Logistic Regression\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression\n",
    "## Trains a Naive Bayes classifier using length and frequency features\n",
    "def logistic_regression(training_file, development_file, counts):\n",
    "    words,labels = load_file(training_file)\n",
    "    labels_np = np.array(labels)\n",
    "    X_features = np.array([[1.0*len(w), counts[w]] if w in counts else [1.0*len(w),0] for w in words])\n",
    "    scaler = sklearn.preprocessing.StandardScaler(); scaler.fit(X_features); X_features = scaler.transform(X_features)\n",
    "    clf = LogisticRegression(); #penalty = 'l1'\n",
    "    clf.fit(X_features, labels_np)\n",
    "    Y_pred_np = clf.predict(X_features)\n",
    "\n",
    "    training_performance = [get_precision(labels_np, Y_pred_np), get_recall(labels_np, Y_pred_np), get_fscore(labels_np, Y_pred_np)]\n",
    "    words, labels = load_file(development_file)\n",
    "    labels_np = np.array(labels)\n",
    "    X_features = np.array([[1.0*len(w), counts[w]] if w in counts else [1.0*len(w),0] for w in words])\n",
    "    scaler = sklearn.preprocessing.StandardScaler(); scaler.fit(X_features); X_features = scaler.transform(X_features)\n",
    "    Y_pred_np = clf.predict(X_features)\n",
    "    # print(\"Y_pred_np\",Y_pred_np)\n",
    "    development_performance = [get_precision(labels_np, Y_pred_np), get_recall(labels_np, Y_pred_np), get_fscore(labels_np, Y_pred_np)]\n",
    "    return training_performance, development_performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 2.7: Build your own classifier\n",
    "## Trains a classifier of your choosing, predicts labels for the test dataset\n",
    "## and writes the predicted labels to the text file 'test_labels.txt',\n",
    "## with ONE LABEL PER LINE\n",
    "from syllables import count_syllables\n",
    "# from nltk.corpus import wordnet as wn #wn.lemma(w).count(),\n",
    "def preprocess_yezheng(words,labels, counts):\n",
    "    Thres_opt_len = 6\n",
    "    Thres_opt_freq = 19904037#<-19903996#<- 19903896#<-19903906# <-19902396 #<- 19881406 #<- 19802396\n",
    "    # 1.0*len(w),\n",
    "#     X_features = [[1.0*len(w),count_syllables(w),[0,1][len(w) > Thres_opt_len], int(counts[w] < Thres_opt_freq), counts[w] ]+[w.count(alp) for alp in letter_sele] if w in counts else [1.0*len(w),count_syllables(w),[0,1][len(w) > Thres_opt_len],1,1120679362]+[w.count(alp) for alp in letter_sele] for w in words]\n",
    "    # best\n",
    "    X_features = np.array([[1.0*len(w),count_syllables(w),[0,1][len(w) > Thres_opt_len], int(counts[w] < Thres_opt_freq), counts[w] ]+[w.count(alp) for alp in letter_sele] if w in counts else [1.0*len(w),count_syllables(w),[0,1][len(w) > Thres_opt_len],1,1120679362]+[w.count(alp) for alp in letter_sele] for w in words]) \n",
    "#     X_features = np.array([[1.0*len(w),count_syllables(w),[0,1][len(w) > Thres_opt_len], int(counts[w] < Thres_opt_freq) ]+[w.count(alp) for alp in letter_sele] if w in counts else [1.0*len(w),count_syllables(w),[0,1][len(w) > Thres_opt_len],1]+[w.count(alp) for alp in letter_sele] for w in words])\n",
    "    scaler = sklearn.preprocessing.StandardScaler(); scaler.fit(X_features); X_features = scaler.transform(X_features)\n",
    "#     X_features = np.array([np.concatenate((row,np.convolve(row,row))) for row in X_features])\n",
    "#     scaler = sklearn.preprocessing.StandardScaler(); scaler.fit(X_features); X_features = scaler.transform(X_features)\n",
    "    return X_features, np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "def improved_naive_bayes(training_file, development_file, counts,show_err_words_flag = False):    \n",
    "    words,labels = load_file(training_file)\n",
    "    labels_np = np.array(labels)\n",
    "    X_features, labels_np = preprocess_yezheng(words, labels,counts)\n",
    "    scaler = sklearn.preprocessing.StandardScaler(); scaler.fit(X_features); X_features = scaler.transform(X_features)\n",
    "    clf = GaussianNB(); clf.fit(X_features, labels_np)\n",
    "    Y_pred_np = clf.predict(X_features)\n",
    "    if show_err_words_flag: L_train = [X_features,words,list(Y_pred_np),labels]\n",
    "#         print(\"Naive Bayes (improved)\")\n",
    "#         Y_lst = list(Y_pred_np)\n",
    "#         print(\"Train:\",[words[i] for i in range(len(words)) if not Ylst[i] == labels[i]])\n",
    "    training_performance = [get_precision(labels_np, Y_pred_np), get_recall(labels_np, Y_pred_np), get_fscore(labels_np, Y_pred_np)]\n",
    "    words, labels = load_file(development_file)\n",
    "    labels_np = np.array(labels)\n",
    "    X_features, labels_np = preprocess_yezheng(words, labels, counts)\n",
    "    scaler = sklearn.preprocessing.StandardScaler(); scaler.fit(X_features); X_features = scaler.transform(X_features)\n",
    "    Y_pred_np = clf.predict(X_features)\n",
    "    if show_err_words_flag: L_dev = [X_features,words,list(Y_pred_np),labels]\n",
    "#     if show_err_words_flag: \n",
    "#         print(\"Dev:\",[words[i] for i in range(len(words)) if not Ylst[i] == labels[i]])\n",
    "    development_performance = [get_precision(labels_np, Y_pred_np), get_recall(labels_np, Y_pred_np), get_fscore(labels_np, Y_pred_np)]\n",
    "    if not show_err_words_flag: return training_performance, development_performance  \n",
    "    else: return clf,L_train, L_dev\n",
    "\n",
    "def improved_log_regression(training_file, development_file, counts):\n",
    "    words,labels = load_file(training_file)\n",
    "    labels_np = np.array(labels)\n",
    "    X_features, labels_np = preprocess_yezheng(words, labels, counts)    \n",
    "    scaler = sklearn.preprocessing.StandardScaler(); scaler.fit(X_features); X_features = scaler.transform(X_features)\n",
    "    clf = LogisticRegression(); #penalty = 'l1'\n",
    "    clf.fit(X_features, labels_np)\n",
    "    Y_pred_np = clf.predict(X_features)\n",
    "    training_performance = [get_precision(labels_np, Y_pred_np), get_recall(labels_np, Y_pred_np), get_fscore(labels_np, Y_pred_np)]\n",
    "    words, labels = load_file(development_file)\n",
    "    labels_np = np.array(labels)\n",
    "    X_features, labels_np = preprocess_yezheng(words, labels, counts)\n",
    "    scaler = sklearn.preprocessing.StandardScaler(); scaler.fit(X_features); X_features = scaler.transform(X_features)\n",
    "    Y_pred_np = clf.predict(X_features)\n",
    "    development_performance = [get_precision(labels_np, Y_pred_np), get_recall(labels_np, Y_pred_np), get_fscore(labels_np, Y_pred_np)]\n",
    "    return training_performance, development_performance\n",
    "\n",
    "def random_forest(training_file, development_file, counts):\n",
    "    words,labels = load_file(training_file)\n",
    "    X_features, labels_np = preprocess_yezheng(words, labels, counts)\n",
    "    clf = RandomForestClassifier(max_depth=2, random_state=0); clf.fit(X_features, labels_np) # http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html#sklearn.ensemble.RandomForestClassifier\n",
    "    Y_pred_np = clf.predict(X_features)\n",
    "    training_performance = [get_precision(labels_np, Y_pred_np), get_recall(labels_np, Y_pred_np), get_fscore(labels_np, Y_pred_np)]\n",
    "    words, labels = load_file(development_file)\n",
    "    X_features, labels_np = preprocess_yezheng(words, labels, counts)\n",
    "    scaler = sklearn.preprocessing.StandardScaler(); scaler.fit(X_features); X_features = scaler.transform(X_features)\n",
    "    Y_pred_np = clf.predict(X_features)\n",
    "    development_performance = [get_precision(labels_np, Y_pred_np), get_recall(labels_np, Y_pred_np), get_fscore(labels_np, Y_pred_np)]\n",
    "    return training_performance, development_performance\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier \n",
    "def decision_tree(training_file, development_file, counts):\n",
    "    words,labels = load_file(training_file)\n",
    "    X_features, labels_np = preprocess_yezheng(words, labels, counts)\n",
    "    clf = DecisionTreeClassifier(max_depth=2, random_state=0); clf.fit(X_features, labels_np) # http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html#sklearn.ensemble.RandomForestClassifier\n",
    "    Y_pred_np = clf.predict(X_features)\n",
    "\n",
    "    training_performance = [get_precision(labels_np, Y_pred_np), get_recall(labels_np, Y_pred_np), get_fscore(labels_np, Y_pred_np)]\n",
    "    words, labels = load_file(development_file)\n",
    "    X_features, labels_np = preprocess_yezheng(words, labels, counts)\n",
    "    Y_pred_np = clf.predict(X_features)\n",
    "    development_performance = [get_precision(labels_np, Y_pred_np), get_recall(labels_np, Y_pred_np), get_fscore(labels_np, Y_pred_np)]\n",
    "    return training_performance, development_performance\n",
    "\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "def SVM_SVC(training_file, development_file, counts,show_err_words_flag = False):\n",
    "    words,labels = load_file(training_file)\n",
    "    X_features, labels_np = preprocess_yezheng(words, labels, counts)\n",
    "    clf = SVC(); clf.fit(X_features, labels_np) \n",
    "    Y_pred_np = clf.predict(X_features)\n",
    "    if show_err_words_flag: L_train = [X_features,words,list(Y_pred_np),labels]\n",
    "    training_performance = [get_precision(labels_np, Y_pred_np), get_recall(labels_np, Y_pred_np), get_fscore(labels_np, Y_pred_np)]\n",
    "    words, labels = load_file(development_file)\n",
    "    X_features, labels_np = preprocess_yezheng(words, labels, counts)\n",
    "    Y_pred_np = clf.predict(X_features)\n",
    "    if show_err_words_flag: L_dev = [X_features,words,list(Y_pred_np),labels]\n",
    "    development_performance = [get_precision(labels_np, Y_pred_np), get_recall(labels_np, Y_pred_np), get_fscore(labels_np, Y_pred_np)]\n",
    "    if not show_err_words_flag: return training_performance, development_performance  \n",
    "    else: return clf,L_train, L_dev\n",
    "\n",
    "def SVM_LinearSVC(training_file, development_file, counts):\n",
    "    words,labels = load_file(training_file)  \n",
    "    X_features, labels_np = preprocess_yezheng(words, labels, counts)\n",
    "    clf = LinearSVC(); clf.fit(X_features, labels_np) # http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html#sklearn.ensemble.RandomForestClassifier\n",
    "    Y_pred_np = clf.predict(X_features)\n",
    "    training_performance = [get_precision(labels_np, Y_pred_np), get_recall(labels_np, Y_pred_np), get_fscore(labels_np, Y_pred_np)]\n",
    "    words, labels = load_file(development_file)\n",
    "    X_features, labels_np = preprocess_yezheng(words, labels, counts)\n",
    "    Y_pred_np = clf.predict(X_features)\n",
    "    development_performance = [get_precision(labels_np, Y_pred_np), get_recall(labels_np, Y_pred_np), get_fscore(labels_np, Y_pred_np)]\n",
    "    return training_performance, development_performance\n",
    "\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis,LinearDiscriminantAnalysis\n",
    "def QDA(training_file, development_file, counts):\n",
    "    words,labels = load_file(training_file)\n",
    "    X_features, labels_np = preprocess_yezheng(words, labels, counts)\n",
    "    clf = QuadraticDiscriminantAnalysis(); clf.fit(X_features, labels_np) # http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html#sklearn.ensemble.RandomForestClassifier\n",
    "    Y_pred_np = clf.predict(X_features)\n",
    "\n",
    "    training_performance = [get_precision(labels_np, Y_pred_np), get_recall(labels_np, Y_pred_np), get_fscore(labels_np, Y_pred_np)]\n",
    "    words, labels = load_file(development_file)\n",
    "    X_features, labels_np = preprocess_yezheng(words, labels, counts)\n",
    "    Y_pred_np = clf.predict(X_features)\n",
    "    development_performance = [get_precision(labels_np, Y_pred_np), get_recall(labels_np, Y_pred_np), get_fscore(labels_np, Y_pred_np)]\n",
    "    return training_performance, development_performance\n",
    "\n",
    "def LDA(training_file, development_file, counts):\n",
    "    words,labels = load_file(training_file)\n",
    "    X_features, labels_np = preprocess_yezheng(words, labels, counts)\n",
    "    clf = LinearDiscriminantAnalysis(); clf.fit(X_features, labels_np) # http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html#sklearn.ensemble.RandomForestClassifier\n",
    "    Y_pred_np = clf.predict(X_features)\n",
    "    training_performance = [get_precision(labels_np, Y_pred_np), get_recall(labels_np, Y_pred_np), get_fscore(labels_np, Y_pred_np)]\n",
    "    words, labels = load_file(development_file)\n",
    "    labels_np = np.array(labels)\n",
    "    X_features, labels_np = preprocess_yezheng(words, labels, counts)\n",
    "    Y_pred_np = clf.predict(X_features)\n",
    "    development_performance = [get_precision(labels_np, Y_pred_np), get_recall(labels_np, Y_pred_np), get_fscore(labels_np, Y_pred_np)]\n",
    "    return training_performance, development_performance\n",
    "from sklearn.ensemble import AdaBoostClassifier, GradientBoostingClassifier\n",
    "def AdaBoost(training_file, development_file, counts):\n",
    "    words,labels = load_file(training_file)\n",
    "    X_features, labels_np = preprocess_yezheng(words, labels, counts)\n",
    "    clf = AdaBoostClassifier(); clf.fit(X_features, labels_np) # http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html#sklearn.ensemble.RandomForestClassifier\n",
    "    Y_pred_np = clf.predict(X_features)\n",
    "    training_performance = [get_precision(labels_np, Y_pred_np), get_recall(labels_np, Y_pred_np), get_fscore(labels_np, Y_pred_np)]\n",
    "    words, labels = load_file(development_file)\n",
    "    labels_np = np.array(labels)\n",
    "    X_features, labels_np = preprocess_yezheng(words, labels, counts)\n",
    "    Y_pred_np = clf.predict(X_features)\n",
    "    development_performance = [get_precision(labels_np, Y_pred_np), get_recall(labels_np, Y_pred_np), get_fscore(labels_np, Y_pred_np)]\n",
    "    return training_performance, development_performance\n",
    "def GradBoost(training_file, development_file, counts, show_err_words_flag = False):\n",
    "    words,labels = load_file(training_file)\n",
    "    X_features, labels_np = preprocess_yezheng(words, labels, counts)\n",
    "    clf = GradientBoostingClassifier(); clf.fit(X_features, labels_np) # http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html#sklearn.ensemble.RandomForestClassifier\n",
    "    Y_pred_np = clf.predict(X_features)\n",
    "    if show_err_words_flag: L_train = [X_features,words,list(Y_pred_np),labels]\n",
    "    training_performance = [get_precision(labels_np, Y_pred_np), get_recall(labels_np, Y_pred_np), get_fscore(labels_np, Y_pred_np)]\n",
    "    words, labels = load_file(development_file)\n",
    "    X_features, labels_np = preprocess_yezheng(words, labels, counts)\n",
    "    Y_pred_np = clf.predict(X_features)\n",
    "    if show_err_words_flag: L_dev = [X_features,words,list(Y_pred_np),labels]\n",
    "    development_performance = [get_precision(labels_np, Y_pred_np), get_recall(labels_np, Y_pred_np), get_fscore(labels_np, Y_pred_np)]\n",
    "    if not show_err_words_flag: return training_performance, development_performance  \n",
    "    else: return clf,L_train, L_dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# training_file = \"data/complex_words_training.txt\"\n",
    "# development_file = \"data/complex_words_development.txt\"\n",
    "# test_file = \"data/complex_words_test_unlabeled.txt\"\n",
    "# ngram_counts_file = \"ngram_counts.txt.gz\"\n",
    "# counts = load_ngram_counts(ngram_counts_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HW2-Writeup 2018-01-25 01:56:12.251059\n",
      "Yezheng Li, Daizhen Li\n",
      "-------------------------\n",
      "\u001b[1;32;10mBaselines\u001b[0m\n",
      "\u001b[1;32;10mAll-complex Baseline:\u001b[0m\n",
      "Train: precision 0.43275 recall 1.0 F-score 0.604083057058105\n",
      "Dev: precision 0.418 recall 1.0 F-score 0.5895627644569816\n",
      "\u001b[1;32;10mWord-length Baseline:\u001b[0m\n",
      "Range of thresholds: 3 to 20  with optimal threshold: 7\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEWCAYAAACJ0YulAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x139128cc0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: precision 0.6007401315789473 recall 0.8440207972270364 F-score 0.7018976699495555\n",
      "Dev: precision 0.6053511705685619 recall 0.8660287081339713 F-score 0.7125984251968505\n",
      "\u001b[1;32;10mWord-frequency Baseline:\u001b[0m\n",
      "Range of thresholds: 137 to 1120679362  with optimal threshold: 19904037\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYYAAAEWCAYAAABi5jCmAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nMzMr8P8Bw6v5DATcGb8AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1392009b0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: precision 0.6140191169776968 recall 0.7793183131138071 F-score 0.6868635437881874\n",
      "Dev: precision 0.599644128113879 recall 0.80622009569378 F-score 0.6877551020408164\n",
      "\u001b[1;32;10mPlot Precison-Recall curve for various thresholds for both baselines together:\u001b[0m\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAdEAAAEWCAYAAAA5Lq2XAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n0kCGqCRJAxmikiQNZIhKkjTQa4iwVfOElOJtAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x139172da0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;32;10mWhich classifier looks better on average?\u001b[0m I think word_length_threshold is better.\n",
      "-------------------------------------------------------------------------------------------------------\n",
      "\u001b[1;32;10mNaive Bayes:\u001b[0m\n",
      "Train: precision 0.867128827267 recall 0.5972940708316753 F-score 0.707351555137\n",
      "Dev: precision 0.894736842105 recall 0.5917721518987342 F-score 0.712380952381\n",
      "\u001b[1;32;10mLogistic regression:\u001b[0m\n",
      "Train: precision 0.643558636626 recall 0.7159383033419023 F-score 0.677821721935\n",
      "Dev: precision 0.684210526316 recall 0.7240506329113924 F-score 0.70356703567\n",
      "\u001b[1;32;10mAdd a paragraph to your write up that discusses which model performed better on this task.\u001b[0m\n",
      "Although naive Bayes performslightly better in term of F-score, I think logistic regression have more balanced performance between precision and recall.\n",
      "-------------------------------------------------------------------------------------------------------\n",
      "\u001b[1;32;10mBuild your own model\u001b[0m\n",
      "\u001b[1;32;10mPlease include a description of all features that you tried (not including length and frequency).\u001b[0m\n",
      "Besides ength and frequency (with and without thresholds -- 4 features), I tried: [see preprocess_yezheng(...)]\n",
      "--- count of character 'aeiou-' (I also tried string.ascii_lowercase,etc. but the latter has worse performance) -- 6 features; \n",
      "--- count_syllables(...) -- 1 features\n",
      "--- convolution of feature with itself: np.convolve(X,X) -- does not work\n",
      "In all, there are 11 features\n",
      "\u001b[1;32;10mPlease include a description of all models that you tried.\u001b[0m\n",
      "Besides improved Naive Bayes and improved logistic regression (with penality l1 or l2), I tried:\n",
      "random forest, decision tree, svm.SVC, svm.LinearSVC, LDA (linear discriminant analysis), QDA (Quadratic Discriminant Analysis). \n",
      "\u001b[1;32;10mPerform a detailed error analysis of your models.\u001b[0m\n",
      "See table below for a summary. \u001b[1;31;10mRed\u001b[0m highlights are best performances (it varies depending on different experiments, I just highlight best performances in common (for various experiments.)).\n",
      "Train:\t\t\t\t\tDev\n",
      "precision\trecall\tF-score\t\tprecision\trecall\tF-score\tclassifier\n",
      "Baselines--------------------------------------------------------------------------------------------------------\n",
      "0.432750000 1.000000000 0.604083057|0.418000000 1.000000000 0.589562764 All-complex\n",
      "0.600740132 0.844020797 0.701897670|0.605351171 0.866028708 0.712598425 Word-length\n",
      "0.614019117 0.779318313 0.686863544|0.599644128 0.806220096 0.687755102 Word-frequency\n",
      "--------------------------------------------------------------------------------------------------------\n",
      "0.867128827 0.597294071 0.707351555|0.894736842 0.591772152 0.712380952 Naive Bayes\n",
      "0.643558637 0.715938303 0.677821722|0.684210526 0.724050633 0.703567036 Logistic regression\n",
      "--------------------------------------------------------------------------------------------------------\n",
      "0.773541306 0.702518363 0.736321144|0.794258373 0.723311547 0.757126568 Naive Bayes (improved)\n",
      "0.738301560 0.740440324 0.739369395|0.767942584 0.741339492 0.754406580 Logistic regression (improved)\n",
      "0.686886193 0.709850746 0.698179683|0.712918660 0.712918660 0.712918660 Random forest\n",
      "0.790872328 0.724722075 0.756353591|0.811004785 0.701863354 0.752497225 Decision tree\n",
      "0.798382438 0.734715577 \u001b[1;31;10m0.765227021\u001b[0m|0.806220096 0.729437229 \u001b[1;31;10m0.765909091\u001b[0m SVM SVC\n",
      "0.741767764 0.734553776 0.738143145|0.770334928 0.743648961 0.756756757 SVM linear SVC\n",
      "0.916233391 0.637459807 0.751836928|0.904306220 0.618657938 0.734693878 QDA\n",
      "0.734835355 0.734835355 0.734835355|0.767942584 0.741339492 0.754406580 LDA\n",
      "0.754477181 0.766881973 0.760629004|0.799043062 0.734065934 0.765177549 AdaBoost\n",
      "0.812247256 0.773377338 \u001b[1;31;10m0.792335869\u001b[0m|0.827751196 0.726890756 \u001b[1;31;10m0.774049217\u001b[0m Gradient Boost\n",
      "--------------------------------------------------------------------------------------------------------\n",
      "\u001b[1;32;10mAnalyze your model\u001b[0m\n",
      "\u001b[1;32;10mAn important part of text classification tasks is to determine what your model is getting correct, and what your model is getting wrong. For this problem, you must train your best model on the training data, and report the precision, recall, and f-score on the development data.\u001b[0m\n",
      "As a result, I think our best model is \u001b[1;31;10mSVM SVC\u001b[0m as well as \u001b[1;31;10mGradient Boost\u001b[0m (Although GradBoost generally outperforms SVM SVC in both Train and Dev with respect to F score, it is surprising that when it comes to Leaderboard, SVM SVC results in better ranking (typically with 2\\% better Fscore).)\n",
      "\u001b[1;32;10mGive several examples of words on which your best model performs well. Also give examples of words which your best model performs poorly on, and identify at least TWO categories of words on which your model is making errors.\u001b[0m\n",
      "-------------------------------------\n",
      "\u001b[1;31;10mGradient Boost: \u001b[1;32;10mcorrect prediction:\u001b[0m\n",
      "Examples of true positive ['derailed', 'magma', 'emergency', 'aced', 'assistance', 'fatalities', 'fair-weather', 'complicated', 'krill', 'affirmed']\n",
      "Examples of false negative ['string', 'shaping', 'worked', 'away', 'spray', 'wear', 'closely', 'code-named', 'blood', 'pass']\n",
      "\u001b[1;32;10mIncorrect prediction:\u001b[0m\n",
      "Examples of false positive (i.e. not complex, but are predicted to be) ['asylum-seekers', 'considers', 'airliners', 'jumping', 'makings', 'fishermen', 'worldwide', 'breathing', 'motorcycle', 'destroying']\n",
      "Examples of true negative (i.e. complex, but are predicted not to be) ['required', 'canoes', 'potential', 'concerts', 'patch', 'nylon', 'assist', 'wartime', 'brisk', 'ironic']\n",
      "\u001b[1;31;10mSVM SVC: \u001b[1;32;10mcorrect prediction:\u001b[0m\n",
      "Examples of true positive ['derailed', 'emergency', 'assistance', 'fatalities', 'complicated', 'affirmed', 'undermining', 'certificates']\n",
      "Examples of false negative ['string', 'worked', 'away', 'spray', 'wear', 'closely', 'code-named', 'blood']\n",
      "\u001b[1;32;10mIncorrect prediction:\u001b[0m\n",
      "Examples of false positive (i.e. not complex, but are predicted to be) ['asylum-seekers', 'shaping', 'considers', 'airliners', 'jumping', 'choices', 'makings', 'jumped']\n",
      "Examples of true negative (i.e. complex, but are predicted not to be) ['magma', 'aced', 'required', 'canoes', 'fair-weather', 'krill', 'potential', 'chug']\n",
      "-------------------------------------\n",
      "Time elapsed: 31.625770092010498\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    import time\n",
    "    T0 = time.time()\n",
    "    training_file = \"data/complex_words_training.txt\"\n",
    "    development_file = \"data/complex_words_development.txt\"\n",
    "    test_file = \"data/complex_words_test_unlabeled.txt\"\n",
    "    ngram_counts_file = \"ngram_counts.txt.gz\"\n",
    "    counts = load_ngram_counts(ngram_counts_file)\n",
    "    all_complex(training_file)\n",
    "    import datetime\n",
    "    print(\"HW2-Writeup\", datetime.datetime.now())\n",
    "    print(\"Yezheng Li, Daizhen Li\")\n",
    "    print(\"-------------------------\")\n",
    "    print(\"\\033[1;32;10mBaselines\\033[0m\")\n",
    "    print(\"\\033[1;32;10mAll-complex Baseline:\\033[0m\")\n",
    "    p1,r1,f1 = all_complex(training_file); print(\"Train: precision\",p1,\"recall\",r1,\"F-score\",f1)\n",
    "    p2,r2,f2 = all_complex(development_file); print(\"Dev: precision\",p2,\"recall\",r2,\"F-score\",f2)\n",
    "    print(\"\\033[1;32;10mWord-length Baseline:\\033[0m\")\n",
    "    result1, result2,Ret_plot_len = word_length_threshold(training_file,development_file ,True)\n",
    "    p1,r1,f1 = result1; p2,r2,f2= result2\n",
    "    print(\"Train: precision\",p1,\"recall\",r1,\"F-score\",f1)\n",
    "    print(\"Dev: precision\",p2,\"recall\",r2,\"F-score\",f2)\n",
    "    print(\"\\033[1;32;10mWord-frequency Baseline:\\033[0m\")\n",
    "    result1, result2, Ret_plot_freq= word_frequency_threshold(training_file,development_file,counts, True)\n",
    "    p1_wf,r1_wf,f1_wf = result1; p2_wf,r2_wf,f2_wf= result2\n",
    "    print(\"Train: precision\",p1_wf,\"recall\",r1_wf,\"F-score\",f1_wf)\n",
    "    print(\"Dev: precision\",p2_wf,\"recall\",r2_wf,\"F-score\",f2_wf)\n",
    "    print(\"\\033[1;32;10mPlot Precison-Recall curve for various thresholds for both baselines together:\\033[0m\")\n",
    "    [pt_len, rt_len, pd_len, rd_len]= Ret_plot_len\n",
    "    [pt_fq, rt_fq, pd_fq, rd_fq]= Ret_plot_freq\n",
    "    plt.plot(rt_len,pt_len,'^g',label = \"Train (length)\")\n",
    "    plt.plot(rd_len,pd_len, '^r',label = \"Dev (length)\")\n",
    "    plt.plot( rt_fq,pt_fq,'.b',label = \"Train (frequency)\")\n",
    "    plt.plot(rd_fq,pd_fq, '.y',label = \"Dev (frequency)\")\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.title('word_length_threshold\\& word_frequency_threshold: precision-recall curve')\n",
    "    plt.xlabel('Recall')\n",
    "    plt.ylabel('Precision')\n",
    "    plt.show()\n",
    "    print(\"\\033[1;32;10mWhich classifier looks better on average?\\033[0m I think word_length_threshold is better.\")\n",
    "    print(\"-------------------------------------------------------------------------------------------------------\")\n",
    "    print(\"\\033[1;32;10mNaive Bayes:\\033[0m\")\n",
    "    result1, result2= naive_bayes(training_file,development_file,counts)\n",
    "    p1,r1,f1 = result1; p2,r2,f2= result2\n",
    "    print(\"Train: precision\",p1,\"recall\",r1,\"F-score\",f1)\n",
    "    print(\"Dev: precision\",p2,\"recall\",r2,\"F-score\",f2)\n",
    "    print(\"\\033[1;32;10mLogistic regression:\\033[0m\")\n",
    "    result1, result2= logistic_regression(training_file,development_file,counts)\n",
    "    p1,r1,f1 = result1; p2,r2,f2= result2\n",
    "    print(\"Train: precision\",p1,\"recall\",r1,\"F-score\",f1)\n",
    "    print(\"Dev: precision\",p2,\"recall\",r2,\"F-score\",f2)\n",
    "#     print(\"-------------------------\")\n",
    "    print(\"\\033[1;32;10mAdd a paragraph to your write up that discusses which model performed better on this task.\\033[0m\")\n",
    "    print(\"Although naive Bayes performslightly better in term of F-score, I think logistic regression have more balanced performance between precision and recall.\")\n",
    "    print(\"-------------------------------------------------------------------------------------------------------\")\n",
    "\n",
    "    print(\"\\033[1;32;10mBuild your own model\\033[0m\")\n",
    "    print(\"\\033[1;32;10mPlease include a description of all features that you tried (not including length and frequency).\\033[0m\")\n",
    "    print(\"Besides ength and frequency (with and without thresholds -- 4 features), I tried: [see preprocess_yezheng(...)]\")\n",
    "    print(\"--- count of character 'aeiou-' (I also tried string.ascii_lowercase,etc. but the latter has worse performance) -- 6 features; \")\n",
    "    print(\"--- count_syllables(...) -- 1 features\")\n",
    "    print(\"--- convolution of feature with itself: np.convolve(X,X) -- does not work\")\n",
    "    print(\"In all, there are 11 features\")\n",
    "    print(\"\\033[1;32;10mPlease include a description of all models that you tried.\\033[0m\")\n",
    "    print(\"Besides improved Naive Bayes and improved logistic regression (with penality l1 or l2), I tried:\") \n",
    "    print(\"random forest, decision tree, svm.SVC, svm.LinearSVC, LDA (linear discriminant analysis), QDA (Quadratic Discriminant Analysis). \")\n",
    "    print(\"\\033[1;32;10mPerform a detailed error analysis of your models.\\033[0m\")  \n",
    "    print(\"See table below for a summary. \\033[1;31;10mRed\\033[0m highlights are best performances (it varies depending on different experiments, I just highlight best performances in common (for various experiments.)).\")\n",
    "    print(\"Train:\\t\\t\\t\\t\\tDev\")\n",
    "    print(\"precision\\trecall\\tF-score\\t\\tprecision\\trecall\\tF-score\\tclassifier\")\n",
    "    print(\"Baselines--------------------------------------------------------------------------------------------------------\")\n",
    "    p1,r1,f1 = all_complex(training_file);p2,r2,f2 = all_complex(development_file);\n",
    "    print(\"%.9f %.9f %.9f|%.9f %.9f %.9f All-complex\" % (p1,r1,f1,p2,r2,f2))\n",
    "    result1, result2= word_length_threshold(training_file,development_file)\n",
    "    p1,r1,f1 = result1; p2,r2,f2= result2\n",
    "    print(\"%.9f %.9f %.9f|%.9f %.9f %.9f Word-length\" % (p1,r1,f1,p2,r2,f2))\n",
    "    #     result1, result2= word_frequency_threshold(training_file,development_file, counts)\n",
    "    #     p1,r1,f1 = result1; p2,r2,f2= result2\n",
    "    print(\"%.9f %.9f %.9f|%.9f %.9f %.9f Word-frequency\" % (p1_wf,r1_wf,f1_wf,p2_wf,r2_wf,f2_wf))\n",
    "    print(\"--------------------------------------------------------------------------------------------------------\")\n",
    "    result1, result2= naive_bayes(training_file,development_file,counts)\n",
    "    p1,r1,f1 = result1; p2,r2,f2= result2\n",
    "    print(\"%.9f %.9f %.9f|%.9f %.9f %.9f Naive Bayes\" % (p1,r1,f1,p2,r2,f2))\n",
    "    result1, result2= logistic_regression(training_file,development_file,counts)\n",
    "    p1,r1,f1 = result1; p2,r2,f2= result2\n",
    "    print(\"%.9f %.9f %.9f|%.9f %.9f %.9f Logistic regression\" % (p1,r1,f1,p2,r2,f2))\n",
    "    print(\"--------------------------------------------------------------------------------------------------------\")\n",
    "    result1, result2= improved_naive_bayes(training_file,development_file,counts)\n",
    "    p1,r1,f1 = result1; p2,r2,f2= result2\n",
    "    print(\"%.9f %.9f %.9f|%.9f %.9f %.9f Naive Bayes (improved)\" % (p1,r1,f1,p2,r2,f2))\n",
    "    result1, result2= improved_log_regression(training_file,development_file,counts)\n",
    "    p1,r1,f1 = result1; p2,r2,f2= result2\n",
    "    print(\"%.9f %.9f %.9f|%.9f %.9f %.9f Logistic regression (improved)\" % (p1,r1,f1,p2,r2,f2))\n",
    "    result1, result2= random_forest(training_file,development_file,counts)\n",
    "    p1,r1,f1 = result1; p2,r2,f2= result2\n",
    "    print(\"%.9f %.9f %.9f|%.9f %.9f %.9f Random forest\" % (p1,r1,f1,p2,r2,f2))\n",
    "    result1, result2= decision_tree(training_file,development_file,counts)\n",
    "    p1,r1,f1 = result1; p2,r2,f2= result2\n",
    "    print(\"%.9f %.9f %.9f|%.9f %.9f %.9f Decision tree\" % (p1,r1,f1,p2,r2,f2))\n",
    "    result1, result2= SVM_SVC(training_file,development_file,counts)\n",
    "    p1,r1,f1 = result1; p2,r2,f2= result2\n",
    "    print(\"%.9f %.9f \\033[1;31;10m%.9f\\033[0m|%.9f %.9f \\033[1;31;10m%.9f\\033[0m SVM SVC\" % (p1,r1,f1,p2,r2,f2))\n",
    "    result1, result2= SVM_LinearSVC(training_file,development_file,counts)\n",
    "    p1,r1,f1 = result1; p2,r2,f2= result2\n",
    "    print(\"%.9f %.9f %.9f|%.9f %.9f %.9f SVM linear SVC\" % (p1,r1,f1,p2,r2,f2))\n",
    "    result1, result2= QDA(training_file,development_file,counts)\n",
    "    p1,r1,f1 = result1; p2,r2,f2= result2\n",
    "    print(\"%.9f %.9f %.9f|%.9f %.9f %.9f QDA\" % (p1,r1,f1,p2,r2,f2))\n",
    "    result1, result2= LDA(training_file,development_file,counts)\n",
    "    p1,r1,f1 = result1; p2,r2,f2= result2\n",
    "    print(\"%.9f %.9f %.9f|%.9f %.9f %.9f LDA\" % (p1,r1,f1,p2,r2,f2))\n",
    "    result1, result2= AdaBoost(training_file,development_file,counts)\n",
    "    p1,r1,f1 = result1; p2,r2,f2= result2\n",
    "    print(\"%.9f %.9f %.9f|%.9f %.9f %.9f AdaBoost\" % (p1,r1,f1,p2,r2,f2))\n",
    "    result1, result2= GradBoost(training_file,development_file,counts)\n",
    "    p1,r1,f1 = result1; p2,r2,f2= result2\n",
    "    print(\"%.9f %.9f \\033[1;31;10m%.9f\\033[0m|%.9f %.9f \\033[1;31;10m%.9f\\033[0m Gradient Boost\" % (p1,r1,f1,p2,r2,f2))\n",
    "    print(\"--------------------------------------------------------------------------------------------------------\")\n",
    "    print(\"\\033[1;32;10mAnalyze your model\\033[0m\")\n",
    "    print(\"\\033[1;32;10mAn important part of text classification tasks is to determine what your model is getting correct, and what your model is getting wrong. For this problem, you must train your best model on the training data, and report the precision, recall, and f-score on the development data.\\033[0m\")\n",
    "    print(\"As a result, I think our best model is \\033[1;31;10mSVM SVC\\033[0m as well as \\033[1;31;10mGradient Boost\\033[0m (Although GradBoost generally outperforms SVM SVC in both Train and Dev with respect to F score, it is surprising that when it comes to Leaderboard, SVM SVC results in better ranking (typically with 2\\% better Fscore).)\")\n",
    "    print(\"\\033[1;32;10mGive several examples of words on which your best model performs well. Also give examples of words which your best model performs poorly on, and identify at least TWO categories of words on which your model is making errors.\\033[0m\")\n",
    "    clf,L_train,L_dev = GradBoost(training_file,development_file,counts, show_err_words_flag = True)\n",
    "    X, w,l_pred, l_true = L_train\n",
    "    print(\"-------------------------------------\")\n",
    "    print(\"\\033[1;31;10mGradient Boost: \\033[1;32;10mcorrect prediction:\\033[0m\")\n",
    "    print(\"Examples of true positive\", [w[i] for i in range(len(l_true)) if 1 == l_pred[i] and 1 == l_true[i]][:10])\n",
    "    print(\"Examples of false negative\", [w[i] for i in range(len(l_true)) if 0 == l_pred[i] and 0 == l_true[i]][:10])\n",
    "    print(\"\\033[1;32;10mIncorrect prediction:\\033[0m\")\n",
    "    print(\"Examples of false positive (i.e. not complex, but are predicted to be)\", [w[i] for i in range(len(l_true)) if 1 == l_pred[i] and 0 ==  l_true[i]][:10])\n",
    "    print(\"Examples of true negative (i.e. complex, but are predicted not to be)\", [w[i] for i in range(len(l_true)) if 0 == l_pred[i] and 1 == l_true[i]][:10])\n",
    "    #     print(\"clf.predict_log_proba(X)\",clf.predict_log_proba(X))\n",
    "    clf,L_train,L_dev = SVM_SVC(training_file,development_file,counts, show_err_words_flag = True)\n",
    "    X,w, l_pred, l_true = L_train\n",
    "    #     print(\"clf.decision_function(X)\",clf.decision_function(X))\n",
    "    print(\"\\033[1;31;10mSVM SVC: \\033[1;32;10mcorrect prediction:\\033[0m\")\n",
    "    num_prt =8\n",
    "    print(\"Examples of true positive\", [w[i] for i in range(len(l_true)) if 1 == l_pred[i] and 1 == l_true[i]][:num_prt])\n",
    "    print(\"Examples of false negative\", [w[i] for i in range(len(l_true)) if 0 == l_pred[i] and 0 == l_true[i]][:num_prt])\n",
    "    print(\"\\033[1;32;10mIncorrect prediction:\\033[0m\")\n",
    "    print(\"Examples of false positive (i.e. not complex, but are predicted to be)\", [w[i] for i in range(len(l_true)) if 1 == l_pred[i] and 0 ==  l_true[i]][:num_prt])\n",
    "    print(\"Examples of true negative (i.e. complex, but are predicted not to be)\", [w[i] for i in range(len(l_true)) if 0 == l_pred[i] and 1 == l_true[i]][:num_prt])\n",
    "    print(\"-------------------------------------\")\n",
    "    print(\"Time elapsed:\", time.time() - T0) # time evaluation -- though improvement of precision(), recall(), fscore()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------\n",
      "Naive Bayes (improved) unlabeled output:\n",
      "Train: precision 0.7761749651 recall 0.707679253288078 F-score 0.74034620506\n",
      "-------------------------------------\n",
      "Time elapsed: 379.8445529937744\n"
     ]
    }
   ],
   "source": [
    "# leaderboard output\n",
    "\n",
    "unlabeled_file = \"data/complex_words_test_unlabeled.txt\"\n",
    "def load_file2(data_file):\n",
    "#     labels = []   \n",
    "    with open(data_file, 'rt', encoding=\"utf8\") as f:\n",
    "        Lines = f.readlines()\n",
    "        Lines = Lines[1:]\n",
    "        num_data = len(Lines)# remove first ele in Lines, remove last one \n",
    "        Lst_pos1 = [line[:-1].find('\\t') for line in Lines]\n",
    "        words = [Lines[i][:Lst_pos1[i]] for i in range(num_data)]\n",
    "        Lst_pos2 = [Lines[i][(Lst_pos1[i]+1):-1].find('\\t')+Lst_pos1[i]+1 for i in range(num_data )]\n",
    "#         for i in range(num_data ):\n",
    "#             # if debug:\n",
    "#             #     print(i,\"----\",Lines[i])\n",
    "#             #     print(\"#########\",Lines[i][(Lst_pos1[i]+1):Lst_pos2[i]],\"#######\")\n",
    "#             if (re.match(\"^\\d+?(\\.\\d+)?$\",Lines[i][(Lst_pos1[i]+1):Lst_pos2[i]])): labels.append(int(Lines[i][(Lst_pos1[i]+1):Lst_pos2[i]]))\n",
    "#             else: labels.append(None)\n",
    "    # if debug: print(\"load file DEBUG:\",len(words),len(labels),labels)\n",
    "        # labels = [int(Lines[i+1][(Lst_pos1[i]+1):Lst_pos2[i]]) for i in range(num_data)] \n",
    "        return words\n",
    "\n",
    "def improved_naive_bayes_unlabeled_output(training_lst, unlabeled_file, counts):\n",
    "    words,labels = load_file(training_lst)\n",
    "    labels_np = np.array(labels)\n",
    "    X_features, labels_np = preprocess_yezheng(words, labels, counts)\n",
    "    clf = GaussianNB(); clf.fit(X_features, labels_np)\n",
    "    Y_pred_np = clf.predict(X_features)\n",
    "    training_performance = [get_precision(labels_np, Y_pred_np), get_recall(labels_np, Y_pred_np), get_fscore(labels_np, Y_pred_np)]\n",
    "    words = load_file2(unlabeled_file)\n",
    "    X_features, labels_np = preprocess_yezheng(words, labels, counts) #labels are not useful here\n",
    "    scaler = sklearn.preprocessing.StandardScaler(); scaler.fit(X_features); X_features = scaler.transform(X_features)\n",
    "    Y_pred_np = clf.predict(X_features)\n",
    "    fname = \"test_labels.txt\"\n",
    "    with open(fname, 'wt', encoding=\"utf8\") as f: \n",
    "        for l in Y_pred_np: f.write(str(l)+'\\n')\n",
    "    return training_performance\n",
    "\n",
    "print(\"-------------------------\")\n",
    "print(\"Naive Bayes (improved) unlabeled output:\")\n",
    "p,r,f= improved_naive_bayes_unlabeled_output([training_file,development_file],unlabeled_file,counts)\n",
    "print(\"Train: precision\",p,\"recall\",r,\"F-score\",f)\n",
    "\n",
    "\n",
    "def SVM_SVC_unlabeled_output(training_lst, unlabeled_file, counts):\n",
    "    words,labels = load_file(training_lst)\n",
    "    labels_np = np.array(labels)\n",
    "    X_features, labels_np = preprocess_yezheng(words, labels, counts)\n",
    "#     print([len(fe) for fe in X_features])\n",
    "#     print([row for row in X_features if None in row])\n",
    "#     print([i for i in list(labels_np) if None == i])\n",
    "    scaler = sklearn.preprocessing.StandardScaler(); scaler.fit(X_features); X_features = scaler.transform(X_features)\n",
    "    clf = SVC(); clf.fit(X_features, labels_np)\n",
    "    Y_pred_np = clf.predict(X_features)\n",
    "    training_performance = [get_precision(labels_np, Y_pred_np), get_recall(labels_np, Y_pred_np), get_fscore(labels_np, Y_pred_np)]\n",
    "    words = load_file2(unlabeled_file)\n",
    "    X_features, labels_np = preprocess_yezheng(words, labels, counts) #labels are not useful here\n",
    "    scaler = sklearn.preprocessing.StandardScaler(); scaler.fit(X_features); X_features = scaler.transform(X_features)\n",
    "    Y_pred_np = clf.predict(X_features)\n",
    "    fname = \"test_labels.txt\"\n",
    "    with open(fname, 'wt', encoding=\"utf8\") as f: \n",
    "        for l in Y_pred_np: f.write(str(l)+'\\n')\n",
    "    return training_performance\n",
    "\n",
    "print(\"-------------------------\")\n",
    "print(\"SVM SVC unlabeled output:\")\n",
    "p,r,f= SVM_SVC_unlabeled_output([training_file,development_file],unlabeled_file,counts)\n",
    "print(\"Train: precision\",p,\"recall\",r,\"F-score\",f)\n",
    "\n",
    "\n",
    "# def AdaBoost_unlabeled_output(training_lst, unlabeled_file, counts):\n",
    "#     words,labels = load_file(training_lst)\n",
    "#     labels_np = np.array(labels)\n",
    "#     X_features, labels_np = preprocess_yezheng(words, labels, counts)\n",
    "#     clf = AdaBoostClassifier(); clf.fit(X_features, labels_np)\n",
    "#     Y_pred_np = clf.predict(X_features)\n",
    "#     training_performance = [get_precision(labels_np, Y_pred_np), get_recall(labels_np, Y_pred_np), get_fscore(labels_np, Y_pred_np)]\n",
    "#     words = load_file2(unlabeled_file)\n",
    "#     X_features, labels_np = preprocess_yezheng(words, labels, counts) #labels are not useful here\n",
    "#     Y_pred_np = clf.predict(X_features)\n",
    "#     fname = \"test_labels.txt\"\n",
    "#     with open(fname, 'wt', encoding=\"utf8\") as f: \n",
    "#         for l in Y_pred_np: f.write(str(l)+'\\n')\n",
    "#     return training_performance\n",
    "\n",
    "# print(\"-------------------------\")\n",
    "# print(\"AdaBoost unlabeled output:\")\n",
    "# p,r,f= GradBoost_unlabeled_output([training_file,development_file],unlabeled_file,counts)\n",
    "# print(\"Train: precision\",p,\"recall\",r,\"F-score\",f)\n",
    "# def GradBoost_unlabeled_output(training_lst, unlabeled_file, counts):\n",
    "#     words,labels = load_file(training_lst)\n",
    "#     labels_np = np.array(labels)\n",
    "#     X_features, labels_np = preprocess_yezheng(words, labels, counts)\n",
    "#     clf = GradientBoostingClassifier(); clf.fit(X_features, labels_np)\n",
    "#     Y_pred_np = clf.predict(X_features)\n",
    "#     training_performance = [get_precision(labels_np, Y_pred_np), get_recall(labels_np, Y_pred_np), get_fscore(labels_np, Y_pred_np)]\n",
    "#     words = load_file2(unlabeled_file)\n",
    "#     X_features, labels_np = preprocess_yezheng(words, labels, counts) #labels are not useful here\n",
    "#     Y_pred_np = clf.predict(X_features)\n",
    "#     fname = \"test_labels.txt\"\n",
    "#     with open(fname, 'wt', encoding=\"utf8\") as f: \n",
    "#         for l in Y_pred_np: f.write(str(l)+'\\n')\n",
    "#     return training_performance\n",
    "\n",
    "# print(\"-------------------------\")\n",
    "# print(\"GradBoost unlabeled output:\")\n",
    "# p,r,f= GradBoost_unlabeled_output([training_file,development_file],unlabeled_file,counts)\n",
    "# print(\"Train: precision\",p,\"recall\",r,\"F-score\",f)\n",
    "print(\"-------------------------------------\")\n",
    "print(\"Time elapsed:\", time.time() - T0) # time evaluation -- though improvement of precision(), recall(), fscore()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
